{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Simon Willison's LLM Ecosystem - Complete Cookbook\n",
    "\n",
    "**Comprehensive Guide to Using Simon's LLM Tools in LlamaAgent**\n",
    "\n",
    "This notebook demonstrates the complete integration of Simon Willison's LLM ecosystem including:\n",
    "\n",
    "- **Core LLM Library**: Universal interface to 100+ language models\n",
    "- **Provider Integrations**: OpenAI, Anthropic, Gemini, Mistral, and more\n",
    "- **Powerful Tools**: SQLite, Datasette, Docker, JavaScript execution\n",
    "- **Data Utilities**: Database operations, conversation logging, analytics\n",
    "\n",
    "**Author:** Nik Jois <nikjois@llamasearch.ai>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Basic LLM Operations](#basic-llm)\n",
    "3. [Multi-Provider Chat](#multi-provider)\n",
    "4. [Database Operations with SQLite](#database-ops)\n",
    "5. [Data Exploration with Datasette](#datasette)\n",
    "6. [Code Execution (Python, JavaScript, Docker)](#code-exec)\n",
    "7. [Conversation Analytics](#analytics)\n",
    "8. [Advanced Workflows](#advanced)\n",
    "9. [Production Deployment](#production)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Setup and Installation {#setup}\n",
    "\n",
    "First, let's install and set up the Simon Willison LLM ecosystem integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this first if not already installed)\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_packages():\n",
    "    packages = [\n",
    "        \"llm>=0.17.0\",\n",
    "        \"llm-anthropic>=0.3.0\", \n",
    "        \"llm-openai-plugin>=0.2.0\",\n",
    "        \"llm-gemini>=0.2.0\",\n",
    "        \"sqlite-utils>=3.37.0\",\n",
    "        \"datasette>=1.0.0\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"✓ Installed {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"✗ Failed to install {package}: {e}\")\n",
    "\n",
    "# Uncomment to install packages\n",
    "# install_packages()\n",
    "\n",
    "print(\"✓ Setup complete - Simon's LLM ecosystem ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LlamaAgent Simon ecosystem integration\n",
    "import json\n",
    "\n",
    "# Import our Simon ecosystem integration\n",
    "try:\n",
    "    from src.llamaagent.integration.simon_tools import SimonToolRegistry, create_simon_tools\n",
    "    from src.llamaagent.llm.simon_ecosystem import LLMProvider, LLMTool, SimonEcosystemConfig, SimonLLMEcosystem\n",
    "    print(\"✓ Successfully imported LlamaAgent Simon ecosystem\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")\n",
    "    print(\"Make sure you're running from the project root directory\")\n",
    "\n",
    "# Configure API keys (set these in your environment)\n",
    "config = SimonEcosystemConfig(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    gemini_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    database_path=\"simon_ecosystem_demo.db\",\n",
    "    enabled_tools=[\n",
    "        LLMTool.SQLITE,\n",
    "        LLMTool.PYTHON, \n",
    "        LLMTool.COMMAND,\n",
    "        LLMTool.QUICKJS\n",
    "    ],\n",
    "    log_conversations=True\n",
    ")\n",
    "\n",
    "print(\"✓ Configuration ready\")\n",
    "print(f\"  - Database: {config.database_path}\")\n",
    "print(f\"  - Enabled tools: {[tool.value for tool in config.enabled_tools]}\")\n",
    "print(f\"  - OpenAI API key: {'✓' if config.openai_api_key else '✗'}\")\n",
    "print(f\"  - Anthropic API key: {'✓' if config.anthropic_api_key else '✗'}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Basic LLM Operations {#basic-llm}\n",
    "\n",
    "Let's start with basic LLM operations using Simon's ecosystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Simon ecosystem\n",
    "ecosystem = SimonLLMEcosystem(config)\n",
    "\n",
    "async def basic_chat_example():\n",
    "    \"\"\"Demonstrate basic chat functionality.\"\"\"\n",
    "    \n",
    "    # Simple chat\n",
    "    print(\"=== Basic Chat Example ===\")\n",
    "    \n",
    "    try:\n",
    "        response = await ecosystem.chat(\n",
    "            prompt=\"Explain quantum computing in exactly 2 sentences.\",\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        print(\"Prompt: Explain quantum computing in exactly 2 sentences.\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Note: This requires an OpenAI API key to be set\")\n",
    "    \n",
    "    # Check ecosystem health\n",
    "    print(\"=== Ecosystem Health Check ===\")\n",
    "    health = await ecosystem.health_check()\n",
    "    print(json.dumps(health, indent=2))\n",
    "\n",
    "# Run the example\n",
    "await basic_chat_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding generation example\n",
    "async def embedding_example():\n",
    "    \"\"\"Demonstrate embedding generation.\"\"\"\n",
    "    \n",
    "    print(\"=== Embedding Generation Example ===\")\n",
    "    \n",
    "    try:\n",
    "        # Generate embeddings for multiple texts\n",
    "        texts = [\n",
    "            \"Machine learning is transforming technology\",\n",
    "            \"Artificial intelligence enables smart systems\", \n",
    "            \"Quantum computing promises exponential speedups\"\n",
    "        ]\n",
    "        \n",
    "        embeddings = await ecosystem.embed(\n",
    "            text=texts,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated embeddings for {len(texts)} texts\")\n",
    "        for i, (text, embedding) in enumerate(zip(texts, embeddings, strict=False)):\n",
    "            print(f\"Text {i+1}: {text}\")\n",
    "            print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "            print(f\"First 5 values: {embedding[:5]}\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Note: This requires an OpenAI API key to be set\")\n",
    "\n",
    "# Run embedding example\n",
    "await embedding_example()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Multi-Provider Chat {#multi-provider}\n",
    "\n",
    "Simon's ecosystem supports multiple LLM providers. Let's demonstrate switching between providers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def multi_provider_example():\n",
    "    \"\"\"Demonstrate using multiple LLM providers.\"\"\"\n",
    "    \n",
    "    print(\"=== Multi-Provider Chat Example ===\")\n",
    "    \n",
    "    # Define the same question for all providers\n",
    "    question = \"What is the key benefit of using async/await in Python?\"\n",
    "    \n",
    "    # List of models to try (some may not be available without API keys)\n",
    "    models_to_try = [\n",
    "        (\"gpt-4o-mini\", LLMProvider.OPENAI),\n",
    "        (\"claude-3-haiku\", LLMProvider.ANTHROPIC),\n",
    "        (\"gemini-pro\", LLMProvider.GEMINI),\n",
    "        (\"llama3.2\", LLMProvider.OLLAMA),  # If running locally\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model, provider in models_to_try:\n",
    "        print(f\"\\n--- Testing {provider.value}: {model} ---\")\n",
    "        \n",
    "        try:\n",
    "            response = await ecosystem.chat(\n",
    "                prompt=question,\n",
    "                model=model,\n",
    "                provider=provider,\n",
    "                temperature=0.3,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            \n",
    "            results[f\"{provider.value}:{model}\"] = {\n",
    "                \"success\": True,\n",
    "                \"response\": response,\n",
    "                \"length\": len(response)\n",
    "            }\n",
    "            \n",
    "            print(f\"✓ Success ({len(response)} chars)\")\n",
    "            print(f\"Response: {response[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[f\"{provider.value}:{model}\"] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"✗ Failed: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    successful = sum(1 for r in results.values() if r[\"success\"])\n",
    "    total = len(results)\n",
    "    print(f\"Successful providers: {successful}/{total}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run multi-provider example\n",
    "provider_results = await multi_provider_example()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Database Operations with SQLite {#database-ops}\n",
    "\n",
    "Simon's sqlite-utils integration provides powerful database operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def database_operations_example():\n",
    "    \"\"\"Demonstrate SQLite database operations.\"\"\"\n",
    "    \n",
    "    print(\"=== Database Operations Example ===\")\n",
    "    \n",
    "    try:\n",
    "        # Create a sample table\n",
    "        await ecosystem.use_tool(\n",
    "            \"sqlite\",\n",
    "            \"create_table\",\n",
    "            table=\"ai_models\",\n",
    "            schema={\n",
    "                \"id\": int,\n",
    "                \"name\": str,\n",
    "                \"provider\": str,\n",
    "                \"parameters\": str,\n",
    "                \"performance_score\": float\n",
    "            }\n",
    "        )\n",
    "        print(\"✓ Created 'ai_models' table\")\n",
    "        \n",
    "        # Insert sample data\n",
    "        sample_models = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"gpt-4o\",\n",
    "                \"provider\": \"OpenAI\", \n",
    "                \"parameters\": \"175B\",\n",
    "                \"performance_score\": 9.2\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"claude-3-opus\",\n",
    "                \"provider\": \"Anthropic\",\n",
    "                \"parameters\": \"Unknown\",\n",
    "                \"performance_score\": 9.1\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"name\": \"gemini-pro\",\n",
    "                \"provider\": \"Google\",\n",
    "                \"parameters\": \"Unknown\", \n",
    "                \"performance_score\": 8.8\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for model in sample_models:\n",
    "            await ecosystem.use_tool(\n",
    "                \"sqlite\",\n",
    "                \"insert\",\n",
    "                table=\"ai_models\",\n",
    "                data=model\n",
    "            )\n",
    "        \n",
    "        print(f\"✓ Inserted {len(sample_models)} model records\")\n",
    "        \n",
    "        # Query the data\n",
    "        results = await ecosystem.use_tool(\n",
    "            \"sqlite\",\n",
    "            \"query\",\n",
    "            sql=\"SELECT * FROM ai_models ORDER BY performance_score DESC\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Top AI Models by Performance ---\")\n",
    "        for row in results:\n",
    "            print(f\"{row['name']} ({row['provider']}): {row['performance_score']}\")\n",
    "        \n",
    "        # Aggregate query\n",
    "        avg_result = await ecosystem.use_tool(\n",
    "            \"sqlite\", \n",
    "            \"query\",\n",
    "            sql=\"SELECT AVG(performance_score) as avg_score, COUNT(*) as total_models FROM ai_models\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Statistics ---\")\n",
    "        stats = avg_result[0]\n",
    "        print(f\"Total models: {stats['total_models']}\")\n",
    "        print(f\"Average performance: {stats['avg_score']:.2f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run database example\n",
    "db_results = await database_operations_example()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Code Execution Tools {#code-exec}\n",
    "\n",
    "Simon's ecosystem includes powerful code execution tools for Python, JavaScript, and Docker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def code_execution_example():\n",
    "    \"\"\"Demonstrate code execution capabilities.\"\"\"\n",
    "    \n",
    "    print(\"=== Code Execution Example ===\")\n",
    "    \n",
    "    # Python code execution\n",
    "    print(\"\\n--- Python Code Execution ---\")\n",
    "    python_code = '''\n",
    "# Calculate fibonacci sequence\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# Generate first 10 fibonacci numbers\n",
    "fib_sequence = [fibonacci(i) for i in range(10)]\n",
    "print(\"Fibonacci sequence:\", fib_sequence)\n",
    "\n",
    "# Calculate some statistics\n",
    "total = sum(fib_sequence)\n",
    "average = total / len(fib_sequence)\n",
    "print(f\"Sum: {total}, Average: {average:.2f}\")\n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        result = await ecosystem.use_tool(\n",
    "            \"python\",\n",
    "            \"run\", \n",
    "            code=python_code\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(\"✓ Python execution successful\")\n",
    "            print(\"Namespace keys:\", list(result[\"namespace\"].keys()))\n",
    "        else:\n",
    "            print(f\"✗ Python execution failed: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Python execution error: {e}\")\n",
    "    \n",
    "    # JavaScript code execution (if available)\n",
    "    print(\"\\n--- JavaScript Code Execution ---\") \n",
    "    js_code = '''\n",
    "// Simple data processing\n",
    "const data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\n",
    "const squared = data.map(x => x * x);\n",
    "const sum = squared.reduce((a, b) => a + b, 0);\n",
    "\n",
    "console.log(\"Original data:\", data);\n",
    "console.log(\"Squared values:\", squared);  \n",
    "console.log(\"Sum of squares:\", sum);\n",
    "console.log(\"Average:\", sum / squared.length);\n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        result = await ecosystem.use_tool(\n",
    "            \"quickjs\",\n",
    "            \"run\",\n",
    "            code=js_code\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(\"✓ JavaScript execution successful\")\n",
    "            print(\"Output:\", result[\"output\"])\n",
    "        else:\n",
    "            print(f\"✗ JavaScript execution failed: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"JavaScript execution error: {e}\")\n",
    "    \n",
    "    # System command execution\n",
    "    print(\"\\n--- System Command Execution ---\")\n",
    "    try:\n",
    "        result = await ecosystem.use_tool(\n",
    "            \"command\",\n",
    "            \"run\",\n",
    "            command=\"echo 'Hello from system command!' && date && python --version\"\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(\"✓ Command execution successful\")\n",
    "            print(\"Output:\", result[\"stdout\"])\n",
    "        else:\n",
    "            print(f\"✗ Command failed: {result['stderr']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Command execution error: {e}\")\n",
    "\n",
    "# Run code execution examples\n",
    "await code_execution_example()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Conversation Analytics {#analytics}\n",
    "\n",
    "Analyze conversation patterns and usage statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analytics_example():\n",
    "    \"\"\"Demonstrate conversation analytics.\"\"\"\n",
    "    \n",
    "    print(\"=== Conversation Analytics Example ===\")\n",
    "    \n",
    "    # Generate some sample conversations for analysis\n",
    "    sample_prompts = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Explain neural networks\",\n",
    "        \"How does reinforcement learning work?\", \n",
    "        \"What are transformers in AI?\",\n",
    "        \"Describe computer vision applications\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating sample conversations...\")\n",
    "    for i, prompt in enumerate(sample_prompts):\n",
    "        try:\n",
    "            # This will log conversations automatically\n",
    "            response = await ecosystem.chat(\n",
    "                prompt=prompt,\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.5\n",
    "            )\n",
    "            print(f\"✓ Conversation {i+1} logged\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to generate conversation {i+1}: {e}\")\n",
    "    \n",
    "    # Get conversation statistics  \n",
    "    print(\"\\n--- Conversation Statistics ---\")\n",
    "    try:\n",
    "        stats = await ecosystem.get_conversation_stats()\n",
    "        \n",
    "        print(f\"Total conversations: {stats.get('total_conversations', 0)}\")\n",
    "        print(f\"Unique providers: {stats.get('unique_providers', 0)}\")\n",
    "        print(f\"Unique models: {stats.get('unique_models', 0)}\")\n",
    "        print(f\"Total tokens: {stats.get('total_tokens', 0)}\")\n",
    "        print(f\"Total cost: ${stats.get('total_cost', 0):.4f}\")\n",
    "        print(f\"Average cost per conversation: ${stats.get('avg_cost_per_conversation', 0):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Stats error: {e}\")\n",
    "    \n",
    "    # Search conversations\n",
    "    print(\"\\n--- Conversation Search ---\")\n",
    "    try:\n",
    "        search_results = await ecosystem.search_conversations(\n",
    "            query=\"learning\",\n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        print(f\"Found {len(search_results)} conversations containing 'learning':\")\n",
    "        for i, conv in enumerate(search_results):\n",
    "            print(f\"{i+1}. {conv['prompt'][:50]}...\")\n",
    "            print(f\"   Response: {conv['response'][:80]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run analytics example\n",
    "analytics_stats = await analytics_example()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Using Simon's Tools with LlamaAgent {#advanced}\n",
    "\n",
    "Now let's integrate Simon's tools into a LlamaAgent workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Simon's tool registry for LlamaAgent integration\n",
    "simon_tools = create_simon_tools(config)\n",
    "tool_registry = simon_tools.get_registry()\n",
    "\n",
    "async def llamaagent_integration_example():\n",
    "    \"\"\"Demonstrate LlamaAgent integration with Simon's tools.\"\"\"\n",
    "    \n",
    "    print(\"=== LlamaAgent Integration with Simon's Tools ===\")\n",
    "    \n",
    "    # List available tools\n",
    "    available_tools = tool_registry.list_tools()\n",
    "    print(f\"Available Simon tools: {len(available_tools)}\")\n",
    "    for tool_name in available_tools:\n",
    "        tool = tool_registry.get_tool(tool_name)\n",
    "        print(f\"  - {tool_name}: {tool.description}\")\n",
    "    \n",
    "    print(\"\\n--- Using Individual Tools ---\")\n",
    "    \n",
    "    # Use LLM chat tool\n",
    "    llm_chat_tool = tool_registry.get_tool(\"llm_chat\")\n",
    "    if llm_chat_tool:\n",
    "        result = await llm_chat_tool.execute(\n",
    "            prompt=\"What are the key benefits of using SQLite for data storage?\",\n",
    "            model=\"gpt-4o-mini\"\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(\"✓ LLM Chat Tool:\")\n",
    "            print(f\"  Response: {result['response'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"✗ LLM Chat failed: {result['error']}\")\n",
    "    \n",
    "    # Use SQLite tool\n",
    "    sqlite_tool = tool_registry.get_tool(\"sqlite_query\")\n",
    "    if sqlite_tool:\n",
    "        result = await sqlite_tool.execute(\n",
    "            sql=\"SELECT COUNT(*) as conversation_count FROM conversations\"\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(\"✓ SQLite Query Tool:\")\n",
    "            print(f\"  Query result: {result['data']}\")\n",
    "        else:\n",
    "            print(f\"✗ SQLite query failed: {result['error']}\")\n",
    "    \n",
    "    # Use ecosystem stats tool\n",
    "    stats_tool = tool_registry.get_tool(\"ecosystem_stats\")\n",
    "    if stats_tool:\n",
    "        result = await stats_tool.execute()\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(\"✓ Ecosystem Stats Tool:\")\n",
    "            print(f\"  Health status: {result['health_status']['status']}\")\n",
    "            print(f\"  Database status: {result['health_status']['database']}\")\n",
    "        else:\n",
    "            print(f\"✗ Stats failed: {result['error']}\")\n",
    "    \n",
    "    # Health check for all tools\n",
    "    print(\"\\n--- Tool Health Check ---\")\n",
    "    health = await simon_tools.health_check()\n",
    "    print(f\"Overall ecosystem health: {health['status']}\")\n",
    "    \n",
    "    return tool_registry\n",
    "\n",
    "# Run integration example\n",
    "integrated_tools = await llamaagent_integration_example()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Data Export and Sharing {#export}\n",
    "\n",
    "Export conversation data and set up data sharing with Datasette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def data_export_example():\n",
    "    \"\"\"Demonstrate data export capabilities.\"\"\"\n",
    "    \n",
    "    print(\"=== Data Export Example ===\")\n",
    "    \n",
    "    # Export conversations to JSON\n",
    "    try:\n",
    "        json_export = await ecosystem.export_data(\n",
    "            table=\"conversations\",\n",
    "            format=\"json\",\n",
    "            output_path=\"conversations_export.json\"\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Exported conversations to: {json_export}\")\n",
    "        \n",
    "        # Check file size\n",
    "        if os.path.exists(json_export):\n",
    "            file_size = os.path.getsize(json_export)\n",
    "            print(f\"  File size: {file_size} bytes\")\n",
    "            \n",
    "            # Show first few lines\n",
    "            with open(json_export, 'r') as f:\n",
    "                content = f.read()\n",
    "                if content:\n",
    "                    data = json.loads(content)\n",
    "                    print(f\"  Records exported: {len(data)}\")\n",
    "                    if data:\n",
    "                        print(f\"  Sample record keys: {list(data[0].keys())}\")\n",
    "                else:\n",
    "                    print(\"  No data exported\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"JSON export error: {e}\")\n",
    "    \n",
    "    # Export to CSV format\n",
    "    try:\n",
    "        csv_export = await ecosystem.export_data(\n",
    "            table=\"conversations\", \n",
    "            format=\"csv\",\n",
    "            output_path=\"conversations_export.csv\"\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Exported conversations to: {csv_export}\")\n",
    "        \n",
    "        if os.path.exists(csv_export):\n",
    "            file_size = os.path.getsize(csv_export)\n",
    "            print(f\"  CSV file size: {file_size} bytes\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"CSV export error: {e}\")\n",
    "    \n",
    "    # Datasette server demo (commented out as it starts a server)\n",
    "    print(\"\\n--- Datasette Server Integration ---\")\n",
    "    print(\"To start a Datasette server for data exploration:\")\n",
    "    print(f\"  datasette {config.database_path} --port 8001\")\n",
    "    print(\"  Then visit: http://localhost:8001\")\n",
    "    \n",
    "    return json_export\n",
    "\n",
    "# Run export example\n",
    "export_path = await data_export_example()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Production Deployment Guide {#production}\n",
    "\n",
    "Guidelines for deploying Simon's LLM ecosystem in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment configuration example\n",
    "production_config = SimonEcosystemConfig(\n",
    "    # Security: Use environment variables for API keys\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    gemini_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    \n",
    "    # Database: Use persistent storage\n",
    "    database_path=os.getenv(\"LLM_DATABASE_PATH\", \"/app/data/llm_ecosystem.db\"),\n",
    "    \n",
    "    # Performance: Optimize for production\n",
    "    default_chat_model=\"gpt-4o-mini\",  # Cost-effective default\n",
    "    default_embedding_model=\"text-embedding-3-small\",\n",
    "    \n",
    "    # Tools: Enable production-safe tools\n",
    "    enabled_tools=[\n",
    "        LLMTool.SQLITE,\n",
    "        LLMTool.PYTHON,  # Sandboxed execution\n",
    "        LLMTool.QUICKJS,  # JavaScript execution\n",
    "        # LLMTool.DOCKER,  # Enable if Docker is available\n",
    "        # LLMTool.COMMAND,  # Disable in production for security\n",
    "    ],\n",
    "    \n",
    "    # Monitoring: Enable comprehensive logging\n",
    "    log_conversations=True,\n",
    "    log_level=\"INFO\",\n",
    "    \n",
    "    # Datasette: Configure for monitoring\n",
    "    datasette_port=8001,\n",
    "    \n",
    "    # Docker: Configure for safe execution\n",
    "    docker_image=\"python:3.11-slim\",\n",
    "    docker_timeout=60  # Shorter timeout for production\n",
    ")\n",
    "\n",
    "print(\"=== Production Configuration ===\")\n",
    "print(f\"Database path: {production_config.database_path}\")\n",
    "print(f\"Enabled tools: {[tool.value for tool in production_config.enabled_tools]}\")\n",
    "print(f\"Default chat model: {production_config.default_chat_model}\")\n",
    "print(f\"Logging enabled: {production_config.log_conversations}\")\n",
    "\n",
    "# Production deployment checklist\n",
    "deployment_checklist = [\n",
    "    \"✓ Set API keys in environment variables\",\n",
    "    \"✓ Configure persistent database storage\", \n",
    "    \"✓ Enable conversation logging\",\n",
    "    \"✓ Set up monitoring with Datasette\",\n",
    "    \"✓ Configure appropriate tool permissions\",\n",
    "    \"✓ Set resource limits and timeouts\",\n",
    "    \"✓ Enable backup and recovery procedures\",\n",
    "    \"✓ Set up health monitoring endpoints\",\n",
    "    \"✓ Configure rate limiting\",\n",
    "    \"✓ Test all provider integrations\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Production Deployment Checklist ===\")\n",
    "for item in deployment_checklist:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "This cookbook has demonstrated the complete integration of Simon Willison's LLM ecosystem with LlamaAgent:\n",
    "\n",
    "### Key Features Covered\n",
    "\n",
    "1. **Multi-Provider LLM Access**: OpenAI, Anthropic, Gemini, Mistral, and more\n",
    "2. **Database Operations**: SQLite operations with sqlite-utils\n",
    "3. **Code Execution**: Python, JavaScript, and Docker execution\n",
    "4. **Data Analytics**: Conversation tracking and analysis\n",
    "5. **Tool Integration**: Seamless integration with LlamaAgent's tool system\n",
    "6. **Data Export**: JSON and CSV export capabilities\n",
    "7. **Production Deployment**: Security and monitoring considerations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Explore Additional Providers**: Try Ollama for local models, or specialized providers\n",
    "- **Advanced Analytics**: Build custom dashboards with Datasette\n",
    "- **Custom Tools**: Create your own tools using the base classes\n",
    "- **Scale Up**: Deploy in production with proper monitoring and security\n",
    "- **Integrate with Workflows**: Use in complex multi-agent scenarios\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Simon Willison's LLM Documentation](https://llm.datasette.io/)\n",
    "- [SQLite Utils Documentation](https://sqlite-utils.datasette.io/)\n",
    "- [Datasette Documentation](https://docs.datasette.io/)\n",
    "- [LlamaAgent Documentation](../docs/getting-started.md)\n",
    "\n",
    "**Happy Building with Simon's LLM Ecosystem!**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
