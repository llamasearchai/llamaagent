{
  "build_id": "20250707_191621",
  "timestamp": "2025-07-07T19:22:05.685765+00:00",
  "build_time": 344.38768219947815,
  "config": {
    "project_name": "llamaagent",
    "version": "1.0.0",
    "python_version": "3.11",
    "docker_registry": "ghcr.io/nikjois",
    "test_timeout": 300,
    "coverage_threshold": 85.0,
    "security_severity": "high",
    "performance_benchmarks": true,
    "build_docs": true,
    "create_distribution": true,
    "push_images": false,
    "deploy_staging": false,
    "deploy_production": false
  },
  "reports": {
    "black_check": {
      "exit_code": 123,
      "output": "--- /Users/nemesis/llamaagent/src/llamaagent/agents/multimodal_advanced.py\t2025-07-06 13:21:00.187860+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/multimodal_advanced.py\t2025-07-07 19:16:48.106783+00:00\n@@ -4,36 +4,50 @@\n \n from typing import Any, Dict, List, Optional\n from dataclasses import dataclass, field\n from enum import Enum\n \n+\n class ModalityType(Enum):\n     \"\"\"Types of modalities.\"\"\"\n+\n     TEXT = 'text'\n     IMAGE = 'image'\n     AUDIO = 'audio'\n     VIDEO = 'video'\n \n+\n @dataclass\n class ModalityData:\n     \"\"\"Data for a specific modality.\"\"\"\n+\n     modality_type: ModalityType\n     data: Any\n     metadata: Dict[str, Any] = field(default_factory=dict)\n \n+\n @dataclass\n class CrossModalContext:\n     \"\"\"Cross-modal context for multimodal processing.\"\"\"\n+\n     modalities: List[str] = field(default_factory=list)\n     metadata: Dict[str, Any] = field(default_factory=dict)\n+\n \n @dataclass\n class MultiModalAdvancedAgent:\n     \"\"\"Advanced multimodal agent.\"\"\"\n+\n     name: str\n     capabilities: List[str] = field(default_factory=list)\n-    \n+\n     def process(self, data: Any) -> Any:\n         \"\"\"Process multimodal data.\"\"\"\n         return f'Processed by {self.name}'\n \n-__all__ = ['MultiModalAdvancedAgent', 'CrossModalContext', 'ModalityData', 'ModalityType']\n+\n+__all__ = [\n+    'MultiModalAdvancedAgent',\n+    'CrossModalContext',\n+    'ModalityData',\n+    'ModalityType',\n+]\n--- /Users/nemesis/llamaagent/src/llamaagent/cache/__init__.py\t2025-07-06 13:19:12.739295+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/cache/__init__.py\t2025-07-07 19:16:48.137142+00:00\n@@ -1,19 +1,21 @@\n \"\"\"Cache module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict, Optional\n \n+\n class CacheManager:\n     \"\"\"Basic cache manager.\"\"\"\n-    \n+\n     def __init__(self):\n         self.cache = {}\n-    \n+\n     def get(self, key: str) -> Optional[Any]:\n         \"\"\"Get value from cache.\"\"\"\n         return self.cache.get(key)\n-    \n+\n     def set(self, key: str, value: Any) -> None:\n         \"\"\"Set value in cache.\"\"\"\n         self.cache[key] = value\n \n+\n __all__ = ['CacheManager']\n--- /Users/nemesis/llamaagent/src/llamaagent/cache/memory_pool.py\t2025-07-06 13:10:17.809505+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/cache/memory_pool.py\t2025-07-07 19:16:48.144306+00:00\n@@ -2,27 +2,28 @@\n Memory pool implementation.\n \"\"\"\n \n from typing import Any, Dict, Optional\n \n+\n class MemoryPool:\n     \"\"\"Memory pool for caching.\"\"\"\n-    \n+\n     def __init__(self, max_size: int = 1000):\n         self.max_size = max_size\n         self.pool = {}\n-    \n+\n     def get(self, key: str) -> Optional[Any]:\n         \"\"\"Get item from pool.\"\"\"\n         return self.pool.get(key)\n-    \n+\n     def set(self, key: str, value: Any) -> None:\n         \"\"\"Set item in pool.\"\"\"\n         if len(self.pool) >= self.max_size:\n             # Remove oldest item\n             oldest_key = next(iter(self.pool))\n             del self.pool[oldest_key]\n         self.pool[key] = value\n-    \n+\n     def clear(self) -> None:\n         \"\"\"Clear the pool.\"\"\"\n         self.pool.clear()\n--- /Users/nemesis/llamaagent/src/llamaagent/cli/__init__.py\t2025-07-06 13:19:12.739561+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/__init__.py\t2025-07-07 19:16:48.148302+00:00\n@@ -1,7 +1,9 @@\n \"\"\"CLI module for LlamaAgent.\"\"\"\n+\n \n def main():\n     \"\"\"Main CLI entry point.\"\"\"\n     print('LlamaAgent CLI')\n \n+\n __all__ = ['main']\n--- /Users/nemesis/llamaagent/src/llamaagent/config/__init__.py\t2025-07-06 13:19:12.739484+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/config/__init__.py\t2025-07-07 19:16:48.167911+00:00\n@@ -1,19 +1,21 @@\n \"\"\"Configuration module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict, Optional\n \n+\n class ConfigManager:\n     \"\"\"Basic configuration manager.\"\"\"\n-    \n+\n     def __init__(self):\n         self.config = {}\n-    \n+\n     def get(self, key: str, default: Any = None) -> Any:\n         \"\"\"Get configuration value.\"\"\"\n         return self.config.get(key, default)\n-    \n+\n     def set(self, key: str, value: Any) -> None:\n         \"\"\"Set configuration value.\"\"\"\n         self.config[key] = value\n \n+\n __all__ = ['ConfigManager']\n--- /Users/nemesis/llamaagent/src/llamaagent/benchmarks/baseline_agents.py\t2025-07-06 13:10:18.128820+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/benchmarks/baseline_agents.py\t2025-07-07 19:16:48.160087+00:00\n@@ -134,11 +134,15 @@\n         base_name = f\"{baseline_type.replace('_', '-').title()}{name_suffix}\"\n \n         if baseline_type == BaselineType.VANILLA_REACT:\n             config = AgentConfig(\n                 agent_name=f\"{base_name}-Vanilla\",\n-                metadata={\"role\": AgentRole.GENERALIST, \"spree_enabled\": False, \"max_iterations\": 10},\n+                metadata={\n+                    \"role\": AgentRole.GENERALIST,\n+                    \"spree_enabled\": False,\n+                    \"max_iterations\": 10,\n+                },\n                 tools=tool_names,  # Pass list of tool names, not registry\n             )\n             return VanillaReactAgent(\n                 config, llm_provider=llm_provider, tools=tools_registry\n             )\n--- /Users/nemesis/llamaagent/src/llamaagent/agents/base.py\t2025-07-07 16:12:14.653988+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/base.py\t2025-07-07 19:16:48.169480+00:00\n@@ -266,13 +266,13 @@\n                     metadata=response.metadata,\n                 )\n \n                 return TaskOutput(\n                     task_id=task_input.id,\n-                    status=TaskStatus.COMPLETED\n-                    if response.success\n-                    else TaskStatus.FAILED,\n+                    status=(\n+                        TaskStatus.COMPLETED if response.success else TaskStatus.FAILED\n+                    ),\n                     result=task_result,\n                 )\n             else:\n                 # Fallback: return a simple dict\n                 return {\n--- /Users/nemesis/llamaagent/src/llamaagent/__init__.py\t2025-07-07 19:06:49.116916+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/__init__.py\t2025-07-07 19:16:48.176480+00:00\n@@ -303,15 +303,17 @@\n ]\n \n # Import orchestrator module\n try:\n     from .orchestrator import AgentOrchestrator\n+\n     __all__.append(\"AgentOrchestrator\")\n except ImportError:\n     pass\n \n # Make integration module available\n try:\n     from . import integration\n+\n     __all__.append(\"integration\")\n except ImportError:\n     pass\n--- /Users/nemesis/llamaagent/src/llamaagent/knowledge/__init__.py\t2025-07-06 13:19:12.739681+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/knowledge/__init__.py\t2025-07-07 19:16:48.204528+00:00\n@@ -1,19 +1,21 @@\n \"\"\"Knowledge module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict, List\n \n+\n class KnowledgeBase:\n     \"\"\"Basic knowledge base.\"\"\"\n-    \n+\n     def __init__(self):\n         self.knowledge = {}\n-    \n+\n     def add(self, key: str, value: Any) -> None:\n         \"\"\"Add knowledge.\"\"\"\n         self.knowledge[key] = value\n-    \n+\n     def get(self, key: str) -> Any:\n         \"\"\"Get knowledge.\"\"\"\n         return self.knowledge.get(key)\n \n+\n __all__ = ['KnowledgeBase']\n--- /Users/nemesis/llamaagent/src/llamaagent/data_generation/base.py\t2025-07-07 15:36:08.173265+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/data_generation/base.py\t2025-07-07 19:16:48.205576+00:00\n@@ -130,6 +130,6 @@\n     \"DebateNode\",\n     \"DebateTrace\",\n     \"DataGenerationConfig\",\n     \"DataMetrics\",\n     \"DataGenerator\",\n-]\n\\ No newline at end of file\n+]\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/mlx_provider.py\t2025-07-06 13:10:17.694967+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/mlx_provider.py\t2025-07-07 19:16:48.214635+00:00\n@@ -16,20 +16,20 @@\n     async def complete(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:\n         \"\"\"Complete using MLX (fallback to Ollama for now).\"\"\"\n         if self.fallback_to_ollama:\n             # Import here to avoid circular imports\n             from .ollama_provider import OllamaProvider\n-            \n+\n             ollama = OllamaProvider(model=self.model)\n             return await ollama.complete(messages, **kwargs)\n \n         # TODO: Implement actual MLX when dependencies are resolved\n         return LLMResponse(\n             content=\"MLX provider not yet implemented - using Ollama fallback\",\n             model=f\"mlx-{self.model}\",\n             tokens_used=0,\n-            **kwargs\n+            **kwargs,\n         )\n \n     async def shutdown(self) -> None:\n         \"\"\"Shutdown the provider.\"\"\"\n-        pass\n\\ No newline at end of file\n+        pass\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/__init__.py\t2025-07-06 13:14:45.660542+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/__init__.py\t2025-07-07 19:16:48.234394+00:00\n@@ -104,11 +104,13 @@\n ) -> BaseLLMProvider:\n     \"\"\"Create provider instance without fallback.\"\"\"\n     provider_class = get_provider_class(provider_name)\n \n     if not provider_class:\n-        raise ValueError(f\"Provider '{provider_name}' not available. Available providers: {get_available_providers()}\")\n+        raise ValueError(\n+            f\"Provider '{provider_name}' not available. Available providers: {get_available_providers()}\"\n+        )\n \n     if provider_name == \"mock\":\n         return provider_class(model_name=model_name or \"mock-model\")\n     else:\n         return provider_class(api_key=api_key, model_name=model_name, **kwargs)\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/factory.py\t2025-07-06 13:16:03.374177+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/factory.py\t2025-07-07 19:16:48.257647+00:00\n@@ -175,42 +175,52 @@\n         # Validate API key for providers that require it\n         def _is_valid_api_key(key: str, provider: str) -> bool:\n             \"\"\"Check if API key is valid (not placeholder or empty).\"\"\"\n             if not key or key.strip() == \"\":\n                 return False\n-            \n+\n             # Reject common placeholder values\n             placeholder_patterns = [\n                 \"${OPENAI_API_KEY}\",\n                 \"${OPENAI_API_KEY}\",\n                 f\"your_{provider}_api_key\",\n                 f\"your-{provider}-api-key\",\n                 \"${OPENAI_API_KEY}\",\n                 \"replace-with-your-key\",\n                 \"your_key_here\",\n                 \"INSERT_YOUR_KEY_HERE\",\n-                \"ADD_YOUR_KEY_HERE\"\n+                \"ADD_YOUR_KEY_HERE\",\n             ]\n-            \n+\n             if key.lower() in [p.lower() for p in placeholder_patterns]:\n                 return False\n-            \n+\n             # Allow test keys and real keys\n             if key == \"test-key\" or key.startswith(\"sk-\") or key.startswith(\"claude-\"):\n                 return True\n-                \n+\n             # For other providers, accept any non-placeholder key\n             return True\n-        \n+\n         if provider_type == \"openai\" and not _is_valid_api_key(api_key, \"openai\"):\n-            raise ValueError(\"OpenAI API key is required and cannot be a placeholder value\")\n-        elif provider_type == \"anthropic\" and not _is_valid_api_key(api_key, \"anthropic\"):\n-            raise ValueError(\"Anthropic API key is required and cannot be a placeholder value\")\n+            raise ValueError(\n+                \"OpenAI API key is required and cannot be a placeholder value\"\n+            )\n+        elif provider_type == \"anthropic\" and not _is_valid_api_key(\n+            api_key, \"anthropic\"\n+        ):\n+            raise ValueError(\n+                \"Anthropic API key is required and cannot be a placeholder value\"\n+            )\n         elif provider_type == \"cohere\" and not _is_valid_api_key(api_key, \"cohere\"):\n-            raise ValueError(\"Cohere API key is required and cannot be a placeholder value\")\n+            raise ValueError(\n+                \"Cohere API key is required and cannot be a placeholder value\"\n+            )\n         elif provider_type == \"together\" and not _is_valid_api_key(api_key, \"together\"):\n-            raise ValueError(\"Together API key is required and cannot be a placeholder value\")\n+            raise ValueError(\n+                \"Together API key is required and cannot be a placeholder value\"\n+            )\n \n         # Use default model if not provided\n         if not model_name:\n             model_name = self.DEFAULT_MODELS.get(provider_type, \"default-model\")\n \n@@ -224,13 +234,11 @@\n         # Create new provider instance\n         provider_class = self.PROVIDER_CLASSES[provider_type]\n \n         # Create provider with appropriate arguments\n         if provider_type == \"mock\":\n-            provider = provider_class(\n-                api_key=api_key, model_name=model_name, **kwargs\n-            )\n+            provider = provider_class(api_key=api_key, model_name=model_name, **kwargs)\n         else:\n             provider = provider_class(api_key=api_key, model=model_name, **kwargs)\n \n         # Cache the provider\n         self._providers[cache_key] = provider\n--- /Users/nemesis/llamaagent/src/llamaagent/api/simple_app.py\t2025-07-07 16:10:29.841188+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/api/simple_app.py\t2025-07-07 19:16:48.261675+00:00\n@@ -36,17 +36,19 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n # Properly typed global state\n class AppState:\n     def __init__(self):\n         self.agents: Dict[str, ReactAgent] = {}\n         self.providers: Dict[str, Any] = {}\n         self.tools: Optional[ToolRegistry] = None\n         self.start_time: float = time.time()\n+\n \n # Global state instance\n app_state = AppState()\n \n \n@@ -134,17 +136,19 @@\n @asynccontextmanager\n async def lifespan(app: FastAPI):\n     \"\"\"Application lifespan handler.\"\"\"\n     # Startup\n     logger.info(\"Starting LlamaAgent Simple API...\")\n-    \n+\n     # Initialize tools\n     tool_registry = ToolRegistry()\n     tool_registry.register(CalculatorTool())\n     app_state.tools = tool_registry\n-    logger.info(\"Tool registry initialized with %d tools\", len(tool_registry.list_names()))\n-    \n+    logger.info(\n+        \"Tool registry initialized with %d tools\", len(tool_registry.list_names())\n+    )\n+\n     # Create a default agent\n     try:\n         provider = MockProvider(model_name=\"mock-model\")\n         config = AgentConfig(\n             name=\"default-agent\",\n@@ -158,26 +162,26 @@\n         )\n         app_state.agents[\"default\"] = agent\n         logger.info(\"Default agent created successfully\")\n     except Exception as e:\n         logger.error(\"Failed to create default agent: %s\", e)\n-    \n+\n     logger.info(\"LlamaAgent Simple API startup complete\")\n-    \n+\n     yield\n-    \n+\n     # Shutdown\n     logger.info(\"Shutting down LlamaAgent Simple API...\")\n-    \n+\n     # Cleanup agents\n     for agent in app_state.agents.values():\n         if hasattr(agent, \"cleanup\"):\n             try:\n                 await agent.cleanup()\n             except Exception as e:\n                 logger.error(\"Agent cleanup error: %s\", e)\n-    \n+\n     logger.info(\"Shutdown complete\")\n \n \n # Create FastAPI app with lifespan\n app = FastAPI(\n@@ -203,12 +207,11 @@\n @app.exception_handler(Exception)\n async def general_exception_handler(request: Request, exc: Exception):\n     \"\"\"Handle general exceptions.\"\"\"\n     logger.error(\"Unhandled exception: %s\", exc)\n     return JSONResponse(\n-        status_code=500,\n-        content={\"detail\": \"Internal server error\", \"error\": str(exc)}\n+        status_code=500, content={\"detail\": \"Internal server error\", \"error\": str(exc)}\n     )\n \n \n # Root endpoint\n @app.get(\"/\", tags=[\"Root\"])\n@@ -233,19 +236,19 @@\n # Health check endpoint\n @app.get(\"/health\", response_model=HealthCheckResponse, tags=[\"Health\"])\n async def health_check():\n     \"\"\"Comprehensive health check.\"\"\"\n     providers_available = [\"mock\"]\n-    \n+\n     # Test other providers if available\n     for provider_name in [\"openai\", \"anthropic\", \"ollama\"]:\n         try:\n             create_provider(provider_name, api_key=\"test\")\n             providers_available.append(provider_name)\n         except Exception:\n             continue\n-    \n+\n     return HealthCheckResponse(\n         status=\"healthy\",\n         timestamp=datetime.now(timezone.utc).isoformat(),\n         version=\"1.0.0\",\n         agents_count=len(app_state.agents),\n@@ -274,19 +277,19 @@\n     \"\"\"OpenAI-compatible chat completions endpoint.\"\"\"\n     try:\n         # Get or create agent\n         agent_name = request.agent_name or \"default\"\n         agent = app_state.agents.get(agent_name)\n-        \n+\n         if not agent:\n             # Create agent on demand\n             try:\n                 if \"gpt\" in request.model:\n                     provider = create_provider(\"openai\", model_name=request.model)\n                 else:\n                     provider = MockProvider(model_name=request.model)\n-                \n+\n                 config = AgentConfig(\n                     name=agent_name,\n                     role=AgentRole.GENERALIST,\n                     description=f\"Agent for {request.model}\",\n                 )\n@@ -300,27 +303,27 @@\n                 logger.error(\"Failed to create agent: %s\", e)\n                 # Fallback to default agent\n                 agent = app_state.agents.get(\"default\")\n                 if not agent:\n                     raise HTTPException(status_code=500, detail=\"No agents available\")\n-        \n+\n         # Extract task from messages\n         if request.messages:\n             task = request.messages[-1].content\n         else:\n             task = \"Hello\"\n-        \n+\n         # Execute task\n         response = await agent.execute(task)\n-        \n+\n         # Format response\n         choice = {\n             \"index\": 0,\n             \"message\": {\"role\": \"assistant\", \"content\": response.content},\n             \"finish_reason\": \"stop\",\n         }\n-        \n+\n         return ChatCompletionResponse(\n             id=f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n             object=\"chat.completion\",\n             created=int(time.time()),\n             model=request.model,\n@@ -329,11 +332,11 @@\n                 \"prompt_tokens\": len(task) // 4,\n                 \"completion_tokens\": response.tokens_used,\n                 \"total_tokens\": (len(task) // 4) + response.tokens_used,\n             },\n         )\n-    \n+\n     except Exception as e:\n         logger.error(\"Chat completion error: %s\", e)\n         raise HTTPException(status_code=500, detail=str(e))\n \n \n@@ -345,117 +348,127 @@\n         # Create provider\n         if request.provider == \"mock\":\n             provider = MockProvider(model_name=request.model)\n         else:\n             provider = create_provider(\n-                request.provider,\n-                model_name=request.model,\n-                api_key=request.api_key\n+                request.provider, model_name=request.model, api_key=request.api_key\n             )\n-        \n+\n         # Create agent config\n         config = AgentConfig(\n             name=request.name,\n             role=request.role,\n             description=request.description,\n         )\n-        \n+\n         # Create agent\n         agent = ReactAgent(\n             config=config,\n             llm_provider=provider,\n             tools=app_state.tools,\n         )\n-        \n+\n         # Generate unique ID and store agent\n         agent_id = str(uuid.uuid4())\n         app_state.agents[agent_id] = agent\n-        \n+\n         # Return response\n         return AgentResponse(\n             agent_id=agent_id,\n             name=request.name,\n             role=request.role.value,\n             provider=request.provider,\n             model=request.model,\n             tools=request.tools,\n             created_at=datetime.now(timezone.utc).isoformat(),\n         )\n-    \n+\n     except Exception as e:\n         logger.error(\"Agent creation error: %s\", e)\n         raise HTTPException(status_code=500, detail=str(e))\n \n \n @app.get(\"/agents\", response_model=AgentListResponse, tags=[\"Agents\"])\n async def list_agents():\n     \"\"\"List all agents.\"\"\"\n     agents: List[Dict[str, Any]] = []\n     for agent_id, agent in app_state.agents.items():\n-        agents.append({\n-            \"agent_id\": agent_id,\n-            \"name\": agent.config.name,\n-            \"role\": agent.config.role.value if hasattr(agent.config.role, 'value') else str(agent.config.role),\n-            \"description\": agent.config.description,\n-            \"created_at\": datetime.now(timezone.utc).isoformat(),\n-        })\n-    \n+        agents.append(\n+            {\n+                \"agent_id\": agent_id,\n+                \"name\": agent.config.name,\n+                \"role\": (\n+                    agent.config.role.value\n+                    if hasattr(agent.config.role, 'value')\n+                    else str(agent.config.role)\n+                ),\n+                \"description\": agent.config.description,\n+                \"created_at\": datetime.now(timezone.utc).isoformat(),\n+            }\n+        )\n+\n     return AgentListResponse(agents=agents, total=len(agents))\n \n \n @app.get(\"/agents/{agent_id}\", tags=[\"Agents\"])\n async def get_agent(agent_id: str):\n     \"\"\"Get agent by ID.\"\"\"\n     agent = app_state.agents.get(agent_id)\n     if not agent:\n         raise HTTPException(status_code=404, detail=f\"Agent {agent_id} not found\")\n-    \n+\n     return {\n         \"agent_id\": agent_id,\n         \"name\": agent.config.name,\n-        \"role\": agent.config.role.value if hasattr(agent.config.role, 'value') else str(agent.config.role),\n+        \"role\": (\n+            agent.config.role.value\n+            if hasattr(agent.config.role, 'value')\n+            else str(agent.config.role)\n+        ),\n         \"description\": agent.config.description,\n         \"status\": \"active\",\n     }\n \n \n @app.delete(\"/agents/{agent_id}\", tags=[\"Agents\"])\n async def delete_agent(agent_id: str):\n     \"\"\"Delete an agent.\"\"\"\n     if agent_id not in app_state.agents:\n         raise HTTPException(status_code=404, detail=f\"Agent {agent_id} not found\")\n-    \n+\n     # Cleanup agent\n     agent = app_state.agents[agent_id]\n     if hasattr(agent, \"cleanup\"):\n         try:\n             await agent.cleanup()\n         except Exception as e:\n             logger.error(\"Agent cleanup error: %s\", e)\n-    \n+\n     # Remove from state\n     del app_state.agents[agent_id]\n-    \n+\n     return {\"message\": f\"Agent {agent_id} deleted successfully\"}\n \n \n # Tools endpoint\n @app.get(\"/tools\", response_model=ToolListResponse, tags=[\"Tools\"])\n async def list_tools():\n     \"\"\"List available tools.\"\"\"\n     if not app_state.tools:\n         return ToolListResponse(tools=[], total=0)\n-    \n+\n     tools: List[Dict[str, Any]] = []\n     for tool_name in app_state.tools.list_names():\n         tool = app_state.tools.get(tool_name)\n-        tools.append({\n-            \"name\": tool_name,\n-            \"description\": getattr(tool, \"description\", \"No description\"),\n-            \"enabled\": True,\n-        })\n-    \n+        tools.append(\n+            {\n+                \"name\": tool_name,\n+                \"description\": getattr(tool, \"description\", \"No description\"),\n+                \"enabled\": True,\n+            }\n+        )\n+\n     return ToolListResponse(tools=tools, total=len(tools))\n \n \n # Metrics endpoint\n @app.get(\"/metrics\", response_model=MetricsResponse, tags=[\"Monitoring\"])\n@@ -481,6 +494,6 @@\n         \"src.llamaagent.api.simple_app:app\",\n         host=\"0.0.0.0\",\n         port=8000,\n         reload=True,\n         log_level=\"info\",\n-    ) \n\\ No newline at end of file\n+    )\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/cohere_provider.py\t2025-07-07 15:03:05.293858+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/cohere_provider.py\t2025-07-07 19:16:48.281565+00:00\n@@ -13,172 +13,166 @@\n from ..messages import LLMMessage, LLMResponse\n \n \n class CohereConfig(BaseModel):\n     \"\"\"Configuration for Cohere provider.\"\"\"\n-    \n+\n     api_key: str = Field(..., description=\"Cohere API key\")\n     base_url: str = Field(\n-        default=\"https://api.cohere.ai/v1\", \n-        description=\"Cohere API base URL\"\n+        default=\"https://api.cohere.ai/v1\", description=\"Cohere API base URL\"\n     )\n     timeout: int = Field(default=60, description=\"Request timeout in seconds\")\n     max_retries: int = Field(default=3, description=\"Maximum number of retries\")\n \n \n class CohereProvider(BaseLLMProvider):\n     \"\"\"Cohere LLM provider implementation.\"\"\"\n-    \n+\n     def __init__(self, config: Optional[CohereConfig] = None) -> None:\n         if config is None:\n             config = CohereConfig(api_key=os.getenv(\"COHERE_API_KEY\", \"\"))\n-        \n+\n         self.config = config\n         self.client = httpx.AsyncClient(\n             base_url=config.base_url,\n             timeout=config.timeout,\n-            headers={\"Authorization\": f\"Bearer {config.api_key}\"}\n+            headers={\"Authorization\": f\"Bearer {config.api_key}\"},\n         )\n-        \n+\n         # Available models\n         self.available_models = [\n             \"command\",\n             \"command-light\",\n             \"command-nightly\",\n-            \"command-light-nightly\"\n+            \"command-light-nightly\",\n         ]\n-    \n+\n     async def generate_response(\n-        self, \n-        prompt: str, \n+        self,\n+        prompt: str,\n         model: Optional[str] = None,\n         max_tokens: int = 1000,\n         temperature: float = 0.7,\n-        **kwargs\n+        **kwargs,\n     ) -> LLMResponse:\n         \"\"\"Generate response using Cohere.\"\"\"\n-        \n+\n         # Use default model if none specified\n         if model is None:\n             model = \"command\"\n-        \n+\n         payload = {\n             \"model\": model,\n             \"prompt\": prompt,\n             \"max_tokens\": max_tokens,\n-            \"temperature\": temperature\n+            \"temperature\": temperature,\n         }\n-        \n+\n         try:\n             response = await self._make_request(\"/generate\", payload)\n-            \n+\n             if \"message\" in response and response.get(\"message\"):\n                 raise Exception(f\"Cohere API error: {response['message']}\")\n-            \n+\n             # Extract response content\n             generations = response.get(\"generations\", [])\n-            \n+\n             if not generations:\n                 raise Exception(\"No generations returned from Cohere API\")\n-            \n+\n             content = generations[0].get(\"text\", \"\")\n-            \n+\n             # Calculate tokens (approximate)\n             input_tokens = len(prompt.split())\n             output_tokens = len(content.split())\n             total_tokens = input_tokens + output_tokens\n-            \n+\n             return LLMResponse(\n                 content=content,\n                 provider=\"cohere\",\n                 model=model,\n                 input_tokens=input_tokens,\n                 output_tokens=output_tokens,\n-                total_tokens=total_tokens\n+                total_tokens=total_tokens,\n             )\n-            \n+\n         except Exception as e:\n             return LLMResponse(\n                 content=f\"Error occurred: {str(e)}\",\n                 provider=\"cohere\",\n                 model=model,\n-                error=str(e)\n+                error=str(e),\n             )\n-    \n+\n     async def complete(\n-        self, \n-        messages: List[LLMMessage], \n-        model: Optional[str] = None,\n-        **kwargs\n+        self, messages: List[LLMMessage], model: Optional[str] = None, **kwargs\n     ) -> LLMResponse:\n         \"\"\"Complete using Cohere API with messages interface.\"\"\"\n-        \n+\n         # Convert messages to a single prompt for Cohere\n         prompt_parts = []\n         for msg in messages:\n             if msg.role == \"system\":\n                 prompt_parts.append(f\"System: {msg.content}\")\n             elif msg.role == \"user\":\n                 prompt_parts.append(f\"User: {msg.content}\")\n             elif msg.role == \"assistant\":\n                 prompt_parts.append(f\"Assistant: {msg.content}\")\n-        \n+\n         prompt = \"\\n\".join(prompt_parts)\n         if not prompt_parts or prompt_parts[-1].startswith(\"User:\"):\n             prompt += \"\\nAssistant:\"\n-        \n+\n         # Use the existing generate_response method\n         return await self.generate_response(prompt=prompt, model=model, **kwargs)\n-    \n-    async def _make_request(self, endpoint: str, data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n+\n+    async def _make_request(\n+        self, endpoint: str, data: Optional[Dict[str, Any]] = None\n+    ) -> Dict[str, Any]:\n         \"\"\"Make HTTP request to Cohere API.\"\"\"\n-        \n+\n         for attempt in range(self.config.max_retries):\n             try:\n                 if data:\n                     response = await self.client.post(endpoint, json=data)\n                 else:\n                     response = await self.client.get(endpoint)\n-                \n+\n                 response.raise_for_status()\n                 return response.json()\n-                \n+\n             except httpx.HTTPStatusError as e:\n                 if attempt == self.config.max_retries - 1:\n                     raise Exception(f\"HTTP {e.response.status_code}: {e.response.text}\")\n                 await asyncio.sleep(2**attempt)  # Exponential backoff\n-                \n+\n             except Exception as e:\n                 if attempt == self.config.max_retries - 1:\n                     raise e\n                 await asyncio.sleep(2**attempt)\n-        \n+\n         # This should never be reached due to the exceptions above\n         raise Exception(\"Request failed after all retries\")\n-    \n+\n     async def health_check(self) -> bool:\n         \"\"\"Check if Cohere API is accessible.\"\"\"\n         try:\n             # Make a simple request to check API health\n-            test_payload = {\n-                \"model\": \"command\",\n-                \"prompt\": \"Test\",\n-                \"max_tokens\": 1\n-            }\n+            test_payload = {\"model\": \"command\", \"prompt\": \"Test\", \"max_tokens\": 1}\n             await self._make_request(\"/generate\", test_payload)\n             return True\n         except Exception:\n             return False\n-    \n+\n     async def list_models(self) -> List[str]:\n         \"\"\"List available models from Cohere.\"\"\"\n         return self.available_models.copy()\n-    \n+\n     async def close(self) -> None:\n         \"\"\"Close the HTTP client.\"\"\"\n         await self.client.aclose()\n-    \n+\n     def __del__(self) -> None:\n         \"\"\"Cleanup on deletion.\"\"\"\n         try:\n             asyncio.create_task(self.close())\n         except Exception:\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/together_provider.py\t2025-07-06 13:10:17.728983+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/together_provider.py\t2025-07-07 19:16:48.283664+00:00\n@@ -4,26 +4,27 @@\n \n from typing import Any, Dict, List, Optional\n from ..base import BaseLLMProvider\n from ..messages import LLMMessage, LLMResponse\n \n+\n class TogetherProvider(BaseLLMProvider):\n     \"\"\"Together AI provider implementation.\"\"\"\n-    \n+\n     def __init__(self, config: Optional[Dict[str, Any]] = None):\n         self.config = config or {}\n-    \n+\n     async def generate_response(self, prompt: str, **kwargs) -> LLMResponse:\n         \"\"\"Generate response using Together AI.\"\"\"\n         return LLMResponse(\n             content=\"Together AI provider not implemented\",\n             provider=\"together\",\n-            error=\"Not implemented\"\n+            error=\"Not implemented\",\n         )\n-    \n+\n     async def complete(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:\n         \"\"\"Complete using Together AI.\"\"\"\n         return LLMResponse(\n             content=\"Together AI provider not implemented\",\n             provider=\"together\",\n-            error=\"Not implemented\"\n+            error=\"Not implemented\",\n         )\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/mlx_provider.py\t2025-07-07 15:03:32.762255+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/mlx_provider.py\t2025-07-07 19:16:48.282812+00:00\n@@ -166,11 +166,11 @@\n         prompt = \"\\n\".join(prompt_parts) + \"\\nAssistant:\"\n \n         # Generate response using MLX\n         # Run MLX generation in a thread pool since it's synchronous\n         loop = asyncio.get_event_loop()\n-        \n+\n         def _generate():\n             try:\n                 response = self.client.generate(\n                     model or self.model,\n                     prompt,\n@@ -234,11 +234,11 @@\n             max_tokens=max_tokens,\n             temperature=temperature,\n             model=model,\n             **kwargs,\n         )\n-        \n+\n         # Split response into chunks for streaming simulation\n         words = response.content.split()\n         for word in words:\n             yield word + \" \"\n             await asyncio.sleep(0.01)  # Small delay to simulate streaming\n@@ -269,11 +269,11 @@\n         Returns:\n             True if model is available, False otherwise\n         \"\"\"\n         if not self.client:\n             return False\n-        \n+\n         # MLX models are typically loaded from HuggingFace or local paths\n         # For now, we'll assume the model is valid if it follows the pattern\n         return \"/\" in model or model.startswith(\"mlx-community/\")\n \n     async def health_check(self) -> bool:\n@@ -282,14 +282,15 @@\n         Returns:\n             True if provider is operational, False otherwise\n         \"\"\"\n         if self.client is None:\n             return False\n-        \n+\n         try:\n             # Try to check if MLX is available\n             import mlx\n+\n             return mlx.core.default_device().type.name == \"gpu\"\n         except Exception:\n             return False\n \n     def calculate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:\n@@ -302,6 +303,6 @@\n             completion_tokens: Number of output tokens\n \n         Returns:\n             Always returns 0.0 for local inference\n         \"\"\"\n-        return 0.0\n\\ No newline at end of file\n+        return 0.0\n--- /Users/nemesis/llamaagent/src/llamaagent/memory/__init__.py\t2025-07-06 13:19:12.739063+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/memory/__init__.py\t2025-07-07 19:16:48.288301+00:00\n@@ -1,19 +1,21 @@\n \"\"\"Memory module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict, List, Optional\n \n+\n class MemoryManager:\n     \"\"\"Basic memory manager.\"\"\"\n-    \n+\n     def __init__(self):\n         self.memory = {}\n-    \n+\n     def store(self, key: str, value: Any) -> None:\n         \"\"\"Store a value in memory.\"\"\"\n         self.memory[key] = value\n-    \n+\n     def retrieve(self, key: str) -> Optional[Any]:\n         \"\"\"Retrieve a value from memory.\"\"\"\n         return self.memory.get(key)\n \n+\n __all__ = ['MemoryManager']\n--- /Users/nemesis/llamaagent/src/llamaagent/ml/__init__.py\t2025-07-06 13:19:12.739768+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/ml/__init__.py\t2025-07-07 19:16:48.289756+00:00\n@@ -1,12 +1,14 @@\n \"\"\"ML module for LlamaAgent.\"\"\"\n \n from typing import Any, List\n \n+\n class MLModel:\n     \"\"\"Basic ML model interface.\"\"\"\n-    \n+\n     def predict(self, data: Any) -> Any:\n         \"\"\"Make prediction.\"\"\"\n         return None\n \n+\n __all__ = ['MLModel']\n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/__init__.py\t2025-07-06 13:19:12.739848+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/__init__.py\t2025-07-07 19:16:48.294691+00:00\n@@ -1,15 +1,17 @@\n \"\"\"Monitoring module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict\n \n+\n class Monitor:\n     \"\"\"Basic monitoring.\"\"\"\n-    \n+\n     def __init__(self):\n         self.metrics = {}\n-    \n+\n     def record(self, metric: str, value: Any) -> None:\n         \"\"\"Record a metric.\"\"\"\n         self.metrics[metric] = value\n \n+\n __all__ = ['Monitor']\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/mock_provider.py\t2025-07-06 19:36:25.196707+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/mock_provider.py\t2025-07-07 19:16:48.282019+00:00\n@@ -29,121 +29,137 @@\n         **kwargs: Any,\n     ):\n         \"\"\"Initialize mock provider.\"\"\"\n         super().__init__(api_key=api_key, model=model_name, **kwargs)\n         self.call_count = 0\n-        \n+\n     def _solve_math_problem(self, prompt: str) -> str:\n         \"\"\"Solve mathematical problems intelligently.\"\"\"\n         # Extract mathematical expressions and solve them\n-        \n+\n         # Handle percentage calculations\n         if \"%\" in prompt and \"of\" in prompt:\n             # Pattern: \"Calculate X% of Y\"\n             match = re.search(r'(\\d+(?:\\.\\d+)?)%\\s+of\\s+(\\d+(?:\\.\\d+)?)', prompt)\n             if match:\n                 percentage = float(match.group(1))\n                 number = float(match.group(2))\n                 result = (percentage / 100) * number\n-                \n+\n                 # Check if we need to add something\n                 if \"add\" in prompt.lower():\n                     add_match = re.search(r'add\\s+(\\d+(?:\\.\\d+)?)', prompt)\n                     if add_match:\n                         add_value = float(add_match.group(1))\n                         result += add_value\n-                \n+\n                 return str(int(result) if result.is_integer() else result)\n-        \n+\n         # Handle perimeter calculations\n         if \"perimeter\" in prompt.lower() and \"rectangle\" in prompt.lower():\n             # Extract length and width\n             length_match = re.search(r'length\\s+(\\d+(?:\\.\\d+)?)', prompt)\n             width_match = re.search(r'width\\s+(\\d+(?:\\.\\d+)?)', prompt)\n             if length_match and width_match:\n                 length = float(length_match.group(1))\n                 width = float(width_match.group(1))\n                 perimeter = 2 * (length + width)\n                 return f\"{int(perimeter) if perimeter.is_integer() else perimeter} cm\"\n-        \n+\n         # Handle compound interest\n         if \"compound interest\" in prompt.lower():\n             # Extract principal, rate, and time\n             principal_match = re.search(r'\\$(\\d+(?:,\\d+)?)', prompt)\n             rate_match = re.search(r'(\\d+(?:\\.\\d+)?)%', prompt)\n             time_match = re.search(r'(\\d+)\\s+years?', prompt)\n-            \n+\n             if principal_match and rate_match and time_match:\n                 principal = float(principal_match.group(1).replace(',', ''))\n                 rate = float(rate_match.group(1)) / 100\n                 time = int(time_match.group(1))\n-                \n+\n                 # Compound interest formula: A = P(1 + r)^t\n                 amount = principal * (1 + rate) ** time\n                 return f\"${amount:.2f}\"\n-        \n+\n         # Handle derivatives\n         if \"derivative\" in prompt.lower():\n             # Simple polynomial derivative\n             if \"f(x) = 3x\u00b3 - 2x\u00b2 + 5x - 1\" in prompt:\n                 # df/dx = 9x\u00b2 - 4x + 5\n                 if \"x = 2\" in prompt:\n                     # Evaluate at x = 2: 9(4) - 4(2) + 5 = 36 - 8 + 5 = 33\n                     return \"33\"\n-        \n+\n         # Handle simple arithmetic\n-        simple_math = re.search(r'(\\d+(?:\\.\\d+)?)\\s*([\\+\\-\\*/])\\s*(\\d+(?:\\.\\d+)?)', prompt)\n+        simple_math = re.search(\n+            r'(\\d+(?:\\.\\d+)?)\\s*([\\+\\-\\*/])\\s*(\\d+(?:\\.\\d+)?)', prompt\n+        )\n         if simple_math:\n             left = float(simple_math.group(1))\n             op = simple_math.group(2)\n             right = float(simple_math.group(3))\n-            \n+\n             if op == '+':\n                 result = left + right\n             elif op == '-':\n                 result = left - right\n             elif op == '*':\n                 result = left * right\n             elif op == '/':\n                 result = left / right\n             else:\n                 return \"Unable to solve\"\n-            \n+\n             return str(int(result) if result.is_integer() else result)\n-        \n+\n         return \"Unable to solve this mathematical problem\"\n-    \n+\n     def _generate_code(self, prompt: str) -> str:\n         \"\"\"Generate code based on the prompt.\"\"\"\n         if \"python function\" in prompt.lower() and \"maximum\" in prompt.lower():\n             return \"\"\"def max_two(a, b):\n     return a if a > b else b\"\"\"\n-        \n+\n         if \"function\" in prompt.lower() and \"return\" in prompt.lower():\n             return \"def example_function(): return 'example'\"\n-        \n+\n         return \"# Code generation not implemented for this request\"\n-    \n+\n     def _analyze_prompt_intent(self, prompt: str) -> str:\n         \"\"\"Analyze prompt and provide intelligent response.\"\"\"\n         prompt_lower = prompt.lower()\n-        \n+\n         # Mathematical problems\n-        if any(word in prompt_lower for word in ['calculate', 'math', '%', 'perimeter', 'interest', 'derivative']):\n+        if any(\n+            word in prompt_lower\n+            for word in [\n+                'calculate',\n+                'math',\n+                '%',\n+                'perimeter',\n+                'interest',\n+                'derivative',\n+            ]\n+        ):\n             return self._solve_math_problem(prompt)\n-        \n+\n         # Programming requests\n-        if any(word in prompt_lower for word in ['function', 'python', 'code', 'write']):\n+        if any(\n+            word in prompt_lower for word in ['function', 'python', 'code', 'write']\n+        ):\n             return self._generate_code(prompt)\n-        \n+\n         # Planning and reasoning\n-        if any(word in prompt_lower for word in ['plan', 'strategy', 'approach', 'steps']):\n+        if any(\n+            word in prompt_lower for word in ['plan', 'strategy', 'approach', 'steps']\n+        ):\n             return \"\"\"Let me break this down into steps:\n 1. First, I'll analyze the requirements\n 2. Then, I'll identify the key components needed\n 3. Finally, I'll execute the solution step by step\"\"\"\n-        \n+\n         # Default intelligent response\n         return f\"I understand you're asking about: {prompt[:100]}... Let me help you with that.\"\n \n     async def complete(\n         self,\n@@ -158,11 +174,11 @@\n \n         self.call_count += 1\n \n         # Get the last message content\n         prompt = messages[-1].content if messages else \"empty prompt\"\n-        \n+\n         # Generate intelligent response based on prompt analysis\n         response_text = self._analyze_prompt_intent(prompt)\n \n         # Calculate mock usage\n         prompt_tokens = len(prompt.split()) + 10\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/cuda_provider.py\t2025-07-06 13:10:17.740437+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/cuda_provider.py\t2025-07-07 19:16:48.299023+00:00\n@@ -1,13 +1,13 @@\n \"\"\"\n CUDA Provider implementation\n \n-CUDA-accelerated provider using Hugging Face Transformers. This provider is \n-intended for workstations or servers equipped with NVIDIA GPUs and a functional \n-CUDA installation. It tries to load the requested model via transformers into \n-GPU memory. If CUDA is unavailable (e.g. running on a CPU-only machine) the \n-provider gracefully falls back to the OllamaProvider so that the remainder \n+CUDA-accelerated provider using Hugging Face Transformers. This provider is\n+intended for workstations or servers equipped with NVIDIA GPUs and a functional\n+CUDA installation. It tries to load the requested model via transformers into\n+GPU memory. If CUDA is unavailable (e.g. running on a CPU-only machine) the\n+provider gracefully falls back to the OllamaProvider so that the remainder\n of the pipeline keeps working.\n \n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n@@ -64,10 +64,11 @@\n             self._setup_cuda()\n         except Exception as exc:\n             logger.warning(f\"Failed to initialize CUDA: {exc}\")\n             if fallback_to_ollama:\n                 from .ollama_provider import OllamaProvider\n+\n                 self._fallback_provider = OllamaProvider(model=model)\n             else:\n                 raise\n \n     def _setup_cuda(self) -> None:\n@@ -89,14 +90,14 @@\n         self._model = AutoModelForCausalLM.from_pretrained(\n             self.model,\n             torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n             device_map=\"auto\" if self.device == \"cuda\" else None,\n         )\n-        \n+\n         if self.device != \"cuda\":\n             self._model = self._model.to(self.device)\n-            \n+\n         self._model.eval()\n \n     async def generate_response(\n         self,\n         prompt: str,\n@@ -177,22 +178,22 @@\n         if self._model is None or self._tokenizer is None:\n             raise LLMError(\"CUDA provider not initialized properly\")\n \n         # Convert messages to prompt\n         prompt = \"\\n\".join(msg.content for msg in messages)\n-        \n+\n         # Run generation in thread pool since it's synchronous\n         loop = asyncio.get_event_loop()\n         response = await loop.run_in_executor(\n             None,\n             self._generate_sync,\n             prompt,\n             max_tokens,\n             temperature,\n             kwargs,\n         )\n-        \n+\n         return response\n \n     def _generate_sync(\n         self,\n         prompt: str,\n@@ -200,13 +201,13 @@\n         temperature: float,\n         kwargs: Dict[str, Any],\n     ) -> LLMResponse:\n         \"\"\"Synchronous generation method for thread pool execution.\"\"\"\n         import torch\n-        \n+\n         start = time.perf_counter()\n-        \n+\n         # Tokenize input\n         inputs = self._tokenizer(prompt, return_tensors=\"pt\")\n         if self.device == \"cuda\":\n             inputs = {k: v.to(self.device) for k, v in inputs.items()}\n \n@@ -220,17 +221,17 @@\n         }\n \n         # Generate\n         with torch.no_grad():\n             output_ids = self._model.generate(**inputs, **generation_args)\n-        \n+\n         # Decode output\n         output_text = self._tokenizer.decode(\n-            output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n+            output_ids[0][inputs[\"input_ids\"].shape[-1] :],\n             skip_special_tokens=True,\n         )\n-        \n+\n         latency_ms = (time.perf_counter() - start) * 1000\n         tokens_used = output_ids.shape[-1]\n \n         return LLMResponse(\n             content=output_text.strip(),\n@@ -274,11 +275,11 @@\n             messages=messages,\n             max_tokens=max_tokens,\n             temperature=temperature,\n             **kwargs,\n         )\n-        \n+\n         # Yield response in chunks\n         words = response.content.split()\n         for word in words:\n             yield word + \" \"\n             await asyncio.sleep(0.01)\n@@ -309,10 +310,11 @@\n         Returns:\n             True if model can be loaded, False otherwise\n         \"\"\"\n         try:\n             from transformers import AutoConfig\n+\n             config = AutoConfig.from_pretrained(model)\n             return config is not None\n         except Exception:\n             return False\n \n@@ -322,13 +324,14 @@\n         Returns:\n             True if provider is operational, False otherwise\n         \"\"\"\n         if self._fallback_provider is not None:\n             return await self._fallback_provider.health_check()\n-        \n+\n         try:\n             import torch\n+\n             if self._model is None:\n                 return False\n             return torch.cuda.is_available() if self.device == \"cuda\" else True\n         except Exception:\n             return False\n@@ -343,6 +346,6 @@\n             completion_tokens: Number of output tokens\n \n         Returns:\n             Always returns 0.0 for local inference\n         \"\"\"\n-        return 0.0\n\\ No newline at end of file\n+        return 0.0\n--- /Users/nemesis/llamaagent/src/llamaagent/integration/langgraph.py\t2025-07-07 15:25:48.567035+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/integration/langgraph.py\t2025-07-07 19:16:48.278918+00:00\n@@ -106,13 +106,13 @@\n         # Execute using execute_task\n         output = await self.execute_task(task_input)\n \n         # Convert to AgentResponse\n         return AgentResponse(\n-            content=output.result.data.get(\"response\", \"\")\n-            if output.result.data\n-            else \"\",\n+            content=(\n+                output.result.data.get(\"response\", \"\") if output.result.data else \"\"\n+            ),\n             success=output.result.success,\n             metadata=output.result.metadata or {},\n         )\n \n     async def execute_task(self, task_input: TaskInput) -> TaskOutput:\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/together.py\t2025-07-06 13:10:17.737087+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/together.py\t2025-07-07 19:16:48.293834+00:00\n@@ -54,10 +54,11 @@\n     def _get_client(self) -> Any:\n         \"\"\"Get or create Together client.\"\"\"\n         if self._client is None:\n             try:\n                 from openai import AsyncOpenAI\n+\n                 # Together uses OpenAI-compatible API\n                 self._client = AsyncOpenAI(\n                     api_key=self.api_key,\n                     base_url=self.BASE_URL,\n                 )\n@@ -90,21 +91,20 @@\n         client = self._get_client()\n         model = model or self.model_name\n \n         # Convert messages to OpenAI format\n         openai_messages = [\n-            {\"role\": msg.role, \"content\": msg.content}\n-            for msg in messages\n+            {\"role\": msg.role, \"content\": msg.content} for msg in messages\n         ]\n \n         try:\n             response = await client.chat.completions.create(\n                 model=model,\n                 messages=openai_messages,\n                 max_tokens=max_tokens,\n                 temperature=temperature,\n-                **kwargs\n+                **kwargs,\n             )\n \n             # Extract usage information\n             usage = {\n                 \"prompt_tokens\": response.usage.prompt_tokens,\n@@ -163,22 +163,21 @@\n         client = self._get_client()\n         model = model or self.model_name\n \n         # Convert messages to OpenAI format\n         openai_messages = [\n-            {\"role\": msg.role, \"content\": msg.content}\n-            for msg in messages\n+            {\"role\": msg.role, \"content\": msg.content} for msg in messages\n         ]\n \n         try:\n             stream = await client.chat.completions.create(\n                 model=model,\n                 messages=openai_messages,\n                 max_tokens=max_tokens,\n                 temperature=temperature,\n                 stream=True,\n-                **kwargs\n+                **kwargs,\n             )\n \n             async for chunk in stream:\n                 if chunk.choices[0].delta.content:\n                     yield chunk.choices[0].delta.content\n@@ -188,11 +187,11 @@\n \n     def calculate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:\n         \"\"\"Calculate cost for Together usage.\"\"\"\n         # Get pricing for the model or use default\n         pricing = self.PRICING.get(self.model_name, self.PRICING[\"default\"])\n-        \n+\n         input_cost = (prompt_tokens / 1000) * pricing[\"input\"]\n         output_cost = (completion_tokens / 1000) * pricing[\"output\"]\n \n         return round(input_cost + output_cost, 6)\n \n@@ -204,12 +203,11 @@\n     async def health_check(self) -> bool:\n         \"\"\"Check if the provider is healthy.\"\"\"\n         try:\n             # Try a minimal completion\n             await self.complete(\n-                [LLMMessage(role=\"user\", content=\"test\")],\n-                max_tokens=10\n+                [LLMMessage(role=\"user\", content=\"test\")], max_tokens=10\n             )\n             return True\n         except Exception:\n             return False\n \n@@ -221,6 +219,6 @@\n             \"description\": \"Together AI - Open source model hosting\",\n             \"context_length\": 4096,  # Varies by model\n             \"supports_streaming\": True,\n             \"supports_embeddings\": False,\n             \"pricing\": self.PRICING.get(self.model_name, self.PRICING[\"default\"]),\n-        }\n\\ No newline at end of file\n+        }\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/openai_provider.py\t2025-07-07 14:13:14.161331+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/openai_provider.py\t2025-07-07 19:16:48.304430+00:00\n@@ -59,11 +59,13 @@\n         \"\"\"Get or create HTTP client.\"\"\"\n         async with self._lock:\n             if self._client is None or self._client.is_closed:\n                 self._client = httpx.AsyncClient(\n                     timeout=httpx.Timeout(self.timeout),\n-                    limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)\n+                    limits=httpx.Limits(\n+                        max_connections=10, max_keepalive_connections=5\n+                    ),\n                 )\n             return self._client\n \n     async def generate_response(\n         self,\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/openai.py\t2025-07-06 13:10:17.718787+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/openai.py\t2025-07-07 19:16:48.317570+00:00\n@@ -143,11 +143,11 @@\n                     # Add cost to usage dict\n                     if usage:\n                         usage[\"cost\"] = cost\n                     else:\n                         usage = {\"cost\": cost}\n-                    \n+\n                     return LLMResponse(\n                         content=content,\n                         model=model,\n                         provider=\"openai\",\n                         tokens_used=usage.get(\"total_tokens\", 0),\n@@ -279,11 +279,11 @@\n                     # Add cost to usage dict\n                     if usage:\n                         usage[\"cost\"] = cost\n                     else:\n                         usage = {\"cost\": cost}\n-                    \n+\n                     return {\n                         \"embeddings\": embeddings,\n                         \"model\": model,\n                         \"usage\": usage,\n                     }\n@@ -343,6 +343,6 @@\n             test_message = LLMMessage(role=\"user\", content=\"Hello\")\n             response = await self.complete([test_message], max_tokens=10)\n             return len(response.content) > 0\n         except Exception as e:\n             logger.error(f\"OpenAI health check failed: {e}\")\n-            return False\n\\ No newline at end of file\n+            return False\n--- /Users/nemesis/llamaagent/src/llamaagent/config/settings.py\t2025-07-06 13:10:17.858585+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/config/settings.py\t2025-07-07 19:16:48.307012+00:00\n@@ -20,60 +20,66 @@\n from pathlib import Path\n from typing import Any, Dict, List, Optional\n \n try:\n     import yaml\n+\n     YAML_AVAILABLE = True\n except ImportError:\n     YAML_AVAILABLE = False\n \n try:\n     from pydantic import BaseSettings, Field\n     from pydantic_settings import SettingsConfigDict\n+\n     PYDANTIC_AVAILABLE = True\n except ImportError:\n     # Fallback for older versions or missing pydantic\n     BaseSettings = object\n     PYDANTIC_AVAILABLE = False\n-    \n+\n     def Field(default, **kwargs):\n         return default\n-    \n+\n     SettingsConfigDict = dict\n \n logger = logging.getLogger(__name__)\n \n \n class LogLevel(str, Enum):\n     \"\"\"Supported log levels.\"\"\"\n+\n     DEBUG = \"DEBUG\"\n     INFO = \"INFO\"\n     WARNING = \"WARNING\"\n     ERROR = \"ERROR\"\n     CRITICAL = \"CRITICAL\"\n \n \n class LLMProvider(str, Enum):\n     \"\"\"Supported LLM providers.\"\"\"\n+\n     MOCK = \"mock\"\n     OPENAI = \"openai\"\n     ANTHROPIC = \"anthropic\"\n     OLLAMA = \"ollama\"\n     MLX = \"mlx\"\n \n \n class AgentRole(str, Enum):\n     \"\"\"Agent role types.\"\"\"\n+\n     GENERALIST = \"generalist\"\n     PLANNER = \"planner\"\n     EXECUTOR = \"executor\"\n     ANALYST = \"analyst\"\n \n \n @dataclass\n class LLMConfig:\n     \"\"\"LLM provider configuration.\"\"\"\n+\n     provider: LLMProvider = LLMProvider.MOCK\n     model: str = \"gpt-4o-mini\"\n     api_key: Optional[str] = None\n     base_url: Optional[str] = None\n     temperature: float = 0.7\n@@ -88,19 +94,20 @@\n \n \n @dataclass\n class AgentConfig:\n     \"\"\"Agent configuration.\"\"\"\n+\n     role: AgentRole = AgentRole.GENERALIST\n     max_iterations: int = 10\n     verbose: bool = False\n     debug: bool = False\n     spree_enabled: bool = True\n     tool_timeout: int = 30\n     memory_enabled: bool = True\n     reasoning_enabled: bool = True\n-    \n+\n     def __post_init__(self):\n         \"\"\"Post-initialization validation.\"\"\"\n         if self.max_iterations < 1:\n             raise ValueError(\"max_iterations must be positive\")\n         if self.tool_timeout < 1:\n@@ -108,10 +115,11 @@\n \n \n @dataclass\n class DatabaseConfig:\n     \"\"\"Database configuration.\"\"\"\n+\n     url: Optional[str] = None\n     pool_size: int = 10\n     max_overflow: int = 20\n     pool_timeout: int = 30\n     pool_recycle: int = 3600\n@@ -121,10 +129,11 @@\n \n \n @dataclass\n class SecurityConfig:\n     \"\"\"Security configuration.\"\"\"\n+\n     secret_key: str = \"development-secret-key-change-in-production\"\n     jwt_algorithm: str = \"HS256\"\n     jwt_expire_minutes: int = 30\n     api_key_expire_days: int = 365\n     rate_limit_enabled: bool = True\n@@ -136,10 +145,11 @@\n \n \n @dataclass\n class MonitoringConfig:\n     \"\"\"Monitoring and observability configuration.\"\"\"\n+\n     enable_metrics: bool = True\n     metrics_port: int = 9090\n     enable_tracing: bool = False\n     enable_health_checks: bool = True\n     log_level: LogLevel = LogLevel.INFO\n@@ -149,10 +159,11 @@\n \n \n @dataclass\n class CacheConfig:\n     \"\"\"Cache configuration.\"\"\"\n+\n     enabled: bool = True\n     backend: str = \"memory\"  # memory, redis, file\n     ttl: int = 3600  # seconds\n     max_size: int = 1000\n     redis_url: Optional[str] = None\n@@ -160,29 +171,33 @@\n \n \n @dataclass\n class APIConfig:\n     \"\"\"API server configuration.\"\"\"\n+\n     host: str = \"127.0.0.1\"\n     port: int = 8000\n     workers: int = 1\n     reload: bool = False\n     debug: bool = False\n     access_log: bool = True\n-    \n+\n     # Request limits\n     max_request_size: int = 10 * 1024 * 1024  # 10MB\n     request_timeout: float = 300.0\n     keepalive_timeout: float = 5.0\n \n \n @dataclass\n class StorageConfig:\n     \"\"\"Storage configuration.\"\"\"\n+\n     data_directory: str = \"./data\"\n     max_file_size: int = 100 * 1024 * 1024  # 100MB\n-    allowed_extensions: List[str] = field(default_factory=lambda: [\".txt\", \".json\", \".csv\"])\n+    allowed_extensions: List[str] = field(\n+        default_factory=lambda: [\".txt\", \".json\", \".csv\"]\n+    )\n     cleanup_interval: int = 3600  # seconds\n \n \n class LlamaAgentSettings:\n     \"\"\"Main configuration settings with environment variable support.\"\"\"\n@@ -221,55 +236,55 @@\n     def _load_from_env_fallback(self):\n         \"\"\"Fallback environment loading when pydantic is not available.\"\"\"\n         # LLM configuration\n         if provider := os.getenv(\"LLAMAAGENT_LLM__PROVIDER\"):\n             self.llm.provider = LLMProvider(provider)\n-        \n+\n         if model := os.getenv(\"LLAMAAGENT_LLM__MODEL\"):\n             self.llm.model = model\n-        \n+\n         if api_key := os.getenv(\"LLAMAAGENT_LLM__API_KEY\"):\n             self.llm.api_key = api_key\n-        \n+\n         if base_url := os.getenv(\"LLAMAAGENT_LLM__BASE_URL\"):\n             self.llm.base_url = base_url\n-        \n+\n         if temperature := os.getenv(\"LLAMAAGENT_LLM__TEMPERATURE\"):\n             try:\n                 self.llm.temperature = float(temperature)\n             except ValueError:\n                 logger.warning(f\"Invalid temperature: {temperature}\")\n-        \n+\n         # Agent configuration\n         if role := os.getenv(\"LLAMAAGENT_AGENT__ROLE\"):\n             self.agent.role = AgentRole(role)\n-        \n+\n         if max_iterations := os.getenv(\"LLAMAAGENT_AGENT__MAX_ITERATIONS\"):\n             try:\n                 self.agent.max_iterations = int(max_iterations)\n             except ValueError:\n                 logger.warning(f\"Invalid max_iterations: {max_iterations}\")\n-        \n+\n         if verbose := os.getenv(\"LLAMAAGENT_AGENT__VERBOSE\"):\n             self.agent.verbose = verbose.lower() in (\"true\", \"1\", \"yes\")\n-        \n+\n         if debug := os.getenv(\"LLAMAAGENT_DEBUG\"):\n             self.debug = debug.lower() in (\"true\", \"1\", \"yes\")\n             self.agent.debug = self.debug\n-        \n+\n         # Database configuration\n         if db_url := os.getenv(\"DATABASE_URL\"):\n             self.database.url = db_url\n-        \n+\n         # Security configuration\n         if secret := os.getenv(\"SECRET_KEY\"):\n             self.security.secret_key = secret\n-        \n+\n         # API configuration\n         if host := os.getenv(\"LLAMAAGENT_API__HOST\"):\n             self.api.host = host\n-        \n+\n         if port := os.getenv(\"LLAMAAGENT_API__PORT\"):\n             try:\n                 self.api.port = int(port)\n             except ValueError:\n                 logger.warning(f\"Invalid port: {port}\")\n@@ -282,23 +297,23 @@\n     @classmethod\n     def from_file(cls, config_path: str) -> 'LlamaAgentSettings':\n         \"\"\"Load configuration from file.\"\"\"\n         try:\n             config_path = Path(config_path)\n-            \n+\n             if not config_path.exists():\n                 logger.warning(f\"Config file not found: {config_path}\")\n                 return cls()\n-            \n+\n             with open(config_path, 'r') as f:\n                 if config_path.suffix in [\".yaml\", \".yml\"]:\n                     if not YAML_AVAILABLE:\n                         raise RuntimeError(\"YAML support not available\")\n                     config_data = yaml.safe_load(f)\n                 else:\n                     config_data = json.load(f)\n-            \n+\n             return cls.from_dict(config_data)\n         except Exception as e:\n             logger.error(f\"Failed to load config from {config_path}: {e}\")\n             return cls()\n \n@@ -306,32 +321,32 @@\n     def from_dict(cls, config_dict: Dict[str, Any]) -> 'LlamaAgentSettings':\n         \"\"\"Create settings from dictionary.\"\"\"\n         # Convert nested dicts to dataclass instances\n         if \"llm\" in config_dict:\n             config_dict[\"llm\"] = LLMConfig(**config_dict[\"llm\"])\n-        \n+\n         if \"agent\" in config_dict:\n             config_dict[\"agent\"] = AgentConfig(**config_dict[\"agent\"])\n-        \n+\n         if \"database\" in config_dict:\n             config_dict[\"database\"] = DatabaseConfig(**config_dict[\"database\"])\n-        \n+\n         if \"security\" in config_dict:\n             config_dict[\"security\"] = SecurityConfig(**config_dict[\"security\"])\n-        \n+\n         if \"monitoring\" in config_dict:\n             config_dict[\"monitoring\"] = MonitoringConfig(**config_dict[\"monitoring\"])\n-        \n+\n         if \"cache\" in config_dict:\n             config_dict[\"cache\"] = CacheConfig(**config_dict[\"cache\"])\n-        \n+\n         if \"api\" in config_dict:\n             config_dict[\"api\"] = APIConfig(**config_dict[\"api\"])\n-        \n+\n         if \"storage\" in config_dict:\n             config_dict[\"storage\"] = StorageConfig(**config_dict[\"storage\"])\n-        \n+\n         return cls(**config_dict)\n \n     def to_dict(self) -> Dict[str, Any]:\n         \"\"\"Convert settings to dictionary.\"\"\"\n         return {\n@@ -342,21 +357,21 @@\n             \"database\": self.database.__dict__,\n             \"security\": self.security.__dict__,\n             \"monitoring\": self.monitoring.__dict__,\n             \"cache\": self.cache.__dict__,\n             \"api\": self.api.__dict__,\n-            \"storage\": self.storage.__dict__\n+            \"storage\": self.storage.__dict__,\n         }\n \n     def to_file(self, config_path: str) -> None:\n         \"\"\"Save configuration to file.\"\"\"\n         config_path = Path(config_path)\n         config_dict = self.to_dict()\n-        \n+\n         # Create directory if it doesn't exist\n         config_path.parent.mkdir(parents=True, exist_ok=True)\n-        \n+\n         with open(config_path, 'w') as f:\n             if config_path.suffix in [\".yaml\", \".yml\"]:\n                 if not YAML_AVAILABLE:\n                     raise RuntimeError(\"YAML support not available\")\n                 yaml.dump(config_dict, f, default_flow_style=False)\n@@ -364,36 +379,36 @@\n                 json.dump(config_dict, f, indent=2)\n \n     def validate(self) -> List[str]:\n         \"\"\"Validate configuration and return list of errors.\"\"\"\n         errors = []\n-        \n+\n         # Validate LLM configuration\n         if (\n             self.llm.provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]\n             and not self.llm.api_key\n         ):\n             errors.append(f\"API key required for {self.llm.provider.value} provider\")\n-        \n+\n         if self.llm.temperature < 0 or self.llm.temperature > 2:\n             errors.append(\"Temperature must be between 0 and 2\")\n-        \n+\n         if self.llm.max_tokens < 1:\n             errors.append(\"Max tokens must be positive\")\n-        \n+\n         # Validate agent configuration\n         if self.agent.max_iterations < 1:\n             errors.append(\"Max iterations must be positive\")\n-        \n+\n         # Validate API configuration\n         if self.api.port < 1 or self.api.port > 65535:\n             errors.append(\"API port must be between 1 and 65535\")\n-        \n+\n         # Validate security configuration\n         if len(self.security.secret_key) < 16:\n             errors.append(\"Secret key must be at least 16 characters long\")\n-        \n+\n         return errors\n \n     def is_development(self) -> bool:\n         \"\"\"Check if running in development mode.\"\"\"\n         return self.environment == \"development\"\n@@ -413,32 +428,32 @@\n \n \n def load_settings(config_path: Optional[str] = None) -> LlamaAgentSettings:\n     \"\"\"Load settings from file or environment.\"\"\"\n     global settings\n-    \n+\n     if config_path:\n         settings = LlamaAgentSettings.from_file(config_path)\n     else:\n         # Try to load from default locations\n         default_paths = [\n             \"config.yaml\",\n             \"config.yml\",\n             \"config.json\",\n             \"llamaagent.yaml\",\n             \"llamaagent.yml\",\n-            \"llamaagent.json\"\n+            \"llamaagent.json\",\n         ]\n-        \n+\n         for path in default_paths:\n             if Path(path).exists():\n                 settings = LlamaAgentSettings.from_file(path)\n                 break\n         else:\n             # No config file found, use environment variables\n             settings = LlamaAgentSettings()\n-    \n+\n     return settings\n \n \n def update_settings(**kwargs) -> None:\n     \"\"\"Update global settings.\"\"\"\n@@ -499,6 +514,6 @@\n     return get_settings().cache\n \n \n def get_storage_config() -> StorageConfig:\n     \"\"\"Get storage configuration.\"\"\"\n-    return get_settings().storage\n\\ No newline at end of file\n+    return get_settings().storage\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/ollama_provider.py\t2025-07-06 13:10:17.722438+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/ollama_provider.py\t2025-07-07 19:16:48.333955+00:00\n@@ -172,11 +172,13 @@\n             raise ValueError(\"Messages list cannot be empty\")\n \n         # Prepare request payload\n         payload = {\n             \"model\": model or self.model,\n-            \"messages\": [{\"role\": msg.role, \"content\": msg.content} for msg in messages],\n+            \"messages\": [\n+                {\"role\": msg.role, \"content\": msg.content} for msg in messages\n+            ],\n             \"options\": {\n                 \"temperature\": temperature,\n                 \"num_predict\": max_tokens,\n                 **kwargs.get(\"options\", {}),\n             },\n@@ -264,11 +266,13 @@\n             String chunks of the response\n         \"\"\"\n         # Prepare request payload\n         payload = {\n             \"model\": model or self.model,\n-            \"messages\": [{\"role\": msg.role, \"content\": msg.content} for msg in messages],\n+            \"messages\": [\n+                {\"role\": msg.role, \"content\": msg.content} for msg in messages\n+            ],\n             \"options\": {\n                 \"temperature\": temperature,\n                 \"num_predict\": max_tokens,\n                 **kwargs.get(\"options\", {}),\n             },\n@@ -281,15 +285,16 @@\n                     \"POST\",\n                     f\"{self.base_url}/api/chat\",\n                     json=payload,\n                 ) as response:\n                     response.raise_for_status()\n-                    \n+\n                     async for line in response.aiter_lines():\n                         if line:\n                             try:\n                                 import json\n+\n                                 data = json.loads(line)\n                                 if \"message\" in data and \"content\" in data[\"message\"]:\n                                     content = data[\"message\"][\"content\"]\n                                     if content:\n                                         yield content\n@@ -412,6 +417,6 @@\n             completion_tokens: Number of output tokens\n \n         Returns:\n             Always returns 0.0 for local inference\n         \"\"\"\n-        return 0.0\n\\ No newline at end of file\n+        return 0.0\n--- /Users/nemesis/llamaagent/src/llamaagent/cli/master_cli.py\t2025-07-07 16:21:52.931780+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/master_cli.py\t2025-07-07 19:16:48.331527+00:00\n@@ -112,13 +112,15 @@\n             self.console.print(f\"  {i}. {provider}\")\n \n         provider_idx = (\n             IntPrompt.ask(\n                 \"Select provider\",\n-                default=providers.index(self.config[\"default_provider\"]) + 1\n-                if self.config[\"default_provider\"] in providers\n-                else 1,\n+                default=(\n+                    providers.index(self.config[\"default_provider\"]) + 1\n+                    if self.config[\"default_provider\"] in providers\n+                    else 1\n+                ),\n             )\n             - 1\n         )\n         provider = providers[provider_idx]\n \n@@ -175,11 +177,11 @@\n                     )\n                 )\n \n             self.session_history.append(\n                 {\"user\": message, \"agent\": response.content, \"time\": elapsed}\n-                )\n+            )\n \n         if Confirm.ask(\"\\nSave conversation?\"):\n             self._save_conversation()\n \n     def advanced_reasoning(self):\n--- /Users/nemesis/llamaagent/src/llamaagent/agents/react.py\t2025-07-07 16:20:28.278489+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/react.py\t2025-07-07 19:16:48.337801+00:00\n@@ -99,11 +99,13 @@\n         self._initialize_storage()\n \n     def _initialize_llm_provider(self, llm_provider: Any | None = None) -> Any:\n         \"\"\"Initialize LLM provider with proper fallback logic.\"\"\"\n         if llm_provider is not None:\n-            logger.info(f\"Using provided LLM provider: {llm_provider.__class__.__name__}\")\n+            logger.info(\n+                f\"Using provided LLM provider: {llm_provider.__class__.__name__}\"\n+            )\n             return self._wrap_provider(llm_provider)\n \n         provider_type = os.getenv(\"LLAMAAGENT_LLM_PROVIDER\", \"mock\").lower()\n         create_kwargs: Dict[str, Any] = {}\n \n@@ -114,16 +116,20 @@\n \n         # Configure provider-specific settings\n         if provider_type == \"openai\":\n             api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n             if not api_key or (api_key and api_key.startswith(\"your_api_\")):\n-                raise ValueError(\"OpenAI API key not properly configured. Set OPENAI_API_KEY environment variable.\")\n+                raise ValueError(\n+                    \"OpenAI API key not properly configured. Set OPENAI_API_KEY environment variable.\"\n+                )\n             create_kwargs[\"api_key\"] = api_key\n         elif provider_type == \"anthropic\":\n             api_key = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n             if not api_key or (api_key and api_key.startswith(\"your_api_\")):\n-                raise ValueError(\"Anthropic API key not properly configured. Set ANTHROPIC_API_KEY environment variable.\")\n+                raise ValueError(\n+                    \"Anthropic API key not properly configured. Set ANTHROPIC_API_KEY environment variable.\"\n+                )\n             create_kwargs[\"api_key\"] = api_key\n         elif provider_type == \"ollama\":\n             api_key = os.getenv(\"OLLAMA_API_KEY\")\n             if api_key:\n                 create_kwargs[\"api_key\"] = api_key\n@@ -131,11 +137,13 @@\n             if base_url:\n                 create_kwargs[\"base_url\"] = base_url\n \n         # Create mock provider if explicitly requested\n         if provider_type == \"mock\":\n-            logger.info(\"Using MockProvider for testing/development. Set LLAMAAGENT_LLM_PROVIDER to 'openai' or 'anthropic' for production use.\")\n+            logger.info(\n+                \"Using MockProvider for testing/development. Set LLAMAAGENT_LLM_PROVIDER to 'openai' or 'anthropic' for production use.\"\n+            )\n             return self._wrap_provider(MockProvider())\n \n         logger.info(f\"Initializing {provider_type} provider...\")\n         provider = create_provider(provider_type, **create_kwargs)\n         logger.info(f\"Successfully initialized {provider_type} provider\")\n@@ -211,13 +219,15 @@\n \n     def _initialize_storage(self) -> None:\n         \"\"\"Initialize storage components.\"\"\"\n         try:\n             # Initialize database manager with config if available\n-            if (self.config.metadata and \n-                \"storage\" in self.config.metadata and \n-                isinstance(self.config.metadata[\"storage\"], dict)):\n+            if (\n+                self.config.metadata\n+                and \"storage\" in self.config.metadata\n+                and isinstance(self.config.metadata[\"storage\"], dict)\n+            ):\n                 # Use storage config from metadata\n                 self._db = DatabaseManager()\n             else:\n                 # Use default database manager\n                 self._db = DatabaseManager()\n@@ -282,11 +292,13 @@\n         self.add_trace(\n             \"task_start\",\n             {\n                 \"task\": task,\n                 \"context\": context,\n-                \"spree_enabled\": (self.config.metadata or {}).get(\"spree_enabled\", False),\n+                \"spree_enabled\": (self.config.metadata or {}).get(\n+                    \"spree_enabled\", False\n+                ),\n             },\n         )\n \n         try:\n             # Fast-path for simple arithmetic (unit test compatibility)\n--- /Users/nemesis/llamaagent/src/llamaagent/research/evidence.py\t2025-07-06 13:11:35.640557+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/evidence.py\t2025-07-07 19:16:48.342149+00:00\n@@ -3,26 +3,29 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n from dataclasses import dataclass, field\n \n+\n @dataclass\n class Evidence:\n     \"\"\"Evidence data structure.\"\"\"\n+\n     content: str\n     source: str\n     confidence: float = 0.0\n     metadata: Dict[str, Any] = field(default_factory=dict)\n \n+\n class EvidenceManager:\n     \"\"\"Manages evidence for research.\"\"\"\n-    \n+\n     def __init__(self):\n         self.evidence_list = []\n-    \n+\n     def add_evidence(self, evidence: Evidence) -> None:\n         \"\"\"Add evidence.\"\"\"\n         self.evidence_list.append(evidence)\n-    \n+\n     def get_evidence(self) -> List[Evidence]:\n         \"\"\"Get all evidence.\"\"\"\n         return self.evidence_list\n--- /Users/nemesis/llamaagent/src/llamaagent/research/citations.py\t2025-07-06 13:11:07.157821+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/citations.py\t2025-07-07 19:16:48.341305+00:00\n@@ -3,26 +3,29 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n from dataclasses import dataclass, field\n \n+\n @dataclass\n class Citation:\n     \"\"\"Citation data structure.\"\"\"\n+\n     title: str\n     authors: List[str] = field(default_factory=list)\n     year: Optional[int] = None\n     url: Optional[str] = None\n-    \n+\n+\n class CitationManager:\n     \"\"\"Manages citations for research.\"\"\"\n-    \n+\n     def __init__(self):\n         self.citations = []\n-    \n+\n     def add_citation(self, citation: Citation) -> None:\n         \"\"\"Add a citation.\"\"\"\n         self.citations.append(citation)\n-    \n+\n     def get_citations(self) -> List[Citation]:\n         \"\"\"Get all citations.\"\"\"\n         return self.citations\n--- /Users/nemesis/llamaagent/src/llamaagent/research/literature_review.py\t2025-07-06 13:11:07.158067+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/literature_review.py\t2025-07-07 19:16:48.346586+00:00\n@@ -3,26 +3,29 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n from dataclasses import dataclass, field\n \n+\n @dataclass\n class Paper:\n     \"\"\"Research paper data structure.\"\"\"\n+\n     title: str\n     abstract: str\n     authors: List[str] = field(default_factory=list)\n     year: Optional[int] = None\n \n+\n class LiteratureReviewer:\n     \"\"\"Literature review engine.\"\"\"\n-    \n+\n     def __init__(self):\n         self.papers = []\n-    \n+\n     def add_paper(self, paper: Paper) -> None:\n         \"\"\"Add a paper to review.\"\"\"\n         self.papers.append(paper)\n-    \n+\n     def generate_review(self) -> str:\n         \"\"\"Generate a literature review.\"\"\"\n         return f'Literature review of {len(self.papers)} papers.'\n--- /Users/nemesis/llamaagent/src/llamaagent/research/knowledge_graph.py\t2025-07-06 13:11:07.157940+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/knowledge_graph.py\t2025-07-07 19:16:48.346997+00:00\n@@ -3,26 +3,29 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n from dataclasses import dataclass, field\n \n+\n @dataclass\n class KnowledgeNode:\n     \"\"\"Knowledge graph node.\"\"\"\n+\n     id: str\n     content: str\n     metadata: Dict[str, Any] = field(default_factory=dict)\n \n+\n class KnowledgeGraph:\n     \"\"\"Knowledge graph implementation.\"\"\"\n-    \n+\n     def __init__(self):\n         self.nodes = {}\n         self.edges = []\n-    \n+\n     def add_node(self, node: KnowledgeNode) -> None:\n         \"\"\"Add a node to the graph.\"\"\"\n         self.nodes[node.id] = node\n-    \n+\n     def get_node(self, node_id: str) -> Optional[KnowledgeNode]:\n         \"\"\"Get a node by ID.\"\"\"\n         return self.nodes.get(node_id)\n--- /Users/nemesis/llamaagent/src/llamaagent/research/scientific_reasoning.py\t2025-07-06 13:11:07.158002+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/scientific_reasoning.py\t2025-07-07 19:16:48.347690+00:00\n@@ -3,25 +3,28 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n from dataclasses import dataclass, field\n \n+\n @dataclass\n class ScientificClaim:\n     \"\"\"Scientific claim data structure.\"\"\"\n+\n     claim: str\n     evidence: List[str] = field(default_factory=list)\n     confidence: float = 0.0\n \n+\n class ScientificReasoner:\n     \"\"\"Scientific reasoning engine.\"\"\"\n-    \n+\n     def __init__(self):\n         self.claims = []\n-    \n+\n     def analyze_claim(self, claim: str) -> ScientificClaim:\n         \"\"\"Analyze a scientific claim.\"\"\"\n         return ScientificClaim(claim=claim, confidence=0.5)\n-    \n+\n     def validate_evidence(self, evidence: List[str]) -> bool:\n         \"\"\"Validate evidence for claims.\"\"\"\n         return len(evidence) > 0\n--- /Users/nemesis/llamaagent/src/llamaagent/research/report_generator.py\t2025-07-06 13:11:35.640445+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/report_generator.py\t2025-07-07 19:16:48.347926+00:00\n@@ -3,28 +3,31 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n from dataclasses import dataclass, field\n \n+\n @dataclass\n class Report:\n     \"\"\"Research report data structure.\"\"\"\n+\n     title: str\n     content: str\n     sections: List[str] = field(default_factory=list)\n     figures: List[Dict[str, Any]] = field(default_factory=list)\n \n+\n class ReportGenerator:\n     \"\"\"Generates research reports.\"\"\"\n-    \n+\n     def __init__(self):\n         self.reports = []\n-    \n+\n     def create_report(self, title: str, content: str) -> Report:\n         \"\"\"Create a new report.\"\"\"\n         report = Report(title=title, content=content)\n         self.reports.append(report)\n         return report\n-    \n+\n     def add_figure(self, report: Report, figure: Dict[str, Any]) -> None:\n         \"\"\"Add a figure to a report.\"\"\"\n         report.figures.append(figure)\n--- /Users/nemesis/llamaagent/src/llamaagent/tools/plugin_framework.py\t2025-07-06 13:10:17.688504+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/tools/plugin_framework.py\t2025-07-07 19:16:48.373029+00:00\n@@ -2,22 +2,23 @@\n Plugin framework implementation.\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n \n+\n class PluginFramework:\n     \"\"\"Plugin framework for dynamic tool loading.\"\"\"\n-    \n+\n     def __init__(self):\n         self.plugins = {}\n-    \n+\n     def load_plugin(self, plugin_path: str) -> bool:\n         \"\"\"Load a plugin.\"\"\"\n         return True\n-    \n+\n     def get_plugin(self, name: str) -> Optional[Any]:\n         \"\"\"Get a loaded plugin.\"\"\"\n         return self.plugins.get(name)\n-    \n+\n     def list_plugins(self) -> List[str]:\n         \"\"\"List loaded plugins.\"\"\"\n         return list(self.plugins.keys())\n--- /Users/nemesis/llamaagent/src/llamaagent/agents/advanced_reasoning.py\t2025-07-07 16:15:26.278208+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/advanced_reasoning.py\t2025-07-07 19:16:48.375086+00:00\n@@ -16,10 +16,11 @@\n \n from ..llm import LLMMessage, LLMProvider, LLMResponse\n from ..tools import ToolRegistry\n from .base import AgentConfig\n from .base import BaseAgent as Agent\n+\n logger = logging.getLogger(__name__)\n \n \n class ReasoningStrategy(Enum):\n     \"\"\"Advanced reasoning strategies.\"\"\"\n@@ -700,13 +701,15 @@\n             ]\n         )\n \n         return ThoughtNode(\n             content=response.content,\n-            confidence=sum(t.confidence for t in sub_thoughts) / len(sub_thoughts)\n-            if sub_thoughts\n-            else 0.7,\n+            confidence=(\n+                sum(t.confidence for t in sub_thoughts) / len(sub_thoughts)\n+                if sub_thoughts\n+                else 0.7\n+            ),\n             reasoning_type=\"combined\",\n         )\n \n     async def _generate_thought_branches(\n         self, problem: str, parent: ThoughtNode, context: Optional[Dict[str, Any]]\n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/logging.py\t2025-07-07 15:05:10.449787+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/logging.py\t2025-07-07 19:16:48.366839+00:00\n@@ -19,19 +19,20 @@\n from pathlib import Path\n from typing import Any, Callable, Dict, List, Optional\n \n try:\n     import psutil\n+\n     PSUTIL_AVAILABLE = True\n except ImportError:\n     PSUTIL_AVAILABLE = False\n \n \n @dataclass\n class LogContext:\n     \"\"\"Context information for structured logging.\"\"\"\n-    \n+\n     trace_id: Optional[str] = None\n     span_id: Optional[str] = None\n     user_id: Optional[str] = None\n     session_id: Optional[str] = None\n     request_id: Optional[str] = None\n@@ -49,11 +50,11 @@\n log_context: ContextVar[Dict[str, Any]] = ContextVar(\"log_context\", default={})\n \n \n class StructuredFormatter(logging.Formatter):\n     \"\"\"Custom formatter for structured JSON logging.\"\"\"\n-    \n+\n     def __init__(self, include_traceback: bool = True):\n         super().__init__()\n         self.include_traceback = include_traceback\n \n     def format(self, record: logging.LogRecord) -> str:\n@@ -89,21 +90,21 @@\n         return json.dumps(log_data, default=str)\n \n \n class StructuredLogger:\n     \"\"\"Structured logger with context support.\"\"\"\n-    \n+\n     def __init__(self, name: str, level: str = \"INFO\"):\n         self.logger = logging.getLogger(name)\n         self.logger.setLevel(getattr(logging, level.upper()))\n \n     def _log(self, level: int, message: str, **kwargs) -> None:\n         \"\"\"Internal log method.\"\"\"\n         # Merge context\n         ctx = log_context.get().copy()\n         ctx.update(kwargs)\n-        \n+\n         # Create log record with extra fields\n         self.logger.log(level, message, extra=ctx)\n \n     def debug(self, message: str, **kwargs) -> None:\n         \"\"\"Log debug message.\"\"\"\n@@ -130,24 +131,24 @@\n         return ContextualLogger(self.logger, LogContext(**kwargs))\n \n \n class ContextualLogger:\n     \"\"\"Logger with context.\"\"\"\n-    \n+\n     def __init__(self, logger: logging.Logger, context: LogContext) -> None:\n         self.logger = logger\n         self.context = context\n \n     def __enter__(self):\n         # Store previous context\n         self._previous_context = log_context.get()\n-        \n+\n         # Merge contexts\n         new_context = self._previous_context.copy()\n         new_context.update(self.context.to_dict())\n         log_context.set(new_context)\n-        \n+\n         return self\n \n     def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n         # Restore previous context\n         log_context.set(self._previous_context)\n@@ -172,52 +173,52 @@\n     operation_name: str,\n     include_memory: bool = False,\n     level: int = logging.INFO,\n ) -> Callable:\n     \"\"\"Decorator for logging performance of operations.\"\"\"\n-    \n+\n     def decorator(func: Callable) -> Callable:\n         @functools.wraps(func)\n         async def async_wrapper(*args, **kwargs):\n             start_time = time.time()\n             logger = logging.getLogger(func.__module__)\n-            \n+\n             # Collect initial metrics\n             initial_memory = None\n             if include_memory and PSUTIL_AVAILABLE:\n                 try:\n                     process = psutil.Process()\n                     initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n                 except Exception:\n                     pass\n-            \n+\n             try:\n                 result = await func(*args, **kwargs)\n                 duration = time.time() - start_time\n-                \n+\n                 # Collect final metrics\n                 extra = {\n                     \"operation\": operation_name,\n                     \"duration_seconds\": duration,\n                     \"status\": \"success\",\n                 }\n-                \n+\n                 if initial_memory is not None and PSUTIL_AVAILABLE:\n                     try:\n                         process = psutil.Process()\n                         final_memory = process.memory_info().rss / 1024 / 1024\n                         extra[\"memory_usage_mb\"] = final_memory - initial_memory\n                     except Exception:\n                         pass\n-                \n+\n                 logger.log(\n                     level,\n                     f\"Operation '{operation_name}' completed in {duration:.3f}s\",\n                     extra=extra,\n                 )\n                 return result\n-                \n+\n             except Exception as e:\n                 duration = time.time() - start_time\n                 logger.error(\n                     f\"Operation '{operation_name}' failed after {duration:.3f}s\",\n                     extra={\n@@ -232,27 +233,27 @@\n \n         @functools.wraps(func)\n         def sync_wrapper(*args, **kwargs):\n             start_time = time.time()\n             logger = logging.getLogger(func.__module__)\n-            \n+\n             # Similar implementation for sync functions\n             try:\n                 result = func(*args, **kwargs)\n                 duration = time.time() - start_time\n-                \n+\n                 logger.log(\n                     level,\n                     f\"Operation '{operation_name}' completed in {duration:.3f}s\",\n                     extra={\n                         \"operation\": operation_name,\n                         \"duration_seconds\": duration,\n                         \"status\": \"success\",\n                     },\n                 )\n                 return result\n-                \n+\n             except Exception as e:\n                 duration = time.time() - start_time\n                 logger.error(\n                     f\"Operation '{operation_name}' failed after {duration:.3f}s\",\n                     extra={\n@@ -277,16 +278,16 @@\n @contextmanager\n def log_context_manager(**kwargs):\n     \"\"\"Context manager for logging context.\"\"\"\n     context = LogContext(**kwargs)\n     previous_context = log_context.get()\n-    \n+\n     # Merge contexts\n     new_context = previous_context.copy()\n     new_context.update(context.to_dict())\n     log_context.set(new_context)\n-    \n+\n     try:\n         yield\n     finally:\n         log_context.set(previous_context)\n \n@@ -320,14 +321,14 @@\n         root_logger.addHandler(console_handler)\n \n     # File handler\n     if enable_file and file_path:\n         from logging.handlers import RotatingFileHandler\n-        \n+\n         # Create directory if needed\n         Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n-        \n+\n         file_handler = RotatingFileHandler(\n             file_path,\n             maxBytes=max_bytes,\n             backupCount=backup_count,\n         )\n@@ -361,6 +362,6 @@\n     \"LogContext\",\n     \"log_performance\",\n     \"log_context_manager\",\n     \"configure_logging\",\n     \"get_logger\",\n-]\n\\ No newline at end of file\n+]\n--- /Users/nemesis/llamaagent/tests/integration/test_api.py\t2025-07-06 13:01:44.937842+00:00\n+++ /Users/nemesis/llamaagent/tests/integration/test_api.py\t2025-07-07 19:16:48.395967+00:00\n@@ -9,52 +9,49 @@\n from src.llamaagent.api.main import app\n \n \n class TestAPI:\n     \"\"\"Test suite for API endpoints.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Setup test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_health_check(self):\n         \"\"\"Test health check endpoint.\"\"\"\n         response = self.client.get(\"/health\")\n         assert response.status_code == 200\n         assert \"status\" in response.json()\n-    \n+\n     def test_agent_creation(self):\n         \"\"\"Test agent creation endpoint.\"\"\"\n         agent_data = {\n             \"agent_name\": \"TestAgent\",\n             \"llm_provider\": \"mock\",\n-            \"metadata\": {\"spree_enabled\": False}\n+            \"metadata\": {\"spree_enabled\": False},\n         }\n         response = self.client.post(\"/agents\", json=agent_data)\n         assert response.status_code == 200\n         assert \"agent_id\" in response.json()\n-    \n+\n     def test_task_processing(self):\n         \"\"\"Test task processing endpoint.\"\"\"\n         # First create an agent\n         agent_data = {\n             \"agent_name\": \"TestAgent\",\n             \"llm_provider\": \"mock\",\n-            \"metadata\": {\"spree_enabled\": False}\n+            \"metadata\": {\"spree_enabled\": False},\n         }\n         agent_response = self.client.post(\"/agents\", json=agent_data)\n         agent_id = agent_response.json()[\"agent_id\"]\n-        \n+\n         # Then process a task\n-        task_data = {\n-            \"task\": \"Test task\",\n-            \"agent_id\": agent_id\n-        }\n+        task_data = {\"task\": \"Test task\", \"agent_id\": agent_id}\n         response = self.client.post(\"/tasks\", json=task_data)\n         assert response.status_code == 200\n         assert \"result\" in response.json()\n-    \n+\n     def test_error_handling(self):\n         \"\"\"Test API error handling.\"\"\"\n         # Test with invalid agent data\n         invalid_data = {\"invalid\": \"data\"}\n         response = self.client.post(\"/agents\", json=invalid_data)\n--- /Users/nemesis/llamaagent/src/llamaagent/api/complete_api.py\t2025-07-07 19:12:59.797541+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/api/complete_api.py\t2025-07-07 19:16:48.379194+00:00\n@@ -104,42 +104,50 @@\n \n \n # Pydantic Models\n class HealthResponse(BaseModel):\n     \"\"\"Health check response model\"\"\"\n-    \n+\n     status: str\n     timestamp: str\n     version: str\n     uptime: float\n     metrics: Dict[str, Any]\n     services: Dict[str, bool]\n \n \n class SPREGenerationRequest(BaseModel):\n     \"\"\"SPRE generation request model\"\"\"\n-    \n+\n     name: str = Field(..., description=\"Dataset name\")\n-    count: int = Field(default=10, ge=1, le=1000, description=\"Number of items to generate\")\n+    count: int = Field(\n+        default=10, ge=1, le=1000, description=\"Number of items to generate\"\n+    )\n     description: str = Field(default=\"\", description=\"Dataset description\")\n-    data_type: Optional[DataType] = Field(default=DataType.TEXT, description=\"Data type to generate\")\n+    data_type: Optional[DataType] = Field(\n+        default=DataType.TEXT, description=\"Data type to generate\"\n+    )\n     topic: Optional[str] = Field(default=None, description=\"Topic for generation\")\n     difficulty: Optional[str] = Field(default=\"medium\", description=\"Difficulty level\")\n-    style: Optional[str] = Field(default=\"default\", description=\"Style for creative content\")\n-    domain: Optional[str] = Field(default=\"general\", description=\"Domain for technical content\")\n+    style: Optional[str] = Field(\n+        default=\"default\", description=\"Style for creative content\"\n+    )\n+    domain: Optional[str] = Field(\n+        default=\"general\", description=\"Domain for technical content\"\n+    )\n     tags: Optional[List[str]] = Field(default=[], description=\"Tags for the dataset\")\n-    \n+\n     @validator('count')\n     def validate_count(cls, v):\n         if v < 1 or v > 1000:\n             raise ValueError('Count must be between 1 and 1000')\n         return v\n \n \n class AgentCreationRequest(BaseModel):\n     \"\"\"Agent creation request model\"\"\"\n-    \n+\n     name: str = Field(..., description=\"Agent name\")\n     role: AgentRole = Field(default=AgentRole.GENERALIST, description=\"Agent role\")\n     description: str = Field(default=\"\", description=\"Agent description\")\n     llm_provider: str = Field(default=\"mock\", description=\"LLM provider to use\")\n     tools: Optional[List[str]] = Field(default=[], description=\"Tools to enable\")\n@@ -147,30 +155,38 @@\n     debug: bool = Field(default=False, description=\"Enable debug mode\")\n \n \n class AgentExecutionRequest(BaseModel):\n     \"\"\"Agent execution request model\"\"\"\n-    \n+\n     message: str = Field(..., description=\"Message to send to agent\")\n-    context: Optional[Dict[str, Any]] = Field(default={}, description=\"Additional context\")\n+    context: Optional[Dict[str, Any]] = Field(\n+        default={}, description=\"Additional context\"\n+    )\n     stream: bool = Field(default=False, description=\"Enable streaming response\")\n-    timeout: int = Field(default=300, ge=10, le=3600, description=\"Execution timeout in seconds\")\n+    timeout: int = Field(\n+        default=300, ge=10, le=3600, description=\"Execution timeout in seconds\"\n+    )\n \n \n class DataGenerationFromPromptsRequest(BaseModel):\n     \"\"\"Data generation from prompts request model\"\"\"\n-    \n+\n     prompts: List[str] = Field(..., description=\"List of prompts to process\")\n     output_format: str = Field(default=\"json\", description=\"Output format\")\n-    batch_size: int = Field(default=10, ge=1, le=100, description=\"Batch processing size\")\n+    batch_size: int = Field(\n+        default=10, ge=1, le=100, description=\"Batch processing size\"\n+    )\n \n \n class FileProcessingRequest(BaseModel):\n     \"\"\"File processing request model\"\"\"\n-    \n+\n     operation: str = Field(..., description=\"Processing operation\")\n-    parameters: Optional[Dict[str, Any]] = Field(default={}, description=\"Operation parameters\")\n+    parameters: Optional[Dict[str, Any]] = Field(\n+        default={}, description=\"Operation parameters\"\n+    )\n \n \n # Rate limiting middleware\n class RateLimitMiddleware(BaseHTTPMiddleware):\n     def __init__(self, app, calls: int = 100, period: int = 60):\n@@ -180,44 +196,46 @@\n         self.clients = {}\n \n     async def dispatch(self, request: Request, call_next):\n         client_ip = request.client.host\n         now = time.time()\n-        \n+\n         if client_ip not in self.clients:\n             self.clients[client_ip] = []\n-        \n+\n         # Clean old requests\n         self.clients[client_ip] = [\n-            req_time for req_time in self.clients[client_ip]\n+            req_time\n+            for req_time in self.clients[client_ip]\n             if now - req_time < self.period\n         ]\n-        \n+\n         # Check rate limit\n         if len(self.clients[client_ip]) >= self.calls:\n             return JSONResponse(\n-                status_code=429,\n-                content={\"detail\": \"Rate limit exceeded\"}\n+                status_code=429, content={\"detail\": \"Rate limit exceeded\"}\n             )\n-        \n+\n         # Add current request\n         self.clients[client_ip].append(now)\n-        \n+\n         response = await call_next(request)\n         return response\n \n \n # Add rate limiting\n app.add_middleware(RateLimitMiddleware, calls=100, period=60)\n \n \n # Dependency functions\n-def get_current_user(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)):\n+def get_current_user(\n+    credentials: Optional[HTTPAuthorizationCredentials] = Depends(security),\n+):\n     \"\"\"Get current user from authorization token\"\"\"\n     if not credentials:\n         return {\"user_id\": \"anonymous\", \"permissions\": [\"read\"]}\n-    \n+\n     # TODO: Implement proper JWT token validation\n     return {\"user_id\": \"authenticated\", \"permissions\": [\"read\", \"write\", \"admin\"]}\n \n \n def update_metrics():\n@@ -228,25 +246,25 @@\n # Health and Status Endpoints\n @app.get(\"/health\", response_model=HealthResponse)\n async def health_check():\n     \"\"\"Comprehensive health check endpoint\"\"\"\n     uptime = time.time() - app_state[\"metrics\"][\"uptime_start\"]\n-    \n+\n     # Check services\n     services = {\n         \"database\": True,  # TODO: Actual database check\n-        \"redis\": True,     # TODO: Actual redis check\n+        \"redis\": True,  # TODO: Actual redis check\n         \"llm_providers\": True,  # TODO: Check LLM providers\n     }\n-    \n+\n     return HealthResponse(\n         status=\"healthy\" if all(services.values()) else \"degraded\",\n         timestamp=datetime.now(timezone.utc).isoformat(),\n         version=\"1.0.0\",\n         uptime=uptime,\n         metrics=app_state[\"metrics\"],\n-        services=services\n+        services=services,\n     )\n \n \n @app.get(\"/metrics\")\n async def get_metrics():\n@@ -271,47 +289,47 @@\n         },\n         \"system_info\": {\n             \"python_version\": \"3.11\",\n             \"environment\": os.getenv(\"LLAMAAGENT_ENV\", \"development\"),\n             \"log_level\": os.getenv(\"LOG_LEVEL\", \"INFO\"),\n-        }\n+        },\n     }\n \n \n # SPRE Data Generation Endpoints\n @app.post(\"/spre/generate\")\n async def generate_spre_data(\n     request: SPREGenerationRequest,\n     background_tasks: BackgroundTasks,\n-    user=Depends(get_current_user)\n+    user=Depends(get_current_user),\n ):\n     \"\"\"Generate SPRE dataset\"\"\"\n     if \"write\" not in user[\"permissions\"]:\n         raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n-    \n+\n     try:\n         # Create generator\n         generator = SPREGenerator()\n         session_id = str(uuid4())\n         app_state[\"spre_generators\"][session_id] = generator\n-        \n+\n         # Generate dataset\n         dataset = generator.generate_dataset(\n             name=request.name,\n             count=request.count,\n             description=request.description,\n             data_type=request.data_type,\n             topic=request.topic,\n             difficulty=request.difficulty,\n             style=request.style,\n             domain=request.domain,\n-            tags=request.tags\n+            tags=request.tags,\n         )\n-        \n+\n         # Update metrics\n         app_state[\"metrics\"][\"generations_count\"] += 1\n-        \n+\n         # Convert to response format\n         response_data = {\n             \"session_id\": session_id,\n             \"dataset\": {\n                 \"name\": dataset.name,\n@@ -328,51 +346,54 @@\n                         \"validation_status\": item.validation_status.value,\n                         \"tags\": item.tags,\n                         \"quality_score\": item.quality_score,\n                     }\n                     for item in dataset.items\n-                ]\n+                ],\n             },\n             \"statistics\": {\n                 \"total_items\": len(dataset.items),\n                 \"valid_items\": len(dataset.get_valid_items()),\n-                \"validation_rate\": len(dataset.get_valid_items()) / len(dataset.items) if dataset.items else 0,\n-            }\n+                \"validation_rate\": (\n+                    len(dataset.get_valid_items()) / len(dataset.items)\n+                    if dataset.items\n+                    else 0\n+                ),\n+            },\n         }\n-        \n+\n         return response_data\n-        \n+\n     except Exception as e:\n         logger.error(f\"SPRE generation failed: {e}\")\n         raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n \n \n @app.post(\"/spre/generate-from-prompts\")\n async def generate_from_prompts(\n-    request: DataGenerationFromPromptsRequest,\n-    user=Depends(get_current_user)\n+    request: DataGenerationFromPromptsRequest, user=Depends(get_current_user)\n ):\n     \"\"\"Generate data from list of prompts\"\"\"\n     if \"write\" not in user[\"permissions\"]:\n         raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n-    \n+\n     try:\n         generator = SPREGenerator()\n-        \n+\n         # Process prompts in batches\n         all_results = []\n         for i in range(0, len(request.prompts), request.batch_size):\n-            batch = request.prompts[i:i + request.batch_size]\n+            batch = request.prompts[i : i + request.batch_size]\n             batch_results = await generator.generate_from_prompts(batch)\n             all_results.extend(batch_results)\n-        \n+\n         return {\n             \"total_prompts\": len(request.prompts),\n             \"generated_items\": len(all_results),\n-            \"results\": all_results\n+            \"results\": all_results,\n         }\n-        \n+\n     except Exception as e:\n         logger.error(f\"Prompt generation failed: {e}\")\n         raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n \n \n@@ -381,11 +402,15 @@\n     \"\"\"List active SPRE generators\"\"\"\n     return {\n         \"generators\": [\n             {\n                 \"session_id\": session_id,\n-                \"stats\": generator.get_dataset_stats() if hasattr(generator, 'get_dataset_stats') else {}\n+                \"stats\": (\n+                    generator.get_dataset_stats()\n+                    if hasattr(generator, 'get_dataset_stats')\n+                    else {}\n+                ),\n             }\n             for session_id, generator in app_state[\"spre_generators\"].items()\n         ]\n     }\n \n@@ -393,70 +418,63 @@\n @app.get(\"/spre/generators/{session_id}/stats\")\n async def get_generator_stats(session_id: str, user=Depends(get_current_user)):\n     \"\"\"Get statistics for a specific generator\"\"\"\n     if session_id not in app_state[\"spre_generators\"]:\n         raise HTTPException(status_code=404, detail=\"Generator not found\")\n-    \n+\n     generator = app_state[\"spre_generators\"][session_id]\n-    \n+\n     if hasattr(generator, 'get_dataset_stats'):\n         return generator.get_dataset_stats()\n     else:\n         return {\"error\": \"Stats not available\"}\n \n \n # Agent Management Endpoints\n @app.post(\"/agents/create\")\n-async def create_agent(\n-    request: AgentCreationRequest,\n-    user=Depends(get_current_user)\n-):\n+async def create_agent(request: AgentCreationRequest, user=Depends(get_current_user)):\n     \"\"\"Create a new agent\"\"\"\n     if \"write\" not in user[\"permissions\"]:\n         raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n-    \n+\n     try:\n         # Initialize provider factory if not exists\n         if not app_state[\"provider_factory\"]:\n             app_state[\"provider_factory\"] = ProviderFactory()\n-        \n+\n         # Initialize tool registry if not exists\n         if not app_state[\"tool_registry\"]:\n             app_state[\"tool_registry\"] = ToolRegistry()\n             for tool in get_all_tools():\n                 app_state[\"tool_registry\"].register(tool)\n-        \n+\n         # Create agent configuration\n         config = AgentConfig(\n             name=request.name,\n             role=request.role,\n             description=request.description,\n             spree_enabled=request.spree_enabled,\n             debug=request.debug,\n         )\n-        \n+\n         # Create LLM provider\n         provider = app_state[\"provider_factory\"].create_provider(request.llm_provider)\n-        \n+\n         # Filter tools\n         tools = app_state[\"tool_registry\"]\n         if request.tools:\n             # TODO: Filter tools based on request\n             pass\n-        \n+\n         # Create agent\n-        agent = ReactAgent(\n-            config=config,\n-            llm_provider=provider,\n-            tools=tools\n-        )\n-        \n+        agent = ReactAgent(config=config, llm_provider=provider, tools=tools)\n+\n         # Store agent\n         agent_id = str(uuid4())\n         app_state[\"agents\"][agent_id] = agent\n         app_state[\"metrics\"][\"agents_created\"] += 1\n-        \n+\n         return {\n             \"agent_id\": agent_id,\n             \"config\": {\n                 \"name\": config.name,\n                 \"role\": config.role.value,\n@@ -465,50 +483,52 @@\n                 \"debug\": config.debug,\n             },\n             \"provider\": request.llm_provider,\n             \"tools\": request.tools,\n         }\n-        \n+\n     except Exception as e:\n         logger.error(f\"Agent creation failed: {e}\")\n         raise HTTPException(status_code=500, detail=f\"Agent creation failed: {str(e)}\")\n \n \n @app.post(\"/agents/{agent_id}/execute\")\n async def execute_agent(\n-    agent_id: str,\n-    request: AgentExecutionRequest,\n-    user=Depends(get_current_user)\n+    agent_id: str, request: AgentExecutionRequest, user=Depends(get_current_user)\n ):\n     \"\"\"Execute a task with an agent\"\"\"\n     if \"write\" not in user[\"permissions\"]:\n         raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n-    \n+\n     if agent_id not in app_state[\"agents\"]:\n         raise HTTPException(status_code=404, detail=\"Agent not found\")\n-    \n+\n     try:\n         agent = app_state[\"agents\"][agent_id]\n-        \n+\n         # Execute task\n         start_time = time.time()\n         result = await agent.execute(request.message)\n         execution_time = time.time() - start_time\n-        \n+\n         return {\n             \"agent_id\": agent_id,\n             \"message\": request.message,\n             \"result\": {\n-                \"content\": result.content if hasattr(result, 'content') else str(result),\n+                \"content\": (\n+                    result.content if hasattr(result, 'content') else str(result)\n+                ),\n                 \"success\": result.success if hasattr(result, 'success') else True,\n                 \"execution_time\": execution_time,\n-                \"tokens_used\": result.tokens_used if hasattr(result, 'tokens_used') else 0,\n+                \"tokens_used\": (\n+                    result.tokens_used if hasattr(result, 'tokens_used') else 0\n+                ),\n             },\n             \"context\": request.context,\n             \"timestamp\": datetime.now(timezone.utc).isoformat(),\n         }\n-        \n+\n     except Exception as e:\n         logger.error(f\"Agent execution failed: {e}\")\n         raise HTTPException(status_code=500, detail=f\"Execution failed: {str(e)}\")\n \n \n@@ -517,15 +537,19 @@\n     \"\"\"List all agents\"\"\"\n     return {\n         \"agents\": [\n             {\n                 \"agent_id\": agent_id,\n-                \"config\": {\n-                    \"name\": agent.config.name,\n-                    \"role\": agent.config.role.value,\n-                    \"description\": agent.config.description,\n-                } if hasattr(agent, 'config') else {}\n+                \"config\": (\n+                    {\n+                        \"name\": agent.config.name,\n+                        \"role\": agent.config.role.value,\n+                        \"description\": agent.config.description,\n+                    }\n+                    if hasattr(agent, 'config')\n+                    else {}\n+                ),\n             }\n             for agent_id, agent in app_state[\"agents\"].items()\n         ]\n     }\n \n@@ -533,153 +557,171 @@\n @app.get(\"/agents/{agent_id}\")\n async def get_agent(agent_id: str, user=Depends(get_current_user)):\n     \"\"\"Get agent details\"\"\"\n     if agent_id not in app_state[\"agents\"]:\n         raise HTTPException(status_code=404, detail=\"Agent not found\")\n-    \n+\n     agent = app_state[\"agents\"][agent_id]\n-    \n+\n     return {\n         \"agent_id\": agent_id,\n-        \"config\": {\n-            \"name\": agent.config.name,\n-            \"role\": agent.config.role.value,\n-            \"description\": agent.config.description,\n-            \"spree_enabled\": agent.config.spree_enabled,\n-            \"debug\": agent.config.debug,\n-        } if hasattr(agent, 'config') else {},\n-        \"tools\": agent.tools.list_names() if hasattr(agent, 'tools') and hasattr(agent.tools, 'list_names') else [],\n-        \"provider\": type(agent.llm_provider).__name__ if hasattr(agent, 'llm_provider') else \"Unknown\",\n+        \"config\": (\n+            {\n+                \"name\": agent.config.name,\n+                \"role\": agent.config.role.value,\n+                \"description\": agent.config.description,\n+                \"spree_enabled\": agent.config.spree_enabled,\n+                \"debug\": agent.config.debug,\n+            }\n+            if hasattr(agent, 'config')\n+            else {}\n+        ),\n+        \"tools\": (\n+            agent.tools.list_names()\n+            if hasattr(agent, 'tools') and hasattr(agent.tools, 'list_names')\n+            else []\n+        ),\n+        \"provider\": (\n+            type(agent.llm_provider).__name__\n+            if hasattr(agent, 'llm_provider')\n+            else \"Unknown\"\n+        ),\n     }\n \n \n @app.delete(\"/agents/{agent_id}\")\n async def delete_agent(agent_id: str, user=Depends(get_current_user)):\n     \"\"\"Delete an agent\"\"\"\n     if \"admin\" not in user[\"permissions\"]:\n         raise HTTPException(status_code=403, detail=\"Admin permissions required\")\n-    \n+\n     if agent_id not in app_state[\"agents\"]:\n         raise HTTPException(status_code=404, detail=\"Agent not found\")\n-    \n+\n     del app_state[\"agents\"][agent_id]\n-    \n+\n     return {\"message\": f\"Agent {agent_id} deleted successfully\"}\n \n \n # File Upload and Processing Endpoints\n @app.post(\"/files/upload\")\n async def upload_file(\n     file: UploadFile = File(...),\n     operation: str = Form(default=\"store\"),\n-    user=Depends(get_current_user)\n+    user=Depends(get_current_user),\n ):\n     \"\"\"Upload and optionally process a file\"\"\"\n     if \"write\" not in user[\"permissions\"]:\n         raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n-    \n+\n     try:\n         # Create upload directory\n         upload_dir = Path(\"uploads\")\n         upload_dir.mkdir(exist_ok=True)\n-        \n+\n         # Save file\n         file_id = str(uuid4())\n         file_path = upload_dir / f\"{file_id}_{file.filename}\"\n-        \n+\n         with open(file_path, \"wb\") as f:\n             content = await file.read()\n             f.write(content)\n-        \n+\n         # Process file based on operation\n         result = {\"file_id\": file_id, \"filename\": file.filename, \"size\": len(content)}\n-        \n+\n         if operation == \"analyze\":\n             # TODO: Implement file analysis\n             result[\"analysis\"] = {\"type\": \"text\", \"encoding\": \"utf-8\"}\n         elif operation == \"extract\":\n             # TODO: Implement content extraction\n             result[\"content\"] = content.decode('utf-8', errors='ignore')[:1000]\n-        \n+\n         return result\n-        \n+\n     except Exception as e:\n         logger.error(f\"File upload failed: {e}\")\n         raise HTTPException(status_code=500, detail=f\"Upload failed: {str(e)}\")\n \n \n @app.get(\"/files/{file_id}\")\n async def download_file(file_id: str, user=Depends(get_current_user)):\n     \"\"\"Download a file\"\"\"\n     upload_dir = Path(\"uploads\")\n-    \n+\n     # Find file\n     for file_path in upload_dir.glob(f\"{file_id}_*\"):\n         return FileResponse(file_path)\n-    \n+\n     raise HTTPException(status_code=404, detail=\"File not found\")\n \n \n @app.post(\"/files/{file_id}/process\")\n async def process_file(\n-    file_id: str,\n-    request: FileProcessingRequest,\n-    user=Depends(get_current_user)\n+    file_id: str, request: FileProcessingRequest, user=Depends(get_current_user)\n ):\n     \"\"\"Process an uploaded file\"\"\"\n     if \"write\" not in user[\"permissions\"]:\n         raise HTTPException(status_code=403, detail=\"Insufficient permissions\")\n-    \n+\n     # TODO: Implement file processing operations\n     return {\n         \"file_id\": file_id,\n         \"operation\": request.operation,\n         \"parameters\": request.parameters,\n         \"status\": \"completed\",\n-        \"result\": \"File processed successfully\"\n+        \"result\": \"File processed successfully\",\n     }\n \n \n # WebSocket Endpoints\n @app.websocket(\"/ws/agent/{agent_id}\")\n async def websocket_agent(websocket: WebSocket, agent_id: str):\n     \"\"\"WebSocket endpoint for real-time agent communication\"\"\"\n     await websocket.accept()\n     app_state[\"websocket_connections\"].add(websocket)\n-    \n+\n     if agent_id not in app_state[\"agents\"]:\n         await websocket.send_json({\"error\": \"Agent not found\"})\n         await websocket.close()\n         return\n-    \n+\n     try:\n         agent = app_state[\"agents\"][agent_id]\n-        \n+\n         while True:\n             # Receive message\n             data = await websocket.receive_json()\n             message = data.get(\"message\", \"\")\n-            \n+\n             if not message:\n                 await websocket.send_json({\"error\": \"No message provided\"})\n                 continue\n-            \n+\n             # Execute agent\n             try:\n                 result = await agent.execute(message)\n-                await websocket.send_json({\n-                    \"type\": \"response\",\n-                    \"content\": result.content if hasattr(result, 'content') else str(result),\n-                    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n-                })\n+                await websocket.send_json(\n+                    {\n+                        \"type\": \"response\",\n+                        \"content\": (\n+                            result.content\n+                            if hasattr(result, 'content')\n+                            else str(result)\n+                        ),\n+                        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n+                    }\n+                )\n             except Exception as e:\n-                await websocket.send_json({\n-                    \"type\": \"error\",\n-                    \"error\": str(e),\n-                    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n-                })\n-                \n+                await websocket.send_json(\n+                    {\n+                        \"type\": \"error\",\n+                        \"error\": str(e),\n+                        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n+                    }\n+                )\n+\n     except WebSocketDisconnect:\n         app_state[\"websocket_connections\"].discard(websocket)\n     except Exception as e:\n         logger.error(f\"WebSocket error: {e}\")\n         await websocket.send_json({\"error\": str(e)})\n@@ -690,25 +732,25 @@\n @app.websocket(\"/ws/metrics\")\n async def websocket_metrics(websocket: WebSocket):\n     \"\"\"WebSocket endpoint for real-time metrics\"\"\"\n     await websocket.accept()\n     app_state[\"websocket_connections\"].add(websocket)\n-    \n+\n     try:\n         while True:\n             metrics = {\n                 \"timestamp\": datetime.now(timezone.utc).isoformat(),\n                 \"metrics\": app_state[\"metrics\"],\n                 \"active_sessions\": len(app_state[\"active_sessions\"]),\n                 \"websocket_connections\": len(app_state[\"websocket_connections\"]),\n                 \"generators\": len(app_state[\"spre_generators\"]),\n                 \"agents\": len(app_state[\"agents\"]),\n             }\n-            \n+\n             await websocket.send_json(metrics)\n             await asyncio.sleep(5)  # Send updates every 5 seconds\n-            \n+\n     except WebSocketDisconnect:\n         app_state[\"websocket_connections\"].discard(websocket)\n     except Exception as e:\n         logger.error(f\"Metrics WebSocket error: {e}\")\n     finally:\n@@ -719,115 +761,108 @@\n @app.get(\"/integrations/openai/status\")\n async def openai_integration_status(user=Depends(get_current_user)):\n     \"\"\"Get OpenAI integration status\"\"\"\n     try:\n         from ..integration.openai_agents import get_openai_integration\n-        \n+\n         integration = get_openai_integration()\n-        \n+\n         return {\n             \"available\": integration is not None,\n             \"configured\": bool(os.getenv(\"OPENAI_API_KEY\")),\n-            \"status\": \"active\" if integration else \"inactive\"\n+            \"status\": \"active\" if integration else \"inactive\",\n         }\n     except ImportError:\n-        return {\n-            \"available\": False,\n-            \"configured\": False,\n-            \"status\": \"not_available\"\n-        }\n+        return {\"available\": False, \"configured\": False, \"status\": \"not_available\"}\n \n \n @app.get(\"/integrations/langgraph/status\")\n async def langgraph_integration_status(user=Depends(get_current_user)):\n     \"\"\"Get LangGraph integration status\"\"\"\n     try:\n         from ..integration.langgraph import is_langgraph_available\n-        \n+\n         return {\n             \"available\": is_langgraph_available(),\n-            \"status\": \"active\" if is_langgraph_available() else \"inactive\"\n+            \"status\": \"active\" if is_langgraph_available() else \"inactive\",\n         }\n     except ImportError:\n-        return {\n-            \"available\": False,\n-            \"status\": \"not_available\"\n-        }\n+        return {\"available\": False, \"status\": \"not_available\"}\n \n \n # Export and Backup Endpoints\n @app.post(\"/export/data\")\n async def export_data(\n     format: str = Query(default=\"json\", description=\"Export format\"),\n     include_metadata: bool = Query(default=True, description=\"Include metadata\"),\n-    user=Depends(get_current_user)\n+    user=Depends(get_current_user),\n ):\n     \"\"\"Export application data\"\"\"\n     if \"admin\" not in user[\"permissions\"]:\n         raise HTTPException(status_code=403, detail=\"Admin permissions required\")\n-    \n+\n     try:\n         export_data = {\n             \"timestamp\": datetime.now(timezone.utc).isoformat(),\n             \"version\": \"1.0.0\",\n             \"generators\": len(app_state[\"spre_generators\"]),\n             \"agents\": len(app_state[\"agents\"]),\n             \"metrics\": app_state[\"metrics\"] if include_metadata else {},\n         }\n-        \n+\n         if format == \"json\":\n             return export_data\n         else:\n             raise HTTPException(status_code=400, detail=\"Unsupported format\")\n-            \n+\n     except Exception as e:\n         logger.error(f\"Data export failed: {e}\")\n         raise HTTPException(status_code=500, detail=f\"Export failed: {str(e)}\")\n \n \n # Startup event\n @app.on_event(\"startup\")\n async def startup_event():\n     \"\"\"Initialize application on startup\"\"\"\n     logger.info(\"Starting LlamaAgent Complete API...\")\n-    \n+\n     # Initialize provider factory\n     app_state[\"provider_factory\"] = ProviderFactory()\n-    \n+\n     # Initialize tool registry\n     app_state[\"tool_registry\"] = ToolRegistry()\n     for tool in get_all_tools():\n         app_state[\"tool_registry\"].register(tool)\n-    \n+\n     # Initialize orchestrator\n     try:\n         app_state[\"orchestrator\"] = AgentOrchestrator()\n         logger.info(\"Agent orchestrator initialized\")\n     except Exception as e:\n         logger.warning(f\"Failed to initialize orchestrator: {e}\")\n-    \n+\n     logger.info(\"LlamaAgent Complete API started successfully\")\n \n \n # Shutdown event\n @app.on_event(\"shutdown\")\n async def shutdown_event():\n     \"\"\"Cleanup on shutdown\"\"\"\n     logger.info(\"Shutting down LlamaAgent Complete API...\")\n-    \n+\n     # Close WebSocket connections\n     for websocket in app_state[\"websocket_connections\"]:\n         try:\n             await websocket.close()\n         except Exception:\n             pass\n-    \n+\n     # Cleanup resources\n     app_state[\"spre_generators\"].clear()\n     app_state[\"agents\"].clear()\n     app_state[\"active_sessions\"].clear()\n-    \n+\n     logger.info(\"Shutdown completed\")\n \n \n # Mount static files\n if Path(\"static\").exists():\n@@ -836,24 +871,26 @@\n \n # Add middleware for request logging\n @app.middleware(\"http\")\n async def log_requests(request: Request, call_next):\n     start_time = time.time()\n-    \n+\n     response = await call_next(request)\n-    \n+\n     process_time = time.time() - start_time\n-    logger.info(f\"{request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s\")\n-    \n+    logger.info(\n+        f\"{request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s\"\n+    )\n+\n     update_metrics()\n-    \n+\n     return response\n \n \n if __name__ == \"__main__\":\n     uvicorn.run(\n         \"src.llamaagent.api.complete_api:app\",\n         host=\"0.0.0.0\",\n         port=8000,\n         reload=True,\n-        log_level=\"info\"\n-    ) \n\\ No newline at end of file\n+        log_level=\"info\",\n+    )\n--- /Users/nemesis/llamaagent/src/llamaagent/tools/calculator.py\t2025-07-06 13:17:02.180945+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/tools/calculator.py\t2025-07-07 19:16:48.386742+00:00\n@@ -103,16 +103,14 @@\n     # ------------------------------------------------------------------ #\n     # Helpers                                                            #\n     # ------------------------------------------------------------------ #\n \n     @overload\n-    def _require_op(self, op: ast.operator) -> BinaryOp:\n-        ...  # noqa: D401\n+    def _require_op(self, op: ast.operator) -> BinaryOp: ...  # noqa: D401\n \n     @overload\n-    def _require_op(self, op: ast.unaryop) -> UnaryOp:\n-        ...  # noqa: D401\n+    def _require_op(self, op: ast.unaryop) -> UnaryOp: ...  # noqa: D401\n \n     def _require_op(self, op: Union[ast.operator, ast.unaryop]) -> OpFunc:\n         \"\"\"Return the function implementing op or raise TypeError.\"\"\"\n         op_type: Type[ast.AST] = type(op)\n         if op_type in self._ops:\n--- /Users/nemesis/llamaagent/tests/e2e/test_complete_system.py\t2025-07-06 13:01:44.938022+00:00\n+++ /Users/nemesis/llamaagent/tests/e2e/test_complete_system.py\t2025-07-07 19:16:48.399005+00:00\n@@ -9,70 +9,64 @@\n from src.llamaagent.api.main import app\n \n \n class TestE2E:\n     \"\"\"End-to-end test suite.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Setup test environment.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_complete_user_journey(self):\n         \"\"\"Test complete user journey from agent creation to task completion.\"\"\"\n         # Step 1: Create agent\n         agent_data = {\n             \"agent_name\": \"E2EAgent\",\n             \"llm_provider\": \"mock\",\n-            \"metadata\": {\"spree_enabled\": False}\n+            \"metadata\": {\"spree_enabled\": False},\n         }\n         agent_response = self.client.post(\"/agents\", json=agent_data)\n         assert agent_response.status_code == 200\n         agent_id = agent_response.json()[\"agent_id\"]\n-        \n+\n         # Step 2: Process multiple tasks\n         tasks = [\n             \"Hello, introduce yourself\",\n             \"What can you help me with?\",\n-            \"Solve this math problem: 15 + 27\"\n+            \"Solve this math problem: 15 + 27\",\n         ]\n-        \n+\n         for task in tasks:\n-            task_data = {\n-                \"task\": task,\n-                \"agent_id\": agent_id\n-            }\n+            task_data = {\"task\": task, \"agent_id\": agent_id}\n             response = self.client.post(\"/tasks\", json=task_data)\n             assert response.status_code == 200\n             assert \"result\" in response.json()\n-        \n+\n         # Step 3: Get agent status\n         status_response = self.client.get(f\"/agents/{agent_id}\")\n         assert status_response.status_code == 200\n         assert \"agent_name\" in status_response.json()\n-    \n+\n     def test_multi_agent_scenario(self):\n         \"\"\"Test scenario with multiple agents.\"\"\"\n         # Create multiple agents\n         agents = []\n         for i in range(3):\n             agent_data = {\n                 \"agent_name\": f\"Agent{i}\",\n                 \"llm_provider\": \"mock\",\n-                \"metadata\": {\"spree_enabled\": False}\n+                \"metadata\": {\"spree_enabled\": False},\n             }\n             response = self.client.post(\"/agents\", json=agent_data)\n             assert response.status_code == 200\n             agents.append(response.json()[\"agent_id\"])\n-        \n+\n         # Process tasks with different agents\n         for i, agent_id in enumerate(agents):\n-            task_data = {\n-                \"task\": f\"Task for agent {i}\",\n-                \"agent_id\": agent_id\n-            }\n+            task_data = {\"task\": f\"Task for agent {i}\", \"agent_id\": agent_id}\n             response = self.client.post(\"/tasks\", json=task_data)\n             assert response.status_code == 200\n-        \n+\n         # Verify all agents are still active\n         for agent_id in agents:\n             response = self.client.get(f\"/agents/{agent_id}\")\n             assert response.status_code == 200\n--- /Users/nemesis/llamaagent/tests/integration/test_workflow.py\t2025-07-06 13:01:44.937953+00:00\n+++ /Users/nemesis/llamaagent/tests/integration/test_workflow.py\t2025-07-07 19:16:48.401429+00:00\n@@ -11,67 +11,62 @@\n from src.llamaagent.llm.factory import LLMFactory\n \n \n class TestWorkflows:\n     \"\"\"Test suite for complete workflows.\"\"\"\n-    \n+\n     def test_simple_workflow(self):\n         \"\"\"Test simple agent workflow.\"\"\"\n         # Create agent\n         config = AgentConfig(\n-            agent_name=\"WorkflowAgent\",\n-            metadata={\"spree_enabled\": False}\n+            agent_name=\"WorkflowAgent\", metadata={\"spree_enabled\": False}\n         )\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n-        \n+\n         # Process multiple tasks\n         tasks = [\n             \"Calculate 2 + 2\",\n             \"What is the capital of France?\",\n-            \"Explain photosynthesis\"\n+            \"Explain photosynthesis\",\n         ]\n-        \n+\n         responses = []\n         for task in tasks:\n             response = agent.process_task(task)\n             responses.append(response)\n             assert response is not None\n             assert response.content is not None\n-        \n+\n         assert len(responses) == 3\n-    \n+\n     def test_spree_workflow(self):\n         \"\"\"Test SPREE workflow.\"\"\"\n-        config = AgentConfig(\n-            agent_name=\"SpreeAgent\",\n-            metadata={\"spree_enabled\": True}\n-        )\n+        config = AgentConfig(agent_name=\"SpreeAgent\", metadata={\"spree_enabled\": True})\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n-        \n+\n         # Process complex task\n         complex_task = \"Plan a marketing campaign for a new product\"\n         response = agent.process_task(complex_task)\n-        \n+\n         assert response is not None\n         assert response.content is not None\n         # SPREE mode should provide more detailed response\n         assert len(response.content) > 50\n-    \n+\n     def test_error_recovery_workflow(self):\n         \"\"\"Test workflow error recovery.\"\"\"\n         config = AgentConfig(\n-            agent_name=\"ErrorTestAgent\",\n-            metadata={\"spree_enabled\": False}\n+            agent_name=\"ErrorTestAgent\", metadata={\"spree_enabled\": False}\n         )\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n-        \n+\n         # Test with various edge cases\n         edge_cases = [\"\", None, \"A\" * 10000]  # Empty, None, very long\n-        \n+\n         for case in edge_cases:\n             try:\n                 response = agent.process_task(case)\n                 # Should handle gracefully\n                 assert response is not None\n--- /Users/nemesis/llamaagent/tests/performance/test_benchmarks.py\t2025-07-06 13:01:44.938095+00:00\n+++ /Users/nemesis/llamaagent/tests/performance/test_benchmarks.py\t2025-07-07 19:16:48.403821+00:00\n@@ -12,68 +12,66 @@\n from src.llamaagent.llm.providers.mock_provider import MockProvider\n \n \n class TestPerformance:\n     \"\"\"Performance test suite.\"\"\"\n-    \n+\n     def test_single_agent_performance(self):\n         \"\"\"Test single agent performance.\"\"\"\n-        config = AgentConfig(\n-            agent_name=\"PerfAgent\",\n-            metadata={\"spree_enabled\": False}\n-        )\n+        config = AgentConfig(agent_name=\"PerfAgent\", metadata={\"spree_enabled\": False})\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n-        \n+\n         # Measure response time\n         start_time = time.time()\n         response = agent.process_task(\"Simple task\")\n         end_time = time.time()\n-        \n+\n         assert response is not None\n         assert (end_time - start_time) < 1.0  # Should complete within 1 second\n-    \n+\n     def test_concurrent_agents(self):\n         \"\"\"Test concurrent agent performance.\"\"\"\n+\n         def create_and_run_agent(agent_id):\n             config = AgentConfig(\n                 agent_name=f\"ConcurrentAgent{agent_id}\",\n-                metadata={\"spree_enabled\": False}\n+                metadata={\"spree_enabled\": False},\n             )\n             provider = MockProvider(model_name=\"test-model\")\n             agent = ReactAgent(config=config, llm_provider=provider)\n-            \n+\n             return agent.process_task(f\"Task {agent_id}\")\n-        \n+\n         # Run multiple agents concurrently\n         start_time = time.time()\n         with ThreadPoolExecutor(max_workers=5) as executor:\n             futures = [executor.submit(create_and_run_agent, i) for i in range(10)]\n             results = [future.result() for future in futures]\n         end_time = time.time()\n-        \n+\n         # All should complete successfully\n         assert len(results) == 10\n         assert all(result is not None for result in results)\n-        \n+\n         # Should complete within reasonable time\n         assert (end_time - start_time) < 10.0\n-    \n+\n     def test_memory_usage(self):\n         \"\"\"Test memory usage doesn't grow excessively.\"\"\"\n         config = AgentConfig(\n-            agent_name=\"MemoryAgent\",\n-            metadata={\"spree_enabled\": False}\n+            agent_name=\"MemoryAgent\", metadata={\"spree_enabled\": False}\n         )\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n-        \n+\n         # Process many tasks\n         for i in range(100):\n             response = agent.process_task(f\"Task {i}\")\n             assert response is not None\n-        \n+\n         # Memory should be reasonable (this is a basic check)\n         import psutil\n+\n         process = psutil.Process()\n         memory_mb = process.memory_info().rss / 1024 / 1024\n         assert memory_mb < 500  # Should use less than 500MB\n--- /Users/nemesis/llamaagent/tests/test_basic_repl.py\t2025-07-07 14:18:49.831231+00:00\n+++ /Users/nemesis/llamaagent/tests/test_basic_repl.py\t2025-07-07 19:16:48.433764+00:00\n@@ -14,66 +14,66 @@\n \n \n def test_chat_message_basic():\n     \"\"\"Test ChatMessage dataclass creation.\"\"\"\n     from src.llamaagent.cli.chat_repl import ChatMessage\n-    \n+\n     message = ChatMessage(\n         role=\"user\",\n         content=\"Hello world\",\n         timestamp=time.time(),\n-        metadata={\"test\": True}\n+        metadata={\"test\": True},\n     )\n-    \n+\n     assert message.role == \"user\"\n     assert message.content == \"Hello world\"\n     assert message.metadata[\"test\"] is True\n     assert isinstance(message.timestamp, float)\n \n \n def test_chat_session_basic():\n     \"\"\"Test ChatSession dataclass creation.\"\"\"\n     from src.llamaagent.cli.chat_repl import ChatSession, ChatMessage\n-    \n+\n     messages = [\n         ChatMessage(role=\"user\", content=\"Hello\", timestamp=time.time()),\n-        ChatMessage(role=\"assistant\", content=\"Hi there\", timestamp=time.time())\n+        ChatMessage(role=\"assistant\", content=\"Hi there\", timestamp=time.time()),\n     ]\n-    \n+\n     session = ChatSession(\n         session_id=\"test-session\",\n         name=\"Test Session\",\n         created_at=time.time(),\n         updated_at=time.time(),\n         messages=messages,\n         context={\"system_prompt\": \"You are helpful\"},\n-        settings={\"temperature\": 0.7}\n+        settings={\"temperature\": 0.7},\n     )\n-    \n+\n     assert session.session_id == \"test-session\"\n     assert session.name == \"Test Session\"\n     assert len(session.messages) == 2\n     assert session.context[\"system_prompt\"] == \"You are helpful\"\n     assert session.settings[\"temperature\"] == 0.7\n \n \n def test_session_manager_basic():\n     \"\"\"Test SessionManager basic functionality.\"\"\"\n     from src.llamaagent.cli.chat_repl import SessionManager\n-    \n+\n     with tempfile.TemporaryDirectory() as tmpdir:\n         manager = SessionManager(storage_dir=tmpdir)\n-        \n+\n         # Test session creation\n         session = manager.create_session(name=\"Test Session\")\n         assert session.name == \"Test Session\"\n         assert isinstance(session.session_id, str)\n-        \n+\n         # Test session saving and loading\n         manager.save_session(session)\n         loaded_session = manager.load_session(session.session_id)\n-        \n+\n         assert loaded_session is not None\n         assert loaded_session.session_id == session.session_id\n         assert loaded_session.name == session.name\n \n \n@@ -82,61 +82,61 @@\n     \"\"\"Test ChatEngine basic functionality.\"\"\"\n     from src.llamaagent.cli.chat_repl import ChatEngine\n     from src.llamaagent.llm.providers.mock_provider import MockProvider\n     from src.llamaagent.types import LLMResponse\n     from unittest.mock import AsyncMock\n-    \n+\n     # Create chat engine with mock provider\n     chat_engine = ChatEngine()\n     chat_engine.provider = MockProvider(model_name=\"test-model\")\n-    \n+\n     # Create a test session\n     session = chat_engine.session_manager.create_session(name=\"Test\")\n-    \n+\n     # Mock the provider's complete method\n     mock_response = LLMResponse(\n         content=\"Hello! How can I help you?\",\n         model=\"test-model\",\n         provider=\"mock\",\n-        tokens_used=20\n+        tokens_used=20,\n     )\n     chat_engine.provider.complete = AsyncMock(return_value=mock_response)\n-    \n+\n     # Test chat\n     response = await chat_engine.chat(session, \"Hello\")\n-    \n+\n     assert response == \"Hello! How can I help you?\"\n     assert len(session.messages) == 2  # User + Assistant\n     assert session.messages[0].role == \"user\"\n     assert session.messages[0].content == \"Hello\"\n     assert session.messages[1].role == \"assistant\"\n     assert session.messages[1].content == \"Hello! How can I help you?\"\n \n \n-@pytest.mark.asyncio \n+@pytest.mark.asyncio\n async def test_repl_interface_commands():\n     \"\"\"Test REPL interface command handling.\"\"\"\n     from src.llamaagent.cli.chat_repl import REPLInterface\n     from unittest.mock import Mock\n-    \n+\n     # Create mock chat engine\n     mock_engine = Mock()\n     mock_engine.session_manager = Mock()\n-    \n+\n     repl = REPLInterface(mock_engine)\n-    \n+\n     # Test exit command\n     assert await repl._handle_command(\"exit\") is True\n     assert repl.running is False\n-    \n+\n     # Reset and test help command\n     repl.running = True\n     result = await repl._handle_command(\"help\")\n     assert result is True\n-    \n+\n     # Test that regular messages are not handled as commands\n     result = await repl._handle_command(\"This is a regular message\")\n     assert result is False\n \n \n if __name__ == \"__main__\":\n-    pytest.main([__file__, \"-v\"]) \n\\ No newline at end of file\n+    pytest.main([__file__, \"-v\"])\n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/middleware.py\t2025-07-07 14:54:33.974318+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/middleware.py\t2025-07-07 19:16:48.410571+00:00\n@@ -15,55 +15,60 @@\n \n from fastapi import FastAPI, Request, Response\n from fastapi.middleware.base import BaseHTTPMiddleware\n from fastapi.responses import JSONResponse\n \n+\n # Simple metrics collector implementation\n class MetricsCollector:\n     \"\"\"Simple metrics collector for monitoring.\"\"\"\n-    \n+\n     def __init__(self):\n         self.metrics: Dict[str, Dict[str, Any]] = {}\n         self.request_count = 0\n         self.error_count = 0\n         self.total_response_time = 0.0\n-    \n-    def record_http_request(self, method: str, endpoint: str, status_code: int, duration: float):\n+\n+    def record_http_request(\n+        self, method: str, endpoint: str, status_code: int, duration: float\n+    ):\n         \"\"\"Record HTTP request metrics.\"\"\"\n         self.request_count += 1\n         self.total_response_time += duration\n-        \n+\n         if status_code >= 400:\n             self.error_count += 1\n-        \n+\n         # Store metrics\n         key = f\"{method}:{endpoint}:{status_code}\"\n         if key not in self.metrics:\n             self.metrics[key] = {\"count\": 0, \"total_duration\": 0.0}\n-        \n+\n         self.metrics[key][\"count\"] += 1\n         self.metrics[key][\"total_duration\"] += duration\n-    \n+\n     def record_rate_limit_exceeded(self, endpoint: str, user_id: str):\n         \"\"\"Record rate limit exceeded event.\"\"\"\n         key = f\"rate_limit:{endpoint}:{user_id}\"\n         if key not in self.metrics:\n             self.metrics[key] = {\"count\": 0}\n         self.metrics[key][\"count\"] += 1\n-    \n+\n     def export_metrics(self) -> str:\n         \"\"\"Export metrics in text format.\"\"\"\n         lines = []\n         lines.append(f\"# Total requests: {self.request_count}\")\n         lines.append(f\"# Total errors: {self.error_count}\")\n-        lines.append(f\"# Average response time: {self.total_response_time / max(self.request_count, 1):.3f}s\")\n-        \n+        lines.append(\n+            f\"# Average response time: {self.total_response_time / max(self.request_count, 1):.3f}s\"\n+        )\n+\n         for key, data in self.metrics.items():\n             lines.append(f\"{key}: {data}\")\n-        \n+\n         return \"\\n\".join(lines)\n-    \n+\n     def get_content_type(self) -> str:\n         \"\"\"Get content type for metrics.\"\"\"\n         return \"text/plain\"\n \n \n@@ -81,475 +86,494 @@\n \n class MetricsMiddleware(BaseHTTPMiddleware):\n     \"\"\"\n     Middleware for collecting HTTP request metrics.\n     \"\"\"\n-    \n+\n     def __init__(self, app: FastAPI):\n         super().__init__(app)\n         self.metrics = get_metrics_collector()\n-    \n+\n     async def dispatch(self, request: Request, call_next: Callable) -> Response:\n         start_time = time.time()\n         request_id = str(uuid.uuid4())\n-        \n+\n         # Add request ID to headers for tracing\n         request.state.request_id = request_id\n-        \n+\n         # Extract request information\n         method = request.method\n         path = request.url.path\n-        \n+\n         # Normalize path for metrics (remove path parameters)\n         normalized_path = self._normalize_path(path)\n-        \n+\n         try:\n             response = await call_next(request)\n             status_code = response.status_code\n         except Exception as e:\n             logger.error(f\"Request {request_id} failed: {e}\")\n             status_code = 500\n             response = JSONResponse(\n                 status_code=500,\n-                content={\"error\": \"Internal server error\", \"request_id\": request_id}\n-            )\n-        \n+                content={\"error\": \"Internal server error\", \"request_id\": request_id},\n+            )\n+\n         # Calculate duration\n         duration = time.time() - start_time\n-        \n+\n         # Record metrics\n         self.metrics.record_http_request(\n             method=method,\n             endpoint=normalized_path,\n             status_code=status_code,\n-            duration=duration\n+            duration=duration,\n         )\n-        \n+\n         # Add response headers\n         response.headers[\"X-Request-ID\"] = request_id\n         response.headers[\"X-Response-Time\"] = f\"{duration:.3f}s\"\n-        \n+\n         return response\n-    \n+\n     def _normalize_path(self, path: str) -> str:\n         \"\"\"Normalize path by removing path parameters.\"\"\"\n         # Common path normalizations\n         normalizations = [\n             (r'/api/v\\d+', '/api/v*'),\n             (r'/agents/[^/]+', '/agents/{id}'),\n             (r'/tasks/[^/]+', '/tasks/{id}'),\n             (r'/users/[^/]+', '/users/{id}'),\n             (r'/sessions/[^/]+', '/sessions/{id}'),\n         ]\n-        \n+\n         normalized = path\n         for pattern, replacement in normalizations:\n             import re\n+\n             normalized = re.sub(pattern, replacement, normalized)\n-        \n+\n         return normalized\n \n \n class RequestLoggingMiddleware(BaseHTTPMiddleware):\n     \"\"\"\n     Middleware for comprehensive request logging.\n     \"\"\"\n-    \n+\n     def __init__(self, app: FastAPI, log_body: bool = False, max_body_size: int = 1024):\n         super().__init__(app)\n         self.log_body = log_body\n         self.max_body_size = max_body_size\n-    \n+\n     async def dispatch(self, request: Request, call_next: Callable) -> Response:\n         start_time = time.time()\n         request_id = getattr(request.state, 'request_id', str(uuid.uuid4()))\n-        \n+\n         # Log request start\n         request_info = {\n             \"request_id\": request_id,\n             \"method\": request.method,\n             \"url\": str(request.url),\n             \"headers\": dict(request.headers),\n             \"client_ip\": request.client.host if request.client else None,\n             \"user_agent\": request.headers.get(\"user-agent\"),\n         }\n-        \n+\n         # Optionally log request body\n         if self.log_body and request.method in [\"POST\", \"PUT\", \"PATCH\"]:\n             try:\n                 body = await request.body()\n                 if len(body) <= self.max_body_size:\n                     try:\n                         request_info[\"body\"] = json.loads(body.decode())\n                     except:\n-                        request_info[\"body\"] = body.decode()[:self.max_body_size]\n+                        request_info[\"body\"] = body.decode()[: self.max_body_size]\n                 else:\n                     request_info[\"body\"] = f\"[Body too large: {len(body)} bytes]\"\n             except Exception as e:\n                 request_info[\"body\"] = f\"[Error reading body: {e}]\"\n-        \n+\n         logger.info(f\"Request started: {json.dumps(request_info, default=str)}\")\n-        \n+\n         try:\n             response = await call_next(request)\n             status_code = response.status_code\n             success = True\n         except Exception as e:\n             logger.error(f\"Request {request_id} failed: {e}\")\n             status_code = 500\n             success = False\n             response = JSONResponse(\n                 status_code=500,\n-                content={\"error\": \"Internal server error\", \"request_id\": request_id}\n-            )\n-        \n+                content={\"error\": \"Internal server error\", \"request_id\": request_id},\n+            )\n+\n         # Calculate duration\n         duration = time.time() - start_time\n-        \n+\n         # Log request completion\n         response_info = {\n             \"request_id\": request_id,\n             \"status_code\": status_code,\n             \"duration_ms\": round(duration * 1000, 2),\n             \"success\": success,\n         }\n-        \n+\n         log_level = logging.INFO if success else logging.ERROR\n         logger.log(log_level, f\"Request completed: {json.dumps(response_info)}\")\n-        \n+\n         return response\n \n \n class HealthCheckMiddleware(BaseHTTPMiddleware):\n     \"\"\"\n     Middleware for health check monitoring.\n     \"\"\"\n-    \n+\n     def __init__(self, app: FastAPI, health_check_paths: Optional[list] = None):\n         super().__init__(app)\n         self.health_check_paths = health_check_paths or [\"/health\", \"/healthz\", \"/ping\"]\n         self.metrics = get_metrics_collector()\n-    \n+\n     async def dispatch(self, request: Request, call_next: Callable) -> Response:\n         # Skip detailed monitoring for health check endpoints\n         if request.url.path in self.health_check_paths:\n             return await call_next(request)\n-        \n+\n         return await call_next(request)\n \n \n class RateLimitMiddleware(BaseHTTPMiddleware):\n     \"\"\"\n     Middleware for rate limiting with metrics integration.\n     \"\"\"\n-    \n+\n     def __init__(\n         self,\n         app: FastAPI,\n         requests_per_minute: int = 60,\n         burst_size: int = 10,\n-        redis_client: Optional[Any] = None\n+        redis_client: Optional[Any] = None,\n     ):\n         super().__init__(app)\n         self.requests_per_minute = requests_per_minute\n         self.burst_size = burst_size\n         self.redis_client = redis_client\n         self.metrics = get_metrics_collector()\n-        \n+\n         # In-memory rate limiting for simple cases\n         self.request_counts: Dict[str, Dict[str, Any]] = {}\n-    \n+\n     async def dispatch(self, request: Request, call_next: Callable) -> Response:\n         client_ip = request.client.host if request.client else \"unknown\"\n         user_id = request.headers.get(\"X-User-ID\", client_ip)\n-        \n+\n         # Check rate limit\n         if await self._is_rate_limited(user_id, request.url.path):\n             self.metrics.record_rate_limit_exceeded(\n-                endpoint=request.url.path,\n-                user_id=user_id\n-            )\n-            \n+                endpoint=request.url.path, user_id=user_id\n+            )\n+\n             return JSONResponse(\n                 status_code=429,\n                 content={\n                     \"error\": \"Rate limit exceeded\",\n-                    \"message\": f\"Too many requests. Limit: {self.requests_per_minute} requests per minute\"\n+                    \"message\": f\"Too many requests. Limit: {self.requests_per_minute} requests per minute\",\n                 },\n                 headers={\n                     \"Retry-After\": \"60\",\n                     \"X-RateLimit-Limit\": str(self.requests_per_minute),\n                     \"X-RateLimit-Remaining\": \"0\",\n-                }\n-            )\n-        \n+                },\n+            )\n+\n         response = await call_next(request)\n-        \n+\n         # Add rate limit headers\n         remaining = await self._get_remaining_requests(user_id)\n         response.headers[\"X-RateLimit-Limit\"] = str(self.requests_per_minute)\n         response.headers[\"X-RateLimit-Remaining\"] = str(remaining)\n-        \n+\n         return response\n-    \n+\n     async def _is_rate_limited(self, user_id: str, endpoint: str) -> bool:\n         \"\"\"Check if user is rate limited.\"\"\"\n         current_time = time.time()\n         minute_window = int(current_time // 60)\n-        \n+\n         # Clean old entries\n         self._cleanup_old_entries(current_time)\n-        \n+\n         # Get current count for this user in this minute\n         user_key = f\"{user_id}:{minute_window}\"\n         if user_key not in self.request_counts:\n             self.request_counts[user_key] = {\n                 \"count\": 0,\n                 \"first_request\": current_time,\n-                \"last_request\": current_time\n+                \"last_request\": current_time,\n             }\n-        \n+\n         user_data = self.request_counts[user_key]\n-        \n+\n         # Check if rate limit exceeded\n         if user_data[\"count\"] >= self.requests_per_minute:\n             return True\n-        \n+\n         # Increment count\n         user_data[\"count\"] += 1\n         user_data[\"last_request\"] = current_time\n-        \n+\n         return False\n-    \n+\n     async def _get_remaining_requests(self, user_id: str) -> int:\n         \"\"\"Get remaining requests for user.\"\"\"\n         current_time = time.time()\n         minute_window = int(current_time // 60)\n         user_key = f\"{user_id}:{minute_window}\"\n-        \n+\n         if user_key in self.request_counts:\n-            return max(0, self.requests_per_minute - self.request_counts[user_key][\"count\"])\n-        \n+            return max(\n+                0, self.requests_per_minute - self.request_counts[user_key][\"count\"]\n+            )\n+\n         return self.requests_per_minute\n-    \n+\n     def _cleanup_old_entries(self, current_time: float):\n         \"\"\"Clean up old rate limit entries.\"\"\"\n         cutoff_time = current_time - 120  # Keep 2 minutes of data\n-        \n+\n         keys_to_remove = []\n         for key, data in self.request_counts.items():\n             if data[\"last_request\"] < cutoff_time:\n                 keys_to_remove.append(key)\n-        \n+\n         for key in keys_to_remove:\n             del self.request_counts[key]\n \n \n class ErrorTrackingMiddleware(BaseHTTPMiddleware):\n     \"\"\"\n     Middleware for comprehensive error tracking and alerting.\n     \"\"\"\n-    \n+\n     def __init__(self, app: FastAPI, enable_alerting: bool = True):\n         super().__init__(app)\n         self.enable_alerting = enable_alerting\n         self.metrics = get_metrics_collector()\n-        \n+\n         # Error tracking\n         self.error_counts: Dict[str, int] = {}\n         self.last_cleanup = time.time()\n-    \n+\n     async def dispatch(self, request: Request, call_next: Callable) -> Response:\n         request_id = getattr(request.state, 'request_id', str(uuid.uuid4()))\n-        \n+\n         try:\n             response = await call_next(request)\n-            \n+\n             # Track client errors (4xx)\n             if 400 <= response.status_code < 500:\n                 await self._track_error(\n                     error_type=\"client_error\",\n                     status_code=response.status_code,\n                     endpoint=request.url.path,\n-                    request_id=request_id\n+                    request_id=request_id,\n                 )\n-            \n+\n             return response\n-            \n+\n         except Exception as e:\n             # Track server errors (5xx)\n             await self._track_error(\n                 error_type=\"server_error\",\n                 status_code=500,\n                 endpoint=request.url.path,\n                 request_id=request_id,\n-                exception=e\n-            )\n-            \n-            logger.error(f\"Unhandled exception in request {request_id}: {e}\", exc_info=True)\n-            \n+                exception=e,\n+            )\n+\n+            logger.error(\n+                f\"Unhandled exception in request {request_id}: {e}\", exc_info=True\n+            )\n+\n             return JSONResponse(\n                 status_code=500,\n                 content={\n                     \"error\": \"Internal server error\",\n                     \"request_id\": request_id,\n-                    \"message\": \"An unexpected error occurred. Please try again later.\"\n-                }\n-            )\n-    \n+                    \"message\": \"An unexpected error occurred. Please try again later.\",\n+                },\n+            )\n+\n     async def _track_error(\n         self,\n         error_type: str,\n         status_code: int,\n         endpoint: str,\n         request_id: str,\n-        exception: Optional[Exception] = None\n+        exception: Optional[Exception] = None,\n     ):\n         \"\"\"Track error occurrence.\"\"\"\n         current_time = time.time()\n-        \n+\n         # Clean up old error counts\n         if current_time - self.last_cleanup > 300:  # 5 minutes\n             self._cleanup_error_counts(current_time)\n             self.last_cleanup = current_time\n-        \n+\n         # Track error\n         error_key = f\"{error_type}:{status_code}:{endpoint}\"\n         self.error_counts[error_key] = self.error_counts.get(error_key, 0) + 1\n-        \n+\n         # Log error details\n         error_info = {\n             \"request_id\": request_id,\n             \"error_type\": error_type,\n             \"status_code\": status_code,\n             \"endpoint\": endpoint,\n             \"count\": self.error_counts[error_key],\n         }\n-        \n+\n         if exception:\n             error_info[\"exception\"] = str(exception)\n             error_info[\"exception_type\"] = type(exception).__name__\n-        \n+\n         logger.warning(f\"Error tracked: {json.dumps(error_info)}\")\n-        \n+\n         # Alert if error rate is high\n-        if self.enable_alerting and self.error_counts[error_key] > 10:  # More than 10 errors\n+        if (\n+            self.enable_alerting and self.error_counts[error_key] > 10\n+        ):  # More than 10 errors\n             await self._send_alert(error_key, self.error_counts[error_key])\n-    \n+\n     def _cleanup_error_counts(self, current_time: float):\n         \"\"\"Clean up old error counts.\"\"\"\n         # Reset counts every 5 minutes for rate calculation\n         self.error_counts.clear()\n-    \n+\n     async def _send_alert(self, error_key: str, count: int):\n         \"\"\"Send alert for high error rate.\"\"\"\n         # This could integrate with alerting systems like PagerDuty, Slack, etc.\n-        logger.critical(f\"HIGH ERROR RATE ALERT: {error_key} - {count} errors in 5 minutes\")\n+        logger.critical(\n+            f\"HIGH ERROR RATE ALERT: {error_key} - {count} errors in 5 minutes\"\n+        )\n \n \n class SecurityMiddleware(BaseHTTPMiddleware):\n     \"\"\"\n     Security middleware for monitoring and protection.\n     \"\"\"\n-    \n+\n     def __init__(self, app: FastAPI, enable_security_headers: bool = True):\n         super().__init__(app)\n         self.enable_security_headers = enable_security_headers\n         self.suspicious_requests: Dict[str, int] = {}\n-    \n+\n     async def dispatch(self, request: Request, call_next: Callable) -> Response:\n         # Check for suspicious patterns\n         await self._check_suspicious_activity(request)\n-        \n+\n         response = await call_next(request)\n-        \n+\n         # Add security headers\n         if self.enable_security_headers:\n             self._add_security_headers(response)\n-        \n+\n         return response\n-    \n+\n     async def _check_suspicious_activity(self, request: Request):\n         \"\"\"Check for suspicious activity patterns.\"\"\"\n         client_ip = request.client.host if request.client else \"unknown\"\n-        \n+\n         # Check for common attack patterns\n         suspicious_patterns = [\n-            \"../\", \"..\\\\\", \"<script\", \"javascript:\", \"eval(\",\n-            \"union select\", \"drop table\", \"insert into\",\n-            \"exec(\", \"system(\", \"cmd.exe\",\n+            \"../\",\n+            \"..\\\\\",\n+            \"<script\",\n+            \"javascript:\",\n+            \"eval(\",\n+            \"union select\",\n+            \"drop table\",\n+            \"insert into\",\n+            \"exec(\",\n+            \"system(\",\n+            \"cmd.exe\",\n         ]\n-        \n+\n         request_data = str(request.url) + str(request.headers)\n-        \n+\n         for pattern in suspicious_patterns:\n             if pattern.lower() in request_data.lower():\n-                self.suspicious_requests[client_ip] = self.suspicious_requests.get(client_ip, 0) + 1\n-                logger.warning(f\"Suspicious request from {client_ip}: {pattern} detected\")\n+                self.suspicious_requests[client_ip] = (\n+                    self.suspicious_requests.get(client_ip, 0) + 1\n+                )\n+                logger.warning(\n+                    f\"Suspicious request from {client_ip}: {pattern} detected\"\n+                )\n                 break\n-        \n+\n         # Alert on repeated suspicious activity\n         if self.suspicious_requests.get(client_ip, 0) > 5:\n-            logger.critical(f\"SECURITY ALERT: Multiple suspicious requests from {client_ip}\")\n-    \n+            logger.critical(\n+                f\"SECURITY ALERT: Multiple suspicious requests from {client_ip}\"\n+            )\n+\n     def _add_security_headers(self, response: Response):\n         \"\"\"Add security headers to response.\"\"\"\n         security_headers = {\n             \"X-Content-Type-Options\": \"nosniff\",\n             \"X-Frame-Options\": \"DENY\",\n             \"X-XSS-Protection\": \"1; mode=block\",\n             \"Strict-Transport-Security\": \"max-age=31536000; includeSubDomains\",\n             \"Referrer-Policy\": \"strict-origin-when-cross-origin\",\n             \"Content-Security-Policy\": \"default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline';\",\n         }\n-        \n+\n         for header, value in security_headers.items():\n             response.headers[header] = value\n \n \n def setup_monitoring_middleware(app: FastAPI, config: Optional[Dict[str, Any]] = None):\n     \"\"\"\n     Setup comprehensive monitoring middleware for FastAPI app.\n     \"\"\"\n     config = config or {}\n-    \n+\n     # Add middleware in reverse order (last added is executed first)\n-    \n+\n     # Security middleware\n     if config.get(\"enable_security\", True):\n         app.add_middleware(SecurityMiddleware)\n-    \n+\n     # Error tracking middleware\n     if config.get(\"enable_error_tracking\", True):\n         app.add_middleware(ErrorTrackingMiddleware)\n-    \n+\n     # Rate limiting middleware\n     if config.get(\"enable_rate_limiting\", True):\n         app.add_middleware(\n-            RateLimitMiddleware,\n-            requests_per_minute=config.get(\"rate_limit_rpm\", 60)\n+            RateLimitMiddleware, requests_per_minute=config.get(\"rate_limit_rpm\", 60)\n         )\n-    \n+\n     # Request logging middleware\n     if config.get(\"enable_request_logging\", True):\n         app.add_middleware(\n-            RequestLoggingMiddleware,\n-            log_body=config.get(\"log_request_body\", False)\n+            RequestLoggingMiddleware, log_body=config.get(\"log_request_body\", False)\n         )\n-    \n+\n     # Health check middleware\n     app.add_middleware(HealthCheckMiddleware)\n-    \n+\n     # Metrics middleware (should be last/first to execute)\n     app.add_middleware(MetricsMiddleware)\n-    \n+\n     logger.info(\"Monitoring middleware setup completed\")\n \n \n # Export metrics endpoint\n async def metrics_endpoint():\n     \"\"\"Endpoint for Prometheus metrics scraping.\"\"\"\n     metrics = get_metrics_collector()\n     return Response(\n-        content=metrics.export_metrics(),\n-        media_type=metrics.get_content_type()\n-    ) \n\\ No newline at end of file\n+        content=metrics.export_metrics(), media_type=metrics.get_content_type()\n+    )\n--- /Users/nemesis/llamaagent/tests/unit/test_llm_providers.py\t2025-07-06 13:01:44.937718+00:00\n+++ /Users/nemesis/llamaagent/tests/unit/test_llm_providers.py\t2025-07-07 19:16:48.472506+00:00\n@@ -10,37 +10,37 @@\n from src.llamaagent.llm.providers.mock_provider import MockProvider\n \n \n class TestLLMProviders:\n     \"\"\"Test suite for LLM providers.\"\"\"\n-    \n+\n     def test_mock_provider_initialization(self):\n         \"\"\"Test mock provider can be initialized.\"\"\"\n         provider = MockProvider(model_name=\"test-model\")\n         assert provider is not None\n         assert provider.model_name == \"test-model\"\n-    \n+\n     def test_llm_factory_creates_mock_provider(self):\n         \"\"\"Test LLM factory can create mock provider.\"\"\"\n         factory = LLMFactory()\n         provider = factory.get_provider(\"mock\")\n         assert provider is not None\n         assert isinstance(provider, MockProvider)\n-    \n+\n     def test_llm_factory_fails_without_api_key(self):\n         \"\"\"Test LLM factory fails properly without API key.\"\"\"\n         factory = LLMFactory()\n         with pytest.raises(ValueError, match=\"API key not properly configured\"):\n             factory.get_provider(\"openai\")\n-    \n+\n     def test_mock_provider_generates_response(self):\n         \"\"\"Test mock provider generates responses.\"\"\"\n         provider = MockProvider(model_name=\"test-model\")\n         response = provider.generate(\"Test prompt\")\n         assert response is not None\n         assert len(response) > 0\n-    \n+\n     def test_provider_error_handling(self):\n         \"\"\"Test provider error handling.\"\"\"\n         provider = MockProvider(model_name=\"test-model\")\n         # Test with invalid input\n         with pytest.raises(Exception):\n--- /Users/nemesis/llamaagent/tests/unit/test_agents.py\t2025-07-06 13:01:44.937782+00:00\n+++ /Users/nemesis/llamaagent/tests/unit/test_agents.py\t2025-07-07 19:16:48.473681+00:00\n@@ -11,57 +11,45 @@\n from src.llamaagent.llm.providers.mock_provider import MockProvider\n \n \n class TestAgents:\n     \"\"\"Test suite for agents.\"\"\"\n-    \n+\n     def test_react_agent_initialization(self):\n         \"\"\"Test ReactAgent can be initialized.\"\"\"\n-        config = AgentConfig(\n-            agent_name=\"TestAgent\",\n-            metadata={\"spree_enabled\": False}\n-        )\n+        config = AgentConfig(agent_name=\"TestAgent\", metadata={\"spree_enabled\": False})\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n         assert agent is not None\n         assert agent.config.agent_name == \"TestAgent\"\n-    \n+\n     def test_react_agent_processes_task(self):\n         \"\"\"Test ReactAgent can process tasks.\"\"\"\n-        config = AgentConfig(\n-            agent_name=\"TestAgent\",\n-            metadata={\"spree_enabled\": False}\n-        )\n+        config = AgentConfig(agent_name=\"TestAgent\", metadata={\"spree_enabled\": False})\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n-        \n+\n         response = agent.process_task(\"Test task\")\n         assert response is not None\n         assert response.content is not None\n         assert len(response.content) > 0\n-    \n+\n     def test_agent_error_handling(self):\n         \"\"\"Test agent error handling.\"\"\"\n-        config = AgentConfig(\n-            agent_name=\"TestAgent\",\n-            metadata={\"spree_enabled\": False}\n-        )\n+        config = AgentConfig(agent_name=\"TestAgent\", metadata={\"spree_enabled\": False})\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n-        \n+\n         # Test with invalid input\n         response = agent.process_task(\"\")\n         assert response is not None\n         # Should handle empty input gracefully\n-    \n+\n     def test_agent_with_spree_mode(self):\n         \"\"\"Test agent with SPREE mode enabled.\"\"\"\n-        config = AgentConfig(\n-            agent_name=\"TestAgent\",\n-            metadata={\"spree_enabled\": True}\n-        )\n+        config = AgentConfig(agent_name=\"TestAgent\", metadata={\"spree_enabled\": True})\n         provider = MockProvider(model_name=\"test-model\")\n         agent = ReactAgent(config=config, llm_provider=provider)\n-        \n+\n         response = agent.process_task(\"Complex task requiring planning\")\n         assert response is not None\n         assert response.content is not None\n--- /Users/nemesis/llamaagent/src/llamaagent/prompting/chain_prompting.py\t2025-07-07 16:21:32.045738+00:00\n+++ /Users/nemesis/llamaagent/src/llamaagent/prompting/chain_prompting.py\t2025-07-07 19:16:48.491421+00:00\n@@ -385,11 +385,11 @@\n                 # Add connections to related nodes\n                 for related_id in thought.get(\"connections\", []):\n                     if related_id in self.thought_nodes:\n                         new_node.metadata.setdefault(\"connections\", []).append(\n                             related_id\n-                            )\n+                        )\n \n                 self.thought_nodes[new_node.id] = new_node\n                 active_nodes.append(new_node)\n \n         # Find best solution path through graph\n@@ -588,11 +588,13 @@\n         return list(reversed(path))\n \n     def _summarize_existing_thoughts(self) -> str:\n         \"\"\"Summarize existing thoughts for context\"\"\"\n         summaries = []\n-        for node_id, node in list(self.thought_nodes.items())[:5]:  # Limit to recent thoughts\n+        for node_id, node in list(self.thought_nodes.items())[\n+            :5\n+        ]:  # Limit to recent thoughts\n             summaries.append(f\"- {node.content[:50]}...\")\n         return \"\\n\".join(summaries)\n \n     def _parse_graph_thoughts(self, response: str) -> List[Dict[str, Any]]:\n         \"\"\"Parse response for graph-based thoughts\"\"\"\n--- /Users/nemesis/llamaagent/tests/test_comprehensive_functionality.py\t2025-07-07 19:08:16.825315+00:00\n+++ /Users/nemesis/llamaagent/tests/test_comprehensive_functionality.py\t2025-07-07 19:16:48.501031+00:00\n@@ -33,158 +33,165 @@\n from src.llamaagent import cli_main\n \n \n class TestSPREGenerator:\n     \"\"\"Test suite for SPREGenerator functionality\"\"\"\n-    \n+\n     @pytest.fixture\n     def config(self):\n         \"\"\"Create test configuration\"\"\"\n         return SpreConfig(\n             max_rounds=3,\n             players_per_session=2,\n             reward_threshold=0.7,\n             diversity_factor=0.3,\n             learning_rate=0.01,\n-            output_path=\"./test_output\"\n-        )\n-    \n+            output_path=\"./test_output\",\n+        )\n+\n     @pytest.fixture\n     def generator(self, config):\n         \"\"\"Create SPREGenerator instance\"\"\"\n         return SPREGenerator(config)\n-    \n+\n     def test_initialization(self, generator, config):\n         \"\"\"Test SPREGenerator initialization\"\"\"\n         assert generator.config == config\n         assert generator.generated_datasets == []\n         assert generator.engine is None  # Lazy initialization\n-    \n+\n     def test_generate_dataset_basic(self, generator):\n         \"\"\"Test basic dataset generation\"\"\"\n         with patch.object(generator, '_generate_dataset_async') as mock_async:\n             mock_dataset = SPREDataset(\n                 name=\"test_dataset\",\n                 description=\"Test dataset\",\n                 items=[\n                     SPREItem(\n                         id=\"item_1\",\n                         data_type=DataType.TEXT,\n-                        content={\"type\": \"text\", \"content\": \"Test content\"}\n+                        content={\"type\": \"text\", \"content\": \"Test content\"},\n                     )\n-                ]\n+                ],\n             )\n             mock_async.return_value = mock_dataset\n-            \n+\n             result = generator.generate_dataset(\n-                name=\"test_dataset\",\n-                count=5,\n-                description=\"Test dataset\"\n-            )\n-            \n+                name=\"test_dataset\", count=5, description=\"Test dataset\"\n+            )\n+\n             assert result.name == \"test_dataset\"\n             assert result.description == \"Test dataset\"\n             mock_async.assert_called_once()\n-    \n+\n     @pytest.mark.asyncio\n     async def test_generate_dataset_async(self, generator):\n         \"\"\"Test async dataset generation\"\"\"\n         with patch.object(generator, '_generate_item') as mock_item:\n             mock_item.return_value = SPREItem(\n                 id=\"item_1\",\n                 data_type=DataType.TEXT,\n-                content={\"type\": \"text\", \"content\": \"Test content\"}\n-            )\n-            \n+                content={\"type\": \"text\", \"content\": \"Test content\"},\n+            )\n+\n             result = await generator._generate_dataset_async(\n-                name=\"async_test\",\n-                count=3,\n-                description=\"Async test dataset\"\n-            )\n-            \n+                name=\"async_test\", count=3, description=\"Async test dataset\"\n+            )\n+\n             assert result.name == \"async_test\"\n             assert len(result.items) == 3\n             assert mock_item.call_count == 3\n-    \n+\n     def test_dataset_validation(self, generator):\n         \"\"\"Test dataset validation\"\"\"\n         dataset = SPREDataset(\n             name=\"validation_test\",\n             description=\"Test validation\",\n             items=[\n                 SPREItem(\n                     id=\"valid_item\",\n                     data_type=DataType.TEXT,\n-                    content={\"type\": \"text\", \"content\": \"Valid content with enough length\"}\n+                    content={\n+                        \"type\": \"text\",\n+                        \"content\": \"Valid content with enough length\",\n+                    },\n                 ),\n                 SPREItem(\n                     id=\"invalid_item\",\n                     data_type=DataType.TEXT,\n-                    content={\"type\": \"text\", \"content\": \"Short\"}\n-                )\n-            ]\n-        )\n-        \n+                    content={\"type\": \"text\", \"content\": \"Short\"},\n+                ),\n+            ],\n+        )\n+\n         asyncio.run(generator._validate_dataset(dataset))\n-        \n+\n         # Check validation results\n-        valid_items = [item for item in dataset.items if item.validation_status == ValidationStatus.VALID]\n-        invalid_items = [item for item in dataset.items if item.validation_status == ValidationStatus.INVALID]\n-        \n+        valid_items = [\n+            item\n+            for item in dataset.items\n+            if item.validation_status == ValidationStatus.VALID\n+        ]\n+        invalid_items = [\n+            item\n+            for item in dataset.items\n+            if item.validation_status == ValidationStatus.INVALID\n+        ]\n+\n         assert len(valid_items) >= 1\n         assert len(invalid_items) >= 0\n-    \n+\n     def test_data_types(self, generator):\n         \"\"\"Test different data types generation\"\"\"\n         data_types = [\n             DataType.TEXT,\n             DataType.CONVERSATION,\n             DataType.REASONING,\n             DataType.CREATIVE,\n             DataType.TECHNICAL,\n-            DataType.EDUCATIONAL\n+            DataType.EDUCATIONAL,\n         ]\n-        \n+\n         for data_type in data_types:\n             with patch.object(generator, '_generate_item') as mock_item:\n                 mock_item.return_value = SPREItem(\n                     id=f\"item_{data_type.value}\",\n                     data_type=data_type,\n-                    content={\"type\": data_type.value, \"content\": \"Test content\"}\n+                    content={\"type\": data_type.value, \"content\": \"Test content\"},\n                 )\n-                \n+\n                 result = generator.generate_dataset(\n-                    name=f\"test_{data_type.value}\",\n-                    count=1,\n-                    data_type=data_type\n+                    name=f\"test_{data_type.value}\", count=1, data_type=data_type\n                 )\n-                \n+\n                 assert result.items[0].data_type == data_type\n-    \n+\n     @pytest.mark.asyncio\n     async def test_generate_from_prompts(self, generator):\n         \"\"\"Test generation from prompts\"\"\"\n         prompts = [\n             \"Explain quantum computing\",\n             \"Write a creative story about AI\",\n-            \"Analyze market trends\"\n+            \"Analyze market trends\",\n         ]\n-        \n+\n         with patch.object(generator, '_get_engine') as mock_engine:\n             mock_session = MagicMock()\n             mock_session.generated_content = {\"content\": \"Generated content\"}\n             mock_session.reward_scores = {\"quality\": 0.8}\n             mock_session.session_id = \"test_session\"\n-            \n-            mock_engine.return_value.run_self_play_session = AsyncMock(return_value=mock_session)\n-            \n+\n+            mock_engine.return_value.run_self_play_session = AsyncMock(\n+                return_value=mock_session\n+            )\n+\n             result = await generator.generate_from_prompts(prompts)\n-            \n+\n             assert len(result) == 3\n             assert all(\"prompt\" in item for item in result)\n             assert all(\"generated_content\" in item for item in result)\n-    \n+\n     def test_dataset_stats(self, generator):\n         \"\"\"Test dataset statistics\"\"\"\n         # Create sample datasets\n         dataset1 = SPREDataset(\n             name=\"dataset1\",\n@@ -192,368 +199,382 @@\n             items=[\n                 SPREItem(\n                     id=\"item1\",\n                     data_type=DataType.TEXT,\n                     content={\"content\": \"Valid content\"},\n-                    validation_status=ValidationStatus.VALID\n+                    validation_status=ValidationStatus.VALID,\n                 ),\n                 SPREItem(\n                     id=\"item2\",\n                     data_type=DataType.TEXT,\n                     content={\"content\": \"Invalid\"},\n-                    validation_status=ValidationStatus.INVALID\n-                )\n-            ]\n-        )\n-        \n+                    validation_status=ValidationStatus.INVALID,\n+                ),\n+            ],\n+        )\n+\n         generator.generated_datasets = [dataset1]\n-        \n+\n         stats = generator.get_dataset_stats()\n-        \n+\n         assert stats[\"total_datasets\"] == 1\n         assert stats[\"total_items\"] == 2\n         assert stats[\"valid_items\"] == 1\n         assert stats[\"validation_rate\"] == 0.5\n \n \n class TestCLIFunctionality:\n     \"\"\"Test suite for CLI functionality\"\"\"\n-    \n+\n     @pytest.fixture\n     def runner(self):\n         \"\"\"Create CLI test runner\"\"\"\n         from click.testing import CliRunner\n+\n         return CliRunner()\n-    \n+\n     def test_cli_main_help(self, runner):\n         \"\"\"Test CLI main help\"\"\"\n         result = runner.invoke(cli_main, ['--help'])\n         assert result.exit_code == 0\n         assert \"LlamaAgent Command Line Interface\" in result.output\n-    \n+\n     def test_chat_command_help(self, runner):\n         \"\"\"Test chat command help\"\"\"\n         result = runner.invoke(cli_main, ['chat', '--help'])\n         assert result.exit_code == 0\n         assert \"Chat with an AI agent\" in result.output\n-    \n+\n     def test_generate_data_command_help(self, runner):\n         \"\"\"Test generate-data command help\"\"\"\n         result = runner.invoke(cli_main, ['generate-data', '--help'])\n         assert result.exit_code == 0\n         assert \"Generate training data\" in result.output\n-    \n+\n     def test_chat_command_basic(self, runner):\n         \"\"\"Test basic chat command\"\"\"\n         with patch('src.llamaagent.ReactAgent') as mock_agent:\n             mock_response = MagicMock()\n             mock_response.content = \"Test response\"\n             mock_response.execution_time = 1.0\n             mock_response.tokens_used = 50\n-            \n+\n             mock_agent.return_value.execute = AsyncMock(return_value=mock_response)\n-            \n+\n             result = runner.invoke(cli_main, ['chat', 'Hello world'])\n-            \n+\n             assert result.exit_code == 0\n             assert \"Test response\" in result.output\n-    \n+\n     def test_generate_data_gdt(self, runner):\n         \"\"\"Test GDT data generation\"\"\"\n         with tempfile.TemporaryDirectory() as temp_dir:\n             input_file = Path(temp_dir) / \"input.txt\"\n             output_file = Path(temp_dir) / \"output.json\"\n-            \n+\n             # Create sample input\n             input_file.write_text(\"Sample problem 1\\nSample problem 2\\n\")\n-            \n-            with patch('src.llamaagent.data_generation.gdt.GDTOrchestrator') as mock_orchestrator:\n+\n+            with patch(\n+                'src.llamaagent.data_generation.gdt.GDTOrchestrator'\n+            ) as mock_orchestrator:\n                 mock_orchestrator.return_value.generate_dataset = AsyncMock()\n-                \n-                result = runner.invoke(cli_main, [\n-                    'generate-data',\n-                    'gdt',\n-                    '-i', str(input_file),\n-                    '-o', str(output_file),\n-                    '-n', '2'\n-                ])\n-                \n+\n+                result = runner.invoke(\n+                    cli_main,\n+                    [\n+                        'generate-data',\n+                        'gdt',\n+                        '-i',\n+                        str(input_file),\n+                        '-o',\n+                        str(output_file),\n+                        '-n',\n+                        '2',\n+                    ],\n+                )\n+\n                 assert result.exit_code == 0\n                 assert \"GDT dataset saved\" in result.output\n-    \n+\n     def test_generate_data_spre(self, runner):\n         \"\"\"Test SPRE data generation\"\"\"\n         with tempfile.TemporaryDirectory() as temp_dir:\n             input_file = Path(temp_dir) / \"input.txt\"\n             output_file = Path(temp_dir) / \"output.json\"\n-            \n+\n             # Create sample input\n             input_file.write_text(\"Sample input for SPRE\\n\")\n-            \n-            with patch('src.llamaagent.data_generation.spre.SPREGenerator') as mock_generator:\n+\n+            with patch(\n+                'src.llamaagent.data_generation.spre.SPREGenerator'\n+            ) as mock_generator:\n                 mock_dataset = MagicMock()\n                 mock_dataset.name = \"Test Dataset\"\n                 mock_dataset.description = \"Test Description\"\n                 mock_dataset.metadata = {}\n                 mock_dataset.items = []\n-                \n+\n                 mock_generator.return_value.generate_dataset.return_value = mock_dataset\n-                \n-                result = runner.invoke(cli_main, [\n-                    'generate-data',\n-                    'spre',\n-                    '-i', str(input_file),\n-                    '-o', str(output_file),\n-                    '-n', '5'\n-                ])\n-                \n+\n+                result = runner.invoke(\n+                    cli_main,\n+                    [\n+                        'generate-data',\n+                        'spre',\n+                        '-i',\n+                        str(input_file),\n+                        '-o',\n+                        str(output_file),\n+                        '-n',\n+                        '5',\n+                    ],\n+                )\n+\n                 assert result.exit_code == 0\n                 assert \"SPRE dataset saved\" in result.output\n \n \n class TestAgentFunctionality:\n     \"\"\"Test suite for agent functionality\"\"\"\n-    \n+\n     @pytest.fixture\n     def config(self):\n         \"\"\"Create agent configuration\"\"\"\n         return AgentConfig(\n             name=\"TestAgent\",\n             role=AgentRole.GENERALIST,\n-            description=\"Test agent for comprehensive testing\"\n-        )\n-    \n+            description=\"Test agent for comprehensive testing\",\n+        )\n+\n     @pytest.fixture\n     def provider(self):\n         \"\"\"Create mock LLM provider\"\"\"\n         return MockProvider()\n-    \n+\n     @pytest.fixture\n     def tools(self):\n         \"\"\"Create tool registry\"\"\"\n         return ToolRegistry()\n-    \n+\n     @pytest.fixture\n     def agent(self, config, provider, tools):\n         \"\"\"Create ReactAgent instance\"\"\"\n         return ReactAgent(config=config, llm_provider=provider, tools=tools)\n-    \n+\n     @pytest.mark.asyncio\n     async def test_agent_initialization(self, agent, config):\n         \"\"\"Test agent initialization\"\"\"\n         assert agent.config == config\n         assert agent.llm_provider is not None\n         assert agent.tools is not None\n-    \n+\n     @pytest.mark.asyncio\n     async def test_agent_execute_basic(self, agent):\n         \"\"\"Test basic agent execution\"\"\"\n         result = await agent.execute(\"What is 2+2?\")\n-        \n+\n         assert result is not None\n         assert hasattr(result, 'content')\n         assert result.content is not None\n-    \n+\n     @pytest.mark.asyncio\n     async def test_agent_execute_with_tools(self, agent):\n         \"\"\"Test agent execution with tools\"\"\"\n         # Add calculator tool\n         from src.llamaagent.tools.calculator import CalculatorTool\n+\n         agent.tools.register(CalculatorTool())\n-        \n+\n         result = await agent.execute(\"Calculate 15 * 23\")\n-        \n+\n         assert result is not None\n         assert hasattr(result, 'content')\n-    \n+\n     @pytest.mark.asyncio\n     async def test_agent_error_handling(self, agent):\n         \"\"\"Test agent error handling\"\"\"\n         with patch.object(agent.llm_provider, 'complete') as mock_complete:\n             mock_complete.side_effect = Exception(\"Test error\")\n-            \n+\n             result = await agent.execute(\"This should fail\")\n-            \n+\n             # Agent should handle errors gracefully\n             assert result is not None\n \n \n class TestIntegrationModules:\n     \"\"\"Test suite for integration modules\"\"\"\n-    \n+\n     def test_openai_integration_import(self):\n         \"\"\"Test OpenAI integration import\"\"\"\n         try:\n             from src.llamaagent.integration.openai_agents import get_openai_integration\n+\n             assert get_openai_integration is not None\n         except ImportError:\n             pytest.skip(\"OpenAI integration not available\")\n-    \n+\n     def test_langgraph_integration_import(self):\n         \"\"\"Test LangGraph integration import\"\"\"\n         try:\n             from src.llamaagent.integration.langgraph import is_langgraph_available\n+\n             assert is_langgraph_available is not None\n         except ImportError:\n             pytest.skip(\"LangGraph integration not available\")\n-    \n+\n     def test_orchestrator_import(self):\n         \"\"\"Test orchestrator import\"\"\"\n         try:\n             from src.llamaagent.orchestrator import AgentOrchestrator\n+\n             assert AgentOrchestrator is not None\n         except ImportError:\n             pytest.skip(\"Orchestrator not available\")\n \n \n class TestErrorHandling:\n     \"\"\"Test suite for error handling and edge cases\"\"\"\n-    \n+\n     def test_invalid_data_type(self):\n         \"\"\"Test handling of invalid data types\"\"\"\n         with pytest.raises(ValueError):\n             DataType(\"invalid_type\")\n-    \n+\n     def test_invalid_validation_status(self):\n         \"\"\"Test handling of invalid validation status\"\"\"\n         with pytest.raises(ValueError):\n             ValidationStatus(\"invalid_status\")\n-    \n+\n     def test_empty_dataset_generation(self):\n         \"\"\"Test generation of empty dataset\"\"\"\n         generator = SPREGenerator()\n         dataset = generator.generate_dataset(\n-            name=\"empty_test\",\n-            count=0,\n-            description=\"Empty dataset test\"\n-        )\n-        \n+            name=\"empty_test\", count=0, description=\"Empty dataset test\"\n+        )\n+\n         assert len(dataset.items) == 0\n         assert dataset.name == \"empty_test\"\n-    \n+\n     def test_large_dataset_generation(self):\n         \"\"\"Test generation of large dataset (performance test)\"\"\"\n         generator = SPREGenerator()\n-        \n+\n         with patch.object(generator, '_generate_item') as mock_item:\n             mock_item.return_value = SPREItem(\n                 id=\"mock_item\",\n                 data_type=DataType.TEXT,\n-                content={\"content\": \"Mock content\"}\n-            )\n-            \n+                content={\"content\": \"Mock content\"},\n+            )\n+\n             # Generate large dataset\n             dataset = generator.generate_dataset(\n-                name=\"large_test\",\n-                count=1000,\n-                description=\"Large dataset test\"\n-            )\n-            \n+                name=\"large_test\", count=1000, description=\"Large dataset test\"\n+            )\n+\n             assert len(dataset.items) == 1000\n             assert mock_item.call_count == 1000\n \n \n class TestPerformanceAndBenchmarks:\n     \"\"\"Test suite for performance and benchmarking\"\"\"\n-    \n+\n     def test_generation_performance(self):\n         \"\"\"Test generation performance\"\"\"\n         import time\n-        \n+\n         generator = SPREGenerator()\n-        \n+\n         with patch.object(generator, '_generate_item') as mock_item:\n             mock_item.return_value = SPREItem(\n                 id=\"perf_item\",\n                 data_type=DataType.TEXT,\n-                content={\"content\": \"Performance test content\"}\n-            )\n-            \n+                content={\"content\": \"Performance test content\"},\n+            )\n+\n             start_time = time.time()\n             dataset = generator.generate_dataset(\n-                name=\"perf_test\",\n-                count=100,\n-                description=\"Performance test\"\n+                name=\"perf_test\", count=100, description=\"Performance test\"\n             )\n             end_time = time.time()\n-            \n+\n             # Should complete within reasonable time\n             assert (end_time - start_time) < 5.0  # 5 seconds max\n             assert len(dataset.items) == 100\n-    \n+\n     def test_memory_efficiency(self):\n         \"\"\"Test memory efficiency\"\"\"\n         import gc\n-        \n+\n         generator = SPREGenerator()\n-        \n+\n         with patch.object(generator, '_generate_item') as mock_item:\n             mock_item.return_value = SPREItem(\n                 id=\"mem_item\",\n                 data_type=DataType.TEXT,\n-                content={\"content\": \"Memory test content\"}\n-            )\n-            \n+                content={\"content\": \"Memory test content\"},\n+            )\n+\n             # Generate and immediately discard datasets\n             for i in range(10):\n                 dataset = generator.generate_dataset(\n-                    name=f\"mem_test_{i}\",\n-                    count=50,\n-                    description=f\"Memory test {i}\"\n+                    name=f\"mem_test_{i}\", count=50, description=f\"Memory test {i}\"\n                 )\n                 del dataset\n                 gc.collect()\n-            \n+\n             # Should not accumulate too much memory\n             assert len(generator.generated_datasets) == 10\n \n \n class TestDataValidation:\n     \"\"\"Test suite for data validation\"\"\"\n-    \n+\n     def test_item_validation(self):\n         \"\"\"Test individual item validation\"\"\"\n         valid_item = SPREItem(\n             id=\"valid_item\",\n             data_type=DataType.TEXT,\n-            content={\"type\": \"text\", \"content\": \"This is valid content with sufficient length\"}\n-        )\n-        \n+            content={\n+                \"type\": \"text\",\n+                \"content\": \"This is valid content with sufficient length\",\n+            },\n+        )\n+\n         invalid_item = SPREItem(\n-            id=\"invalid_item\",\n-            data_type=DataType.TEXT,\n-            content={}  # Empty content\n-        )\n-        \n+            id=\"invalid_item\", data_type=DataType.TEXT, content={}  # Empty content\n+        )\n+\n         generator = SPREGenerator()\n         dataset = SPREDataset(\n             name=\"validation_test\",\n             description=\"Test validation\",\n-            items=[valid_item, invalid_item]\n-        )\n-        \n+            items=[valid_item, invalid_item],\n+        )\n+\n         asyncio.run(generator._validate_dataset(dataset))\n-        \n+\n         assert valid_item.validation_status == ValidationStatus.VALID\n         assert invalid_item.validation_status == ValidationStatus.INVALID\n-    \n+\n     def test_quality_score_calculation(self):\n         \"\"\"Test quality score calculation\"\"\"\n         item = SPREItem(\n             id=\"quality_test\",\n             data_type=DataType.TEXT,\n             content={\n                 \"type\": \"text\",\n                 \"content\": \"High quality content with detailed information and metadata\",\n-                \"metadata\": {\"quality\": \"high\"}\n-            }\n-        )\n-        \n+                \"metadata\": {\"quality\": \"high\"},\n+            },\n+        )\n+\n         generator = SPREGenerator()\n         score = asyncio.run(generator._calculate_quality_score(item))\n-        \n+\n         assert 0.0 <= score <= 1.0\n         assert score > 0.5  # Should be above average for good content\n \n \n if __name__ == \"__main__\":\n-    pytest.main([__file__, \"-v\"]) \n\\ No newline at end of file\n+    pytest.main([__file__, \"-v\"])\n--- /Users/nemesis/llamaagent/tests/test_simon_ecosystem_integration.py\t2025-07-05 18:10:27.371949+00:00\n+++ /Users/nemesis/llamaagent/tests/test_simon_ecosystem_integration.py\t2025-07-07 19:16:48.529306+00:00\n@@ -348,13 +348,14 @@\n     @pytest.mark.asyncio\n     async def test_chat_with_mocked_subprocess(self):\n         \"\"\"Test chat with mocked subprocess call.\"\"\"\n         config = SimonEcosystemConfig(log_conversations=False)\n \n-        with patch(\"src.llamaagent.llm.simon_ecosystem.HAS_LLM\", True), patch(\n-            \"subprocess.run\"\n-        ) as mock_run:\n+        with (\n+            patch(\"src.llamaagent.llm.simon_ecosystem.HAS_LLM\", True),\n+            patch(\"subprocess.run\") as mock_run,\n+        ):\n             mock_run.return_value.stdout = \"Mocked response\"\n             mock_run.return_value.stderr = \"\"\n             mock_run.return_value.returncode = 0\n \n             ecosystem = SimonLLMEcosystem(config)\n@@ -368,13 +369,14 @@\n     @pytest.mark.asyncio\n     async def test_embed_with_mocked_subprocess(self):\n         \"\"\"Test embedding with mocked subprocess call.\"\"\"\n         config = SimonEcosystemConfig(log_conversations=False)\n \n-        with patch(\"src.llamaagent.llm.simon_ecosystem.HAS_LLM\", True), patch(\n-            \"subprocess.run\"\n-        ) as mock_run:\n+        with (\n+            patch(\"src.llamaagent.llm.simon_ecosystem.HAS_LLM\", True),\n+            patch(\"subprocess.run\") as mock_run,\n+        ):\n             mock_embedding = [0.1, 0.2, 0.3, 0.4, 0.5]\n             mock_run.return_value.stdout = json.dumps(mock_embedding)\n             mock_run.return_value.stderr = \"\"\n             mock_run.return_value.returncode = 0\n \n--- /Users/nemesis/llamaagent/tests/test_chat_repl_comprehensive.py\t2025-07-07 14:15:08.783697+00:00\n+++ /Users/nemesis/llamaagent/tests/test_chat_repl_comprehensive.py\t2025-07-07 19:16:48.552294+00:00\n@@ -33,25 +33,22 @@\n         \"\"\"Test creating a chat message.\"\"\"\n         message = ChatMessage(\n             role=\"user\",\n             content=\"Hello world\",\n             timestamp=time.time(),\n-            metadata={\"test\": True}\n-        )\n-        \n+            metadata={\"test\": True},\n+        )\n+\n         assert message.role == \"user\"\n         assert message.content == \"Hello world\"\n         assert message.metadata[\"test\"] is True\n         assert isinstance(message.timestamp, float)\n \n     def test_chat_message_minimal(self):\n         \"\"\"Test creating a minimal chat message.\"\"\"\n-        message = ChatMessage(\n-            role=\"assistant\",\n-            content=\"Response\"\n-        )\n-        \n+        message = ChatMessage(role=\"assistant\", content=\"Response\")\n+\n         assert message.role == \"assistant\"\n         assert message.content == \"Response\"\n         assert message.metadata is None\n         assert isinstance(message.timestamp, float)\n \n@@ -61,23 +58,23 @@\n \n     def test_chat_session_creation(self):\n         \"\"\"Test creating a chat session.\"\"\"\n         messages = [\n             ChatMessage(role=\"user\", content=\"Hello\", timestamp=time.time()),\n-            ChatMessage(role=\"assistant\", content=\"Hi there\", timestamp=time.time())\n+            ChatMessage(role=\"assistant\", content=\"Hi there\", timestamp=time.time()),\n         ]\n-        \n+\n         session = ChatSession(\n             session_id=\"test-session\",\n             name=\"Test Session\",\n             created_at=time.time(),\n             updated_at=time.time(),\n             messages=messages,\n             context={\"system_prompt\": \"You are helpful\"},\n-            settings={\"temperature\": 0.7}\n-        )\n-        \n+            settings={\"temperature\": 0.7},\n+        )\n+\n         assert session.session_id == \"test-session\"\n         assert session.name == \"Test Session\"\n         assert len(session.messages) == 2\n         assert session.context[\"system_prompt\"] == \"You are helpful\"\n         assert session.settings[\"temperature\"] == 0.7\n@@ -104,24 +101,23 @@\n         assert manager.storage_dir.exists()\n \n     def test_create_session(self, session_manager):\n         \"\"\"Test creating a new session.\"\"\"\n         session = session_manager.create_session(\n-            name=\"Test Session\",\n-            context={\"system_prompt\": \"Be helpful\"}\n-        )\n-        \n+            name=\"Test Session\", context={\"system_prompt\": \"Be helpful\"}\n+        )\n+\n         assert session.name == \"Test Session\"\n         assert session.context[\"system_prompt\"] == \"Be helpful\"\n         assert session.settings[\"model\"] == \"gpt-4o-mini\"\n         assert session.settings[\"temperature\"] == 0.7\n         assert len(session.messages) == 0\n \n     def test_create_session_with_defaults(self, session_manager):\n         \"\"\"Test creating a session with default parameters.\"\"\"\n         session = session_manager.create_session()\n-        \n+\n         assert session.name.startswith(\"Chat_\")\n         assert isinstance(session.session_id, str)\n         assert len(session.session_id) > 10  # Should be a UUID\n         assert session.context == {}\n         assert \"model\" in session.settings\n@@ -132,14 +128,14 @@\n         original_session = session_manager.create_session(name=\"Test Session\")\n         original_session.messages.append(\n             ChatMessage(role=\"user\", content=\"Test message\", timestamp=time.time())\n         )\n         session_manager.save_session(original_session)\n-        \n+\n         # Load session\n         loaded_session = session_manager.load_session(original_session.session_id)\n-        \n+\n         assert loaded_session is not None\n         assert loaded_session.session_id == original_session.session_id\n         assert loaded_session.name == original_session.name\n         assert len(loaded_session.messages) == 1\n         assert loaded_session.messages[0].content == \"Test message\"\n@@ -152,33 +148,37 @@\n     def test_list_sessions(self, session_manager):\n         \"\"\"Test listing sessions.\"\"\"\n         # Create multiple sessions\n         session1 = session_manager.create_session(name=\"Session 1\")\n         session2 = session_manager.create_session(name=\"Session 2\")\n-        \n+\n         # Add messages to sessions\n         session1.messages.append(\n             ChatMessage(role=\"user\", content=\"Message 1\", timestamp=time.time())\n         )\n-        session2.messages.extend([\n-            ChatMessage(role=\"user\", content=\"Message 2\", timestamp=time.time()),\n-            ChatMessage(role=\"assistant\", content=\"Response 2\", timestamp=time.time()),\n-        ])\n-        \n+        session2.messages.extend(\n+            [\n+                ChatMessage(role=\"user\", content=\"Message 2\", timestamp=time.time()),\n+                ChatMessage(\n+                    role=\"assistant\", content=\"Response 2\", timestamp=time.time()\n+                ),\n+            ]\n+        )\n+\n         session_manager.save_session(session1)\n         session_manager.save_session(session2)\n-        \n+\n         # List sessions\n         sessions = session_manager.list_sessions()\n-        \n+\n         assert len(sessions) == 2\n-        \n+\n         # Check session information\n         session_names = {s[\"name\"] for s in sessions}\n         assert \"Session 1\" in session_names\n         assert \"Session 2\" in session_names\n-        \n+\n         # Check message counts\n         for session_info in sessions:\n             if session_info[\"name\"] == \"Session 1\":\n                 assert session_info[\"message_count\"] == 1\n             elif session_info[\"name\"] == \"Session 2\":\n@@ -187,18 +187,18 @@\n     def test_delete_session(self, session_manager):\n         \"\"\"Test deleting a session.\"\"\"\n         # Create session\n         session = session_manager.create_session(name=\"To Delete\")\n         session_manager.save_session(session)\n-        \n+\n         # Verify it exists\n         assert session_manager.load_session(session.session_id) is not None\n-        \n+\n         # Delete it\n         result = session_manager.delete_session(session.session_id)\n         assert result is True\n-        \n+\n         # Verify it's gone\n         assert session_manager.load_session(session.session_id) is None\n \n     def test_delete_nonexistent_session(self, session_manager):\n         \"\"\"Test deleting a session that doesn't exist.\"\"\"\n@@ -225,27 +225,27 @@\n     @pytest.fixture\n     def sample_session(self, chat_engine):\n         \"\"\"Create a sample chat session.\"\"\"\n         return chat_engine.session_manager.create_session(\n             name=\"Test Session\",\n-            context={\"system_prompt\": \"You are a helpful assistant\"}\n+            context={\"system_prompt\": \"You are a helpful assistant\"},\n         )\n \n     @pytest.mark.asyncio\n     async def test_chat_basic(self, chat_engine, sample_session):\n         \"\"\"Test basic chat functionality.\"\"\"\n         # Mock the provider's complete method\n         mock_response = LLMResponse(\n             content=\"Hello! How can I help you?\",\n             model=\"test-model\",\n             provider=\"mock\",\n-            tokens_used=20\n+            tokens_used=20,\n         )\n         chat_engine.provider.complete = AsyncMock(return_value=mock_response)\n-        \n+\n         response = await chat_engine.chat(sample_session, \"Hello\")\n-        \n+\n         assert response == \"Hello! How can I help you?\"\n         assert len(sample_session.messages) == 2  # User + Assistant\n         assert sample_session.messages[0].role == \"user\"\n         assert sample_session.messages[0].content == \"Hello\"\n         assert sample_session.messages[1].role == \"assistant\"\n@@ -253,30 +253,34 @@\n \n     @pytest.mark.asyncio\n     async def test_chat_with_conversation_history(self, chat_engine, sample_session):\n         \"\"\"Test chat with existing conversation history.\"\"\"\n         # Add some history\n-        sample_session.messages.extend([\n-            ChatMessage(role=\"user\", content=\"What's 2+2?\", timestamp=time.time()),\n-            ChatMessage(role=\"assistant\", content=\"2+2 equals 4.\", timestamp=time.time()),\n-        ])\n-        \n+        sample_session.messages.extend(\n+            [\n+                ChatMessage(role=\"user\", content=\"What's 2+2?\", timestamp=time.time()),\n+                ChatMessage(\n+                    role=\"assistant\", content=\"2+2 equals 4.\", timestamp=time.time()\n+                ),\n+            ]\n+        )\n+\n         # Mock the provider\n         mock_response = LLMResponse(\n             content=\"The result is 9.\",\n             model=\"test-model\",\n             provider=\"mock\",\n-            tokens_used=15\n+            tokens_used=15,\n         )\n         chat_engine.provider.complete = AsyncMock(return_value=mock_response)\n-        \n+\n         response = await chat_engine.chat(sample_session, \"What's 3+6?\")\n-        \n+\n         # Verify the conversation history was used\n         call_args = chat_engine.provider.complete.call_args[0][0]\n         assert len(call_args) >= 4  # System + 2 history + new user message\n-        \n+\n         # Check message structure\n         user_messages = [msg for msg in call_args if msg.role == \"user\"]\n         assert len(user_messages) == 2\n         assert \"2+2\" in user_messages[0].content\n         assert \"3+6\" in user_messages[1].content\n@@ -284,53 +288,67 @@\n     @pytest.mark.asyncio\n     async def test_chat_error_handling(self, chat_engine, sample_session):\n         \"\"\"Test chat error handling.\"\"\"\n         # Mock provider to raise an error\n         chat_engine.provider.complete = AsyncMock(side_effect=Exception(\"API Error\"))\n-        \n+\n         response = await chat_engine.chat(sample_session, \"Test message\")\n-        \n+\n         assert \"Sorry, I encountered an error\" in response\n         assert \"API Error\" in response\n \n-    def test_build_conversation_context_with_system_prompt(self, chat_engine, sample_session):\n+    def test_build_conversation_context_with_system_prompt(\n+        self, chat_engine, sample_session\n+    ):\n         \"\"\"Test building conversation context with system prompt.\"\"\"\n         sample_session.context[\"system_prompt\"] = \"You are a math tutor\"\n-        sample_session.messages.extend([\n-            ChatMessage(role=\"user\", content=\"Help me\", timestamp=time.time()),\n-            ChatMessage(role=\"assistant\", content=\"I'll help\", timestamp=time.time()),\n-        ])\n-        \n+        sample_session.messages.extend(\n+            [\n+                ChatMessage(role=\"user\", content=\"Help me\", timestamp=time.time()),\n+                ChatMessage(\n+                    role=\"assistant\", content=\"I'll help\", timestamp=time.time()\n+                ),\n+            ]\n+        )\n+\n         messages = chat_engine._build_conversation_context(sample_session)\n-        \n+\n         assert len(messages) >= 3  # System + user + assistant\n         assert messages[0].role == \"system\"\n         assert messages[0].content == \"You are a math tutor\"\n \n-    def test_build_conversation_context_default_system(self, chat_engine, sample_session):\n+    def test_build_conversation_context_default_system(\n+        self, chat_engine, sample_session\n+    ):\n         \"\"\"Test building conversation context with default system prompt.\"\"\"\n         sample_session.messages.append(\n             ChatMessage(role=\"user\", content=\"Hello\", timestamp=time.time())\n         )\n-        \n+\n         messages = chat_engine._build_conversation_context(sample_session)\n-        \n+\n         assert len(messages) >= 2  # System + user\n         assert messages[0].role == \"system\"\n         assert \"helpful AI assistant\" in messages[0].content\n \n     def test_build_conversation_context_max_history(self, chat_engine, sample_session):\n         \"\"\"Test that conversation context respects max history limit.\"\"\"\n         # Add many messages\n         for i in range(30):\n-            sample_session.messages.extend([\n-                ChatMessage(role=\"user\", content=f\"Message {i}\", timestamp=time.time()),\n-                ChatMessage(role=\"assistant\", content=f\"Response {i}\", timestamp=time.time()),\n-            ])\n-        \n+            sample_session.messages.extend(\n+                [\n+                    ChatMessage(\n+                        role=\"user\", content=f\"Message {i}\", timestamp=time.time()\n+                    ),\n+                    ChatMessage(\n+                        role=\"assistant\", content=f\"Response {i}\", timestamp=time.time()\n+                    ),\n+                ]\n+            )\n+\n         messages = chat_engine._build_conversation_context(sample_session)\n-        \n+\n         # Should have system message + limited history (max_history = 20 by default)\n         assert len(messages) <= 21  # 1 system + 20 history\n \n \n class TestREPLInterface:\n@@ -351,25 +369,25 @@\n     @pytest.mark.asyncio\n     async def test_handle_exit_command(self, repl_interface):\n         \"\"\"Test handling exit commands.\"\"\"\n         assert await repl_interface._handle_command(\"exit\") is True\n         assert repl_interface.running is False\n-        \n+\n         # Reset and test other exit commands\n         repl_interface.running = True\n         assert await repl_interface._handle_command(\"quit\") is True\n         assert repl_interface.running is False\n-        \n+\n         repl_interface.running = True\n         assert await repl_interface._handle_command(\"bye\") is True\n         assert repl_interface.running is False\n \n     @pytest.mark.asyncio\n     async def test_handle_help_command(self, repl_interface, capsys):\n         \"\"\"Test handling help command.\"\"\"\n         result = await repl_interface._handle_command(\"help\")\n-        \n+\n         assert result is True\n         captured = capsys.readouterr()\n         assert \"Available commands\" in captured.out\n         assert \"exit\" in captured.out\n         assert \"/model\" in captured.out\n@@ -377,70 +395,72 @@\n     @pytest.mark.asyncio\n     async def test_handle_clear_command(self, repl_interface):\n         \"\"\"Test handling clear command.\"\"\"\n         with patch('os.system') as mock_system:\n             result = await repl_interface._handle_command(\"clear\")\n-            \n+\n             assert result is True\n             mock_system.assert_called_once()\n \n     @pytest.mark.asyncio\n     async def test_handle_temp_command(self, repl_interface):\n         \"\"\"Test handling temperature command.\"\"\"\n         # Mock current session\n         mock_session = Mock()\n         mock_session.settings = {}\n         repl_interface.current_session = mock_session\n-        \n+\n         result = await repl_interface._handle_command(\"/temp 0.9\")\n-        \n+\n         assert result is True\n         assert mock_session.settings[\"temperature\"] == 0.9\n \n     @pytest.mark.asyncio\n     async def test_handle_temp_command_invalid(self, repl_interface, capsys):\n         \"\"\"Test handling invalid temperature command.\"\"\"\n         mock_session = Mock()\n         mock_session.settings = {}\n         repl_interface.current_session = mock_session\n-        \n+\n         result = await repl_interface._handle_command(\"/temp invalid\")\n-        \n+\n         assert result is True\n         captured = capsys.readouterr()\n         assert \"Invalid temperature value\" in captured.out\n \n     @pytest.mark.asyncio\n     async def test_handle_temp_command_no_session(self, repl_interface, capsys):\n         \"\"\"Test handling temperature command without active session.\"\"\"\n         repl_interface.current_session = None\n-        \n+\n         result = await repl_interface._handle_command(\"/temp 0.5\")\n-        \n+\n         assert result is True\n         captured = capsys.readouterr()\n         assert \"No active session\" in captured.out\n \n     @pytest.mark.asyncio\n     async def test_handle_system_command(self, repl_interface):\n         \"\"\"Test handling system prompt command.\"\"\"\n         mock_session = Mock()\n         mock_session.context = {}\n         repl_interface.current_session = mock_session\n-        \n-        result = await repl_interface._handle_command(\"/system You are a coding assistant\")\n-        \n+\n+        result = await repl_interface._handle_command(\n+            \"/system You are a coding assistant\"\n+        )\n+\n         assert result is True\n         assert mock_session.context[\"system_prompt\"] == \"You are a coding assistant\"\n \n     @pytest.mark.asyncio\n     async def test_handle_system_command_no_session(self, repl_interface, capsys):\n         \"\"\"Test handling system command without active session.\"\"\"\n         repl_interface.current_session = None\n-        \n+\n         result = await repl_interface._handle_command(\"/system Test prompt\")\n-        \n+\n         assert result is True\n         captured = capsys.readouterr()\n         assert \"No active session\" in captured.out\n \n     @pytest.mark.asyncio\n@@ -448,17 +468,17 @@\n         \"\"\"Test handling model change command.\"\"\"\n         # Mock the chat engine and session\n         mock_session = Mock()\n         mock_session.settings = {}\n         repl_interface.current_session = mock_session\n-        \n+\n         mock_provider = Mock()\n         repl_interface.chat_engine.factory = Mock()\n         repl_interface.chat_engine.factory.create_provider.return_value = mock_provider\n-        \n+\n         result = await repl_interface._handle_command(\"/model gpt-4\")\n-        \n+\n         assert result is True\n         assert mock_session.settings[\"model\"] == \"gpt-4\"\n         repl_interface.chat_engine.factory.create_provider.assert_called_once_with(\n             \"openai\", model_name=\"gpt-4\"\n         )\n@@ -466,16 +486,18 @@\n     @pytest.mark.asyncio\n     async def test_handle_model_command_error(self, repl_interface, capsys):\n         \"\"\"Test handling model change command with error.\"\"\"\n         mock_session = Mock()\n         repl_interface.current_session = mock_session\n-        \n+\n         repl_interface.chat_engine.factory = Mock()\n-        repl_interface.chat_engine.factory.create_provider.side_effect = Exception(\"Model not found\")\n-        \n+        repl_interface.chat_engine.factory.create_provider.side_effect = Exception(\n+            \"Model not found\"\n+        )\n+\n         result = await repl_interface._handle_command(\"/model invalid-model\")\n-        \n+\n         assert result is True\n         captured = capsys.readouterr()\n         assert \"Error changing model\" in captured.out\n \n     @pytest.mark.asyncio\n@@ -493,13 +515,13 @@\n         mock_session.created_at = time.time()\n         mock_session.updated_at = time.time()\n         mock_session.messages = [Mock(), Mock(), Mock()]  # 3 messages\n         mock_session.settings = {\"model\": \"gpt-4\", \"temperature\": 0.7}\n         repl_interface.current_session = mock_session\n-        \n+\n         repl_interface._show_session_info()\n-        \n+\n         captured = capsys.readouterr()\n         assert \"Session Information\" in captured.out\n         assert \"test-123\" in captured.out\n         assert \"Test Session\" in captured.out\n         assert \"Total messages: 3\" in captured.out\n@@ -507,29 +529,33 @@\n         assert \"Temperature: 0.7\" in captured.out\n \n     def test_show_session_info_no_session(self, repl_interface):\n         \"\"\"Test showing session info when no session exists.\"\"\"\n         repl_interface.current_session = None\n-        \n+\n         # Should not raise an error\n         repl_interface._show_session_info()\n \n     def test_show_history(self, repl_interface, capsys):\n         \"\"\"Test showing conversation history.\"\"\"\n         # Mock messages\n         messages = [\n             ChatMessage(role=\"user\", content=\"Hello there\", timestamp=time.time()),\n-            ChatMessage(role=\"assistant\", content=\"Hi! How can I help?\", timestamp=time.time()),\n-            ChatMessage(role=\"user\", content=\"What's the weather?\", timestamp=time.time()),\n+            ChatMessage(\n+                role=\"assistant\", content=\"Hi! How can I help?\", timestamp=time.time()\n+            ),\n+            ChatMessage(\n+                role=\"user\", content=\"What's the weather?\", timestamp=time.time()\n+            ),\n         ]\n-        \n+\n         mock_session = Mock()\n         mock_session.messages = messages\n         repl_interface.current_session = mock_session\n-        \n+\n         repl_interface._show_history()\n-        \n+\n         captured = capsys.readouterr()\n         assert \"Conversation History\" in captured.out\n         assert \"Hello there\" in captured.out\n         assert \"Hi! How can I help?\" in captured.out\n         assert \"What's the weather?\" in captured.out\n@@ -537,22 +563,22 @@\n     def test_show_history_empty(self, repl_interface, capsys):\n         \"\"\"Test showing history when no messages exist.\"\"\"\n         mock_session = Mock()\n         mock_session.messages = []\n         repl_interface.current_session = mock_session\n-        \n+\n         repl_interface._show_history()\n-        \n+\n         captured = capsys.readouterr()\n         assert \"No conversation history yet\" in captured.out\n \n     def test_show_history_no_session(self, repl_interface, capsys):\n         \"\"\"Test showing history when no session exists.\"\"\"\n         repl_interface.current_session = None\n-        \n+\n         repl_interface._show_history()\n-        \n+\n         captured = capsys.readouterr()\n         assert \"No conversation history yet\" in captured.out\n \n \n class TestUtilityFunctions:\n@@ -562,51 +588,56 @@\n     def temp_storage(self):\n         \"\"\"Create temporary storage with test sessions.\"\"\"\n         with tempfile.TemporaryDirectory() as tmpdir:\n             # Create test sessions\n             manager = SessionManager(storage_dir=tmpdir)\n-            \n+\n             session1 = manager.create_session(name=\"Session 1\")\n             session1.messages.append(\n                 ChatMessage(role=\"user\", content=\"Test 1\", timestamp=time.time())\n             )\n             manager.save_session(session1)\n-            \n+\n             session2 = manager.create_session(name=\"Session 2\")\n-            session2.messages.extend([\n-                ChatMessage(role=\"user\", content=\"Test 2\", timestamp=time.time()),\n-                ChatMessage(role=\"assistant\", content=\"Response 2\", timestamp=time.time()),\n-            ])\n+            session2.messages.extend(\n+                [\n+                    ChatMessage(role=\"user\", content=\"Test 2\", timestamp=time.time()),\n+                    ChatMessage(\n+                        role=\"assistant\", content=\"Response 2\", timestamp=time.time()\n+                    ),\n+                ]\n+            )\n             manager.save_session(session2)\n-            \n+\n             yield tmpdir\n \n     def test_list_sessions_function(self, temp_storage, capsys):\n         \"\"\"Test the list_sessions utility function.\"\"\"\n         with patch('src.llamaagent.cli.chat_repl.SessionManager') as mock_manager_class:\n             mock_manager = Mock()\n             mock_manager_class.return_value = mock_manager\n-            \n+\n             # Mock session data\n             mock_manager.list_sessions.return_value = [\n                 {\n                     \"session_id\": \"session-1-long-id-here\",\n                     \"name\": \"Test Session 1\",\n                     \"message_count\": 5,\n-                    \"updated_at\": time.time()\n+                    \"updated_at\": time.time(),\n                 },\n                 {\n                     \"session_id\": \"session-2-long-id-here\",\n                     \"name\": \"Very Long Session Name That Should Be Truncated\",\n                     \"message_count\": 10,\n-                    \"updated_at\": time.time()\n-                }\n+                    \"updated_at\": time.time(),\n+                },\n             ]\n-            \n+\n             from src.llamaagent.cli.chat_repl import list_sessions\n+\n             list_sessions()\n-            \n+\n             captured = capsys.readouterr()\n             assert \"Available Chat Sessions\" in captured.out\n             assert \"Test Session 1\" in captured.out\n             assert \"Very Long Session Nam...\" in captured.out  # Should be truncated\n             assert \"session-1...\" in captured.out  # ID should be truncated\n@@ -615,35 +646,39 @@\n         \"\"\"Test list_sessions when no sessions exist.\"\"\"\n         with patch('src.llamaagent.cli.chat_repl.SessionManager') as mock_manager_class:\n             mock_manager = Mock()\n             mock_manager_class.return_value = mock_manager\n             mock_manager.list_sessions.return_value = []\n-            \n+\n             from src.llamaagent.cli.chat_repl import list_sessions\n+\n             list_sessions()\n-            \n+\n             captured = capsys.readouterr()\n             assert \"No chat sessions found\" in captured.out\n \n     def test_show_session_messages_function(self, capsys):\n         \"\"\"Test the show_session_messages utility function.\"\"\"\n         with patch('src.llamaagent.cli.chat_repl.SessionManager') as mock_manager_class:\n             mock_manager = Mock()\n             mock_manager_class.return_value = mock_manager\n-            \n+\n             # Mock session\n             mock_session = Mock()\n             mock_session.name = \"Test Session\"\n             mock_session.messages = [\n                 ChatMessage(role=\"user\", content=\"Hello\", timestamp=time.time()),\n-                ChatMessage(role=\"assistant\", content=\"Hi there!\", timestamp=time.time()),\n+                ChatMessage(\n+                    role=\"assistant\", content=\"Hi there!\", timestamp=time.time()\n+                ),\n             ]\n             mock_manager.load_session.return_value = mock_session\n-            \n+\n             from src.llamaagent.cli.chat_repl import show_session_messages\n+\n             show_session_messages(\"test-session-id\")\n-            \n+\n             captured = capsys.readouterr()\n             assert \"Messages from session: Test Session\" in captured.out\n             assert \"Hello\" in captured.out\n             assert \"Hi there!\" in captured.out\n \n@@ -651,14 +686,15 @@\n         \"\"\"Test show_session_messages when session doesn't exist.\"\"\"\n         with patch('src.llamaagent.cli.chat_repl.SessionManager') as mock_manager_class:\n             mock_manager = Mock()\n             mock_manager_class.return_value = mock_manager\n             mock_manager.load_session.return_value = None\n-            \n+\n             from src.llamaagent.cli.chat_repl import show_session_messages\n+\n             show_session_messages(\"nonexistent-id\")\n-            \n+\n             captured = capsys.readouterr()\n             assert \"Session 'nonexistent-id' not found\" in captured.out\n \n \n class TestIntegrationScenarios:\n@@ -675,50 +711,56 @@\n         \"\"\"Test a complete chat session workflow.\"\"\"\n         # Create chat engine with mock provider\n         chat_engine = ChatEngine()\n         chat_engine.provider = MockProvider(model_name=\"test-model\")\n         chat_engine.session_manager = SessionManager(storage_dir=temp_storage)\n-        \n+\n         # Mock provider responses\n         responses = [\n-            LLMResponse(content=\"Hello! I'm here to help.\", model=\"test-model\", provider=\"mock\"),\n+            LLMResponse(\n+                content=\"Hello! I'm here to help.\", model=\"test-model\", provider=\"mock\"\n+            ),\n             LLMResponse(content=\"2 + 2 equals 4.\", model=\"test-model\", provider=\"mock\"),\n-            LLMResponse(content=\"The weather is sunny today.\", model=\"test-model\", provider=\"mock\"),\n+            LLMResponse(\n+                content=\"The weather is sunny today.\",\n+                model=\"test-model\",\n+                provider=\"mock\",\n+            ),\n         ]\n         chat_engine.provider.complete = AsyncMock(side_effect=responses)\n-        \n+\n         # Create REPL interface\n         repl = REPLInterface(chat_engine)\n-        \n+\n         # Simulate starting a session\n         session = chat_engine.session_manager.create_session(name=\"Test Chat\")\n         repl.current_session = session\n-        \n+\n         # Simulate chat interactions\n         response1 = await chat_engine.chat(session, \"Hello\")\n         assert response1 == \"Hello! I'm here to help.\"\n         assert len(session.messages) == 2\n-        \n+\n         response2 = await chat_engine.chat(session, \"What's 2+2?\")\n         assert response2 == \"2 + 2 equals 4.\"\n         assert len(session.messages) == 4\n-        \n+\n         response3 = await chat_engine.chat(session, \"How's the weather?\")\n         assert response3 == \"The weather is sunny today.\"\n         assert len(session.messages) == 6\n-        \n+\n         # Test command handling\n         assert await repl._handle_command(\"/temp 0.9\") is True\n         assert session.settings[\"temperature\"] == 0.9\n-        \n+\n         assert await repl._handle_command(\"/system You are a weather expert\") is True\n         assert session.context[\"system_prompt\"] == \"You are a weather expert\"\n-        \n+\n         # Verify session persistence\n         chat_engine.session_manager.save_session(session)\n         loaded_session = chat_engine.session_manager.load_session(session.session_id)\n-        \n+\n         assert loaded_session is not None\n         assert len(loaded_session.messages) == 6\n         assert loaded_session.settings[\"temperature\"] == 0.9\n         assert loaded_session.context[\"system_prompt\"] == \"You are a weather expert\"\n \n@@ -726,115 +768,121 @@\n     async def test_session_recovery_and_continuation(self, temp_storage):\n         \"\"\"Test session recovery and continuation.\"\"\"\n         # Create initial session\n         manager1 = SessionManager(storage_dir=temp_storage)\n         session = manager1.create_session(name=\"Recovery Test\")\n-        \n+\n         # Add some conversation history\n-        session.messages.extend([\n-            ChatMessage(role=\"user\", content=\"Start conversation\", timestamp=time.time()),\n-            ChatMessage(role=\"assistant\", content=\"Conversation started\", timestamp=time.time()),\n-        ])\n+        session.messages.extend(\n+            [\n+                ChatMessage(\n+                    role=\"user\", content=\"Start conversation\", timestamp=time.time()\n+                ),\n+                ChatMessage(\n+                    role=\"assistant\",\n+                    content=\"Conversation started\",\n+                    timestamp=time.time(),\n+                ),\n+            ]\n+        )\n         session.context[\"system_prompt\"] = \"You are helpful\"\n         session.settings[\"temperature\"] = 0.8\n         manager1.save_session(session)\n-        \n+\n         # Simulate restart - create new manager instance\n         manager2 = SessionManager(storage_dir=temp_storage)\n-        \n+\n         # Load the session\n         recovered_session = manager2.load_session(session.session_id)\n-        \n+\n         assert recovered_session is not None\n         assert recovered_session.name == \"Recovery Test\"\n         assert len(recovered_session.messages) == 2\n         assert recovered_session.context[\"system_prompt\"] == \"You are helpful\"\n         assert recovered_session.settings[\"temperature\"] == 0.8\n-        \n+\n         # Continue the conversation\n         chat_engine = ChatEngine()\n         chat_engine.provider = MockProvider(model_name=\"test-model\")\n         chat_engine.session_manager = manager2\n-        \n+\n         mock_response = LLMResponse(\n-            content=\"Continuing our conversation\",\n-            model=\"test-model\",\n-            provider=\"mock\"\n+            content=\"Continuing our conversation\", model=\"test-model\", provider=\"mock\"\n         )\n         chat_engine.provider.complete = AsyncMock(return_value=mock_response)\n-        \n+\n         response = await chat_engine.chat(recovered_session, \"Continue please\")\n-        \n+\n         assert response == \"Continuing our conversation\"\n         assert len(recovered_session.messages) == 4\n \n     @pytest.mark.asyncio\n     async def test_error_handling_and_recovery(self, temp_storage):\n         \"\"\"Test error handling and recovery scenarios.\"\"\"\n         chat_engine = ChatEngine()\n         chat_engine.provider = MockProvider(model_name=\"test-model\")\n         chat_engine.session_manager = SessionManager(storage_dir=temp_storage)\n-        \n+\n         session = chat_engine.session_manager.create_session(name=\"Error Test\")\n-        \n+\n         # Test API error handling\n         chat_engine.provider.complete = AsyncMock(side_effect=Exception(\"API Error\"))\n-        \n+\n         response = await chat_engine.chat(session, \"This should fail\")\n-        \n+\n         assert \"Sorry, I encountered an error\" in response\n         assert \"API Error\" in response\n-        \n+\n         # Verify session state is still valid\n         assert len(session.messages) == 1  # Only user message added\n         assert session.messages[0].role == \"user\"\n         assert session.messages[0].content == \"This should fail\"\n-        \n+\n         # Test recovery after error\n         mock_response = LLMResponse(\n-            content=\"I'm back online\",\n-            model=\"test-model\",\n-            provider=\"mock\"\n+            content=\"I'm back online\", model=\"test-model\", provider=\"mock\"\n         )\n         chat_engine.provider.complete = AsyncMock(return_value=mock_response)\n-        \n+\n         response = await chat_engine.chat(session, \"Are you working now?\")\n-        \n+\n         assert response == \"I'm back online\"\n         assert len(session.messages) == 3  # Previous user + new user + assistant\n \n     @pytest.mark.asyncio\n     async def test_concurrent_session_access(self, temp_storage):\n         \"\"\"Test concurrent access to sessions.\"\"\"\n         manager = SessionManager(storage_dir=temp_storage)\n-        \n+\n         # Create session\n         session = manager.create_session(name=\"Concurrent Test\")\n         manager.save_session(session)\n-        \n+\n         # Simulate concurrent access\n         async def modify_session(session_id: str, message_content: str):\n             loaded_session = manager.load_session(session_id)\n             if loaded_session:\n                 loaded_session.messages.append(\n-                    ChatMessage(role=\"user\", content=message_content, timestamp=time.time())\n+                    ChatMessage(\n+                        role=\"user\", content=message_content, timestamp=time.time()\n+                    )\n                 )\n                 manager.save_session(loaded_session)\n-        \n+\n         # Run concurrent modifications\n         await asyncio.gather(\n             modify_session(session.session_id, \"Message 1\"),\n             modify_session(session.session_id, \"Message 2\"),\n             modify_session(session.session_id, \"Message 3\"),\n         )\n-        \n+\n         # Load final session and verify\n         final_session = manager.load_session(session.session_id)\n-        \n+\n         # Note: Due to file-based storage, last write wins\n         # In a production system, you'd want proper concurrency control\n         assert final_session is not None\n         assert len(final_session.messages) >= 1\n \n \n if __name__ == \"__main__\":\n-    pytest.main([__file__, \"-v\"]) \n\\ No newline at end of file\n+    pytest.main([__file__, \"-v\"])\n--- /Users/nemesis/llamaagent/tests/test_production_comprehensive.py\t2025-07-06 13:01:44.935069+00:00\n+++ /Users/nemesis/llamaagent/tests/test_production_comprehensive.py\t2025-07-07 19:16:48.586508+00:00\n@@ -31,10 +31,11 @@\n import requests\n from fastapi.testclient import TestClient\n \n # Import the production app and components\n import sys\n+\n sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))\n \n from llamaagent.agents.base import AgentConfig, AgentRole\n from llamaagent.agents.react import ReactAgent\n from llamaagent.api.production_app import app, app_state\n@@ -43,782 +44,783 @@\n from llamaagent.types import TaskStatus\n \n \n class TestProductionAPI:\n     \"\"\"Comprehensive API endpoint testing.\"\"\"\n-    \n+\n     @pytest.fixture\n     def client(self):\n         \"\"\"Create test client.\"\"\"\n         return TestClient(app)\n-    \n+\n     @pytest.fixture\n     def auth_headers(self):\n         \"\"\"Create authentication headers.\"\"\"\n         return {\"Authorization\": \"Bearer test-token\"}\n-    \n+\n     def test_root_endpoint(self, client):\n         \"\"\"Test root endpoint returns system information.\"\"\"\n         response = client.get(\"/\")\n         assert response.status_code == 200\n-        \n+\n         data = response.json()\n         assert \"message\" in data\n         assert \"version\" in data\n         assert \"features\" in data\n         assert \"endpoints\" in data\n         assert data[\"status\"] == \"running\"\n-    \n+\n     def test_health_endpoint(self, client):\n         \"\"\"Test comprehensive health check.\"\"\"\n         response = client.get(\"/health\")\n         assert response.status_code == 200\n-        \n+\n         data = response.json()\n         assert data[\"status\"] == \"healthy\"\n         assert \"timestamp\" in data\n         assert \"components\" in data\n         assert \"memory_usage\" in data\n         assert \"uptime\" in data\n         assert \"active_connections\" in data\n-    \n+\n     def test_metrics_endpoint(self, client):\n         \"\"\"Test metrics collection.\"\"\"\n         response = client.get(\"/metrics\")\n         assert response.status_code == 200\n-        \n+\n         data = response.json()\n         assert \"requests_total\" in data\n         assert \"active_agents\" in data\n         assert \"memory_usage\" in data\n         assert \"websocket_connections\" in data\n-    \n+\n     def test_agents_crud_operations(self, client, auth_headers):\n         \"\"\"Test complete CRUD operations for agents.\"\"\"\n         # Create agent\n         agent_data = {\n             \"name\": \"TestAgent\",\n             \"role\": \"generalist\",\n             \"description\": \"Test agent for CRUD operations\",\n             \"provider\": \"mock\",\n-            \"model\": \"mock-model\"\n+            \"model\": \"mock-model\",\n         }\n-        \n+\n         response = client.post(\"/agents\", json=agent_data, headers=auth_headers)\n         assert response.status_code == 200\n-        \n+\n         created_agent = response.json()\n         assert \"agent_id\" in created_agent\n         agent_id = created_agent[\"agent_id\"]\n-        \n+\n         # List agents\n         response = client.get(\"/agents\")\n         assert response.status_code == 200\n         agents_list = response.json()\n         assert \"agents\" in agents_list\n         assert len(agents_list[\"agents\"]) >= 1\n-        \n+\n         # Test agent exists in list\n-        agent_found = any(agent[\"agent_id\"] == agent_id for agent in agents_list[\"agents\"])\n+        agent_found = any(\n+            agent[\"agent_id\"] == agent_id for agent in agents_list[\"agents\"]\n+        )\n         assert agent_found\n-    \n+\n     def test_task_execution(self, client):\n         \"\"\"Test task execution endpoints.\"\"\"\n-        task_data = {\n-            \"task\": \"Calculate 2 + 2\",\n-            \"agent_id\": \"default\",\n-            \"timeout\": 30\n-        }\n-        \n+        task_data = {\"task\": \"Calculate 2 + 2\", \"agent_id\": \"default\", \"timeout\": 30}\n+\n         response = client.post(\"/tasks\", json=task_data)\n         assert response.status_code == 200\n-        \n+\n         result = response.json()\n         assert \"task_id\" in result\n         assert \"status\" in result\n         assert \"result\" in result\n         assert \"execution_time\" in result\n-    \n+\n     def test_chat_completion_openai_compatible(self, client):\n         \"\"\"Test OpenAI-compatible chat endpoint.\"\"\"\n         chat_data = {\n-            \"messages\": [\n-                {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n-            ],\n+            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n             \"model\": \"gpt-4o-mini\",\n-            \"temperature\": 0.7\n+            \"temperature\": 0.7,\n         }\n-        \n+\n         response = client.post(\"/v1/chat/completions\", json=chat_data)\n         assert response.status_code == 200\n-        \n+\n         result = response.json()\n         assert \"id\" in result\n         assert \"choices\" in result\n         assert \"usage\" in result\n         assert len(result[\"choices\"]) > 0\n         assert result[\"choices\"][0][\"message\"][\"role\"] == \"assistant\"\n-    \n+\n     def test_file_upload_and_processing(self, client, auth_headers):\n         \"\"\"Test file upload and processing workflow.\"\"\"\n         # Create test file\n         test_content = \"This is a test file for processing.\"\n-        \n+\n         with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".txt\", delete=False) as f:\n             f.write(test_content)\n             temp_file_path = f.name\n-        \n+\n         try:\n             # Upload file\n             with open(temp_file_path, \"rb\") as f:\n                 response = client.post(\n                     \"/files/upload\",\n                     files={\"file\": (\"test.txt\", f, \"text/plain\")},\n                     data={\"description\": \"Test file\"},\n-                    headers=auth_headers\n+                    headers=auth_headers,\n                 )\n-            \n+\n             assert response.status_code == 200\n             upload_result = response.json()\n             assert \"file_id\" in upload_result\n             file_id = upload_result[\"file_id\"]\n-            \n+\n             # Get file metadata\n             response = client.get(f\"/files/{file_id}\", headers=auth_headers)\n             assert response.status_code == 200\n-            \n+\n             # Process file\n             response = client.post(\n                 f\"/files/{file_id}/process\",\n                 json={\"task\": \"Summarize this file\"},\n-                headers=auth_headers\n+                headers=auth_headers,\n             )\n             assert response.status_code == 200\n-            \n+\n             process_result = response.json()\n             assert \"task_id\" in process_result\n             assert process_result[\"status\"] == \"processing\"\n-            \n+\n         finally:\n             # Clean up\n             os.unlink(temp_file_path)\n-    \n+\n     def test_authentication_required_endpoints(self, client):\n         \"\"\"Test that authentication is required for protected endpoints.\"\"\"\n         protected_endpoints = [\n             (\"/agents\", \"POST\", {\"name\": \"test\"}),\n             (\"/files/upload\", \"POST\", {}),\n             (\"/admin/system\", \"GET\", {}),\n-            (\"/dev/reset\", \"POST\", {})\n+            (\"/dev/reset\", \"POST\", {}),\n         ]\n-        \n+\n         for endpoint, method, data in protected_endpoints:\n             if method == \"GET\":\n                 response = client.get(endpoint)\n             elif method == \"POST\":\n                 response = client.post(endpoint, json=data)\n-            \n+\n             assert response.status_code == 401\n-    \n+\n     def test_rate_limiting(self, client):\n         \"\"\"Test rate limiting functionality.\"\"\"\n         # Make multiple rapid requests\n         responses = []\n         for _ in range(150):  # Exceed default limit of 100\n             response = client.get(\"/health\")\n             responses.append(response.status_code)\n-        \n+\n         # Should eventually get rate limited (429)\n         rate_limited = any(status == 429 for status in responses)\n         # Note: This might not trigger in test environment, so we'll check gracefully\n         assert all(status in [200, 429] for status in responses)\n-    \n+\n     def test_error_handling(self, client):\n         \"\"\"Test API error handling.\"\"\"\n         # Test non-existent agent\n         response = client.get(\"/agents/non-existent-id\")\n         assert response.status_code == 404\n-        \n+\n         # Test invalid task data\n         response = client.post(\"/tasks\", json={})\n         assert response.status_code == 422  # Validation error\n-        \n+\n         # Test non-existent file\n         response = client.get(\"/files/non-existent-file\")\n         assert response.status_code == 401  # Needs auth first\n \n \n class TestWebSocketFunctionality:\n     \"\"\"Test WebSocket real-time chat functionality.\"\"\"\n-    \n+\n     def test_websocket_connection(self):\n         \"\"\"Test WebSocket connection and basic communication.\"\"\"\n         with TestClient(app) as client:\n             with client.websocket_connect(\"/ws\") as websocket:\n                 # Send ping\n-                websocket.send_text(json.dumps({\n-                    \"type\": \"ping\",\n-                    \"content\": \"test ping\"\n-                }))\n-                \n+                websocket.send_text(\n+                    json.dumps({\"type\": \"ping\", \"content\": \"test ping\"})\n+                )\n+\n                 # Receive pong\n                 response = websocket.receive_text()\n                 data = json.loads(response)\n                 assert data[\"type\"] == \"pong\"\n                 assert data[\"content\"] == \"Server is alive\"\n-    \n+\n     def test_websocket_chat(self):\n         \"\"\"Test WebSocket chat functionality.\"\"\"\n         with TestClient(app) as client:\n             with client.websocket_connect(\"/ws\") as websocket:\n                 # Send chat message\n-                websocket.send_text(json.dumps({\n-                    \"type\": \"chat\",\n-                    \"content\": \"Hello, agent!\",\n-                    \"agent_id\": \"default\"\n-                }))\n-                \n+                websocket.send_text(\n+                    json.dumps(\n+                        {\n+                            \"type\": \"chat\",\n+                            \"content\": \"Hello, agent!\",\n+                            \"agent_id\": \"default\",\n+                        }\n+                    )\n+                )\n+\n                 # Receive response\n                 response = websocket.receive_text()\n                 data = json.loads(response)\n                 assert data[\"type\"] == \"response\"\n                 assert \"content\" in data\n                 assert \"execution_time\" in data\n-    \n+\n     def test_websocket_error_handling(self):\n         \"\"\"Test WebSocket error handling.\"\"\"\n         with TestClient(app) as client:\n             with client.websocket_connect(\"/ws\") as websocket:\n                 # Send message to non-existent agent\n-                websocket.send_text(json.dumps({\n-                    \"type\": \"chat\",\n-                    \"content\": \"Hello\",\n-                    \"agent_id\": \"non-existent\"\n-                }))\n-                \n+                websocket.send_text(\n+                    json.dumps(\n+                        {\"type\": \"chat\", \"content\": \"Hello\", \"agent_id\": \"non-existent\"}\n+                    )\n+                )\n+\n                 # Should receive error\n                 response = websocket.receive_text()\n                 data = json.loads(response)\n                 assert data[\"type\"] == \"error\"\n \n \n class TestPerformanceAndLoad:\n     \"\"\"Performance and load testing.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_concurrent_task_execution(self):\n         \"\"\"Test concurrent task execution performance.\"\"\"\n         # Create multiple agents\n         agents = []\n         for i in range(5):\n             config = AgentConfig(name=f\"LoadTestAgent{i}\")\n             agent = ReactAgent(config=config)\n             agents.append(agent)\n-        \n+\n         # Execute tasks concurrently\n         tasks = [f\"Calculate {i} + {i}\" for i in range(10)]\n-        \n+\n         async def execute_task(agent, task):\n             start_time = time.time()\n             try:\n                 result = await agent.execute(task)\n                 execution_time = time.time() - start_time\n                 return {\"success\": True, \"time\": execution_time, \"result\": result}\n             except Exception as e:\n                 execution_time = time.time() - start_time\n                 return {\"success\": False, \"time\": execution_time, \"error\": str(e)}\n-        \n+\n         # Run all tasks concurrently\n         start_time = time.time()\n         results = await asyncio.gather(\n-            *[execute_task(agents[i % len(agents)], task) for i, task in enumerate(tasks)],\n-            return_exceptions=True\n+            *[\n+                execute_task(agents[i % len(agents)], task)\n+                for i, task in enumerate(tasks)\n+            ],\n+            return_exceptions=True,\n         )\n         total_time = time.time() - start_time\n-        \n+\n         # Analyze results\n-        successful_results = [r for r in results if isinstance(r, dict) and r.get(\"success\")]\n+        successful_results = [\n+            r for r in results if isinstance(r, dict) and r.get(\"success\")\n+        ]\n         success_rate = len(successful_results) / len(results)\n-        \n+\n         assert success_rate >= 0.5  # At least 50% should succeed\n         assert total_time < 60  # Should complete within 1 minute\n-        \n+\n         if successful_results:\n-            avg_time = sum(r[\"time\"] for r in successful_results) / len(successful_results)\n+            avg_time = sum(r[\"time\"] for r in successful_results) / len(\n+                successful_results\n+            )\n             assert avg_time < 10  # Average execution should be under 10 seconds\n-    \n+\n     def test_memory_usage_monitoring(self):\n         \"\"\"Test memory usage monitoring.\"\"\"\n         import psutil\n-        \n+\n         process = psutil.Process()\n         initial_memory = process.memory_info().rss\n-        \n+\n         # Create many agents to test memory usage\n         agents = []\n         for i in range(50):\n             config = AgentConfig(name=f\"MemoryTestAgent{i}\")\n             agent = ReactAgent(config=config)\n             agents.append(agent)\n-        \n+\n         current_memory = process.memory_info().rss\n         memory_increase = current_memory - initial_memory\n-        \n+\n         # Memory increase should be reasonable (less than 500MB)\n         assert memory_increase < 500 * 1024 * 1024\n-        \n+\n         # Clean up\n         del agents\n-    \n+\n     @pytest.mark.asyncio\n     async def test_background_task_performance(self):\n         \"\"\"Test background task processing performance.\"\"\"\n         task_count = 20\n         background_tasks = []\n-        \n+\n         async def mock_background_task(task_id: str):\n             \"\"\"Mock background task.\"\"\"\n             await asyncio.sleep(0.1)  # Simulate work\n             return {\"task_id\": task_id, \"completed\": True}\n-        \n+\n         # Start background tasks\n         start_time = time.time()\n         background_tasks = [\n             asyncio.create_task(mock_background_task(f\"task_{i}\"))\n             for i in range(task_count)\n         ]\n-        \n+\n         # Wait for completion\n         results = await asyncio.gather(*background_tasks, return_exceptions=True)\n         total_time = time.time() - start_time\n-        \n+\n         # Verify results\n-        successful_tasks = [r for r in results if isinstance(r, dict) and r.get(\"completed\")]\n+        successful_tasks = [\n+            r for r in results if isinstance(r, dict) and r.get(\"completed\")\n+        ]\n         assert len(successful_tasks) == task_count\n         assert total_time < 5  # Should complete within 5 seconds\n \n \n class TestSecurityFeatures:\n     \"\"\"Security testing for authentication, authorization, and input validation.\"\"\"\n-    \n+\n     def test_jwt_token_validation(self):\n         \"\"\"Test JWT token validation.\"\"\"\n         client = TestClient(app)\n-        \n+\n         # Test with invalid token\n         invalid_headers = {\"Authorization\": \"Bearer invalid-token\"}\n-        response = client.post(\"/agents\", json={\"name\": \"test\"}, headers=invalid_headers)\n+        response = client.post(\n+            \"/agents\", json={\"name\": \"test\"}, headers=invalid_headers\n+        )\n         assert response.status_code == 401\n-        \n+\n         # Test with valid token\n         valid_headers = {\"Authorization\": \"Bearer test-token\"}\n         response = client.post(\n-            \"/agents\", \n-            json={\n-                \"name\": \"TestAgent\",\n-                \"role\": \"generalist\",\n-                \"provider\": \"mock\"\n-            }, \n-            headers=valid_headers\n+            \"/agents\",\n+            json={\"name\": \"TestAgent\", \"role\": \"generalist\", \"provider\": \"mock\"},\n+            headers=valid_headers,\n         )\n         assert response.status_code == 200\n-    \n+\n     def test_input_validation_and_sanitization(self):\n         \"\"\"Test input validation and sanitization.\"\"\"\n         client = TestClient(app)\n         auth_headers = {\"Authorization\": \"Bearer test-token\"}\n-        \n+\n         # Test SQL injection attempt\n         malicious_agent_data = {\n             \"name\": \"'; DROP TABLE agents; --\",\n             \"role\": \"generalist\",\n-            \"provider\": \"mock\"\n+            \"provider\": \"mock\",\n         }\n-        \n-        response = client.post(\"/agents\", json=malicious_agent_data, headers=auth_headers)\n+\n+        response = client.post(\n+            \"/agents\", json=malicious_agent_data, headers=auth_headers\n+        )\n         # Should either succeed with sanitized input or fail validation\n         assert response.status_code in [200, 422]\n-        \n+\n         # Test XSS attempt\n-        xss_task_data = {\n-            \"task\": \"<script>alert('xss')</script>\",\n-            \"agent_id\": \"default\"\n-        }\n-        \n+        xss_task_data = {\"task\": \"<script>alert('xss')</script>\", \"agent_id\": \"default\"}\n+\n         response = client.post(\"/tasks\", json=xss_task_data)\n         assert response.status_code == 200\n         # The task should be processed safely\n-    \n+\n     def test_admin_access_control(self):\n         \"\"\"Test admin access control.\"\"\"\n         client = TestClient(app)\n-        \n+\n         # Test with regular user token\n         user_headers = {\"Authorization\": \"Bearer test-token\"}\n         response = client.get(\"/admin/system\", headers=user_headers)\n         # Should work since test-token is admin in our mock\n         assert response.status_code in [200, 403]\n-        \n+\n         # Test without authentication\n         response = client.get(\"/admin/system\")\n         assert response.status_code == 401\n-    \n+\n     def test_file_upload_security(self):\n         \"\"\"Test file upload security measures.\"\"\"\n         client = TestClient(app)\n         auth_headers = {\"Authorization\": \"Bearer test-token\"}\n-        \n+\n         # Test executable file upload\n         malicious_content = b\"#!/bin/bash\\nrm -rf /\"\n-        \n+\n         response = client.post(\n             \"/files/upload\",\n-            files={\"file\": (\"malicious.sh\", malicious_content, \"application/x-shellscript\")},\n-            headers=auth_headers\n+            files={\n+                \"file\": (\"malicious.sh\", malicious_content, \"application/x-shellscript\")\n+            },\n+            headers=auth_headers,\n         )\n-        \n+\n         # Should either be rejected or handled safely\n         assert response.status_code in [200, 400, 415]\n-        \n+\n         # Test oversized file (mock)\n         large_content = b\"x\" * (10 * 1024 * 1024)  # 10MB\n         response = client.post(\n             \"/files/upload\",\n             files={\"file\": (\"large.txt\", large_content, \"text/plain\")},\n-            headers=auth_headers\n+            headers=auth_headers,\n         )\n-        \n+\n         # Should handle large files appropriately\n         assert response.status_code in [200, 413]\n \n \n class TestDatabaseOperations:\n     \"\"\"Database functionality testing.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_database_connection(self):\n         \"\"\"Test database connection and basic operations.\"\"\"\n         # Test database initialization\n         if app_state[\"database\"]:\n             # Test connection\n             assert app_state[\"database\"] is not None\n-            \n+\n             # Test basic health check\n             try:\n                 # Mock database health check\n                 health_status = True  # Mock successful connection\n                 assert health_status is True\n             except Exception:\n                 pytest.skip(\"Database not available for testing\")\n-    \n+\n     def test_data_persistence(self):\n         \"\"\"Test data persistence across operations.\"\"\"\n         # Test that application state persists across requests\n         client = TestClient(app)\n         auth_headers = {\"Authorization\": \"Bearer test-token\"}\n-        \n+\n         # Create agent\n         agent_data = {\n             \"name\": \"PersistenceTestAgent\",\n             \"role\": \"generalist\",\n-            \"provider\": \"mock\"\n+            \"provider\": \"mock\",\n         }\n-        \n+\n         response = client.post(\"/agents\", json=agent_data, headers=auth_headers)\n         assert response.status_code == 200\n-        \n+\n         created_agent = response.json()\n         agent_id = created_agent[\"agent_id\"]\n-        \n+\n         # Verify agent persists\n         response = client.get(\"/agents\")\n         assert response.status_code == 200\n-        \n+\n         agents = response.json()[\"agents\"]\n         persisted_agent = next((a for a in agents if a[\"agent_id\"] == agent_id), None)\n         assert persisted_agent is not None\n         assert persisted_agent[\"name\"] == \"PersistenceTestAgent\"\n \n \n class TestMonitoringAndAlerting:\n     \"\"\"Test monitoring, metrics, and alerting functionality.\"\"\"\n-    \n+\n     def test_health_check_components(self):\n         \"\"\"Test individual component health checks.\"\"\"\n         client = TestClient(app)\n-        \n+\n         response = client.get(\"/health\")\n         assert response.status_code == 200\n-        \n+\n         health_data = response.json()\n         components = health_data.get(\"components\", {})\n-        \n+\n         # Test that health check includes all components\n         expected_components = [\n             \"orchestrator\",\n-            \"database\", \n+            \"database\",\n             \"health_monitor\",\n             \"metrics\",\n             \"security_manager\",\n-            \"openai_integration\"\n+            \"openai_integration\",\n         ]\n-        \n+\n         for component in expected_components:\n             assert component in components\n             # Component status should be boolean\n             assert isinstance(components[component], bool)\n-    \n+\n     def test_metrics_collection(self):\n         \"\"\"Test comprehensive metrics collection.\"\"\"\n         client = TestClient(app)\n-        \n+\n         # Generate some activity\n         client.get(\"/\")\n         client.get(\"/health\")\n-        \n+\n         response = client.get(\"/metrics\")\n         assert response.status_code == 200\n-        \n+\n         metrics = response.json()\n-        \n+\n         # Test required metrics\n         required_metrics = [\n             \"requests_total\",\n             \"active_agents\",\n             \"memory_usage\",\n             \"websocket_connections\",\n             \"cache_hits\",\n-            \"cache_misses\"\n+            \"cache_misses\",\n         ]\n-        \n+\n         for metric in required_metrics:\n             assert metric in metrics\n             assert isinstance(metrics[metric], (int, float, dict))\n-    \n+\n     def test_performance_monitoring(self):\n         \"\"\"Test performance monitoring and tracking.\"\"\"\n         client = TestClient(app)\n-        \n+\n         # Make multiple requests to generate performance data\n         start_time = time.time()\n         for _ in range(10):\n             response = client.get(\"/health\")\n             assert response.status_code == 200\n-        \n+\n         total_time = time.time() - start_time\n         avg_response_time = total_time / 10\n-        \n+\n         # Response time should be reasonable\n         assert avg_response_time < 1.0  # Less than 1 second average\n \n \n class TestConfigurationManagement:\n     \"\"\"Test configuration management and environment handling.\"\"\"\n-    \n+\n     def test_environment_configuration(self):\n         \"\"\"Test environment-specific configuration.\"\"\"\n         # Test development environment\n         with patch.dict(os.environ, {\"ENVIRONMENT\": \"development\"}):\n             # Configuration should adapt to development\n             assert os.getenv(\"ENVIRONMENT\") == \"development\"\n-        \n+\n         # Test production environment\n         with patch.dict(os.environ, {\"ENVIRONMENT\": \"production\"}):\n             # Configuration should adapt to production\n             assert os.getenv(\"ENVIRONMENT\") == \"production\"\n-    \n+\n     def test_api_key_configuration(self):\n         \"\"\"Test API key configuration and validation.\"\"\"\n         # Test with valid-looking API key\n         with patch.dict(os.environ, {\"OPENAI_API_KEY\": \"sk-test123456789\"}):\n             api_key = os.getenv(\"OPENAI_API_KEY\")\n             assert api_key.startswith(\"sk-\")\n-        \n+\n         # Test with placeholder API key\n         with patch.dict(os.environ, {\"OPENAI_API_KEY\": \"${OPENAI_API_KEY}\"}):\n             api_key = os.getenv(\"OPENAI_API_KEY\")\n             # Should be detected as placeholder\n             assert api_key.startswith(\"your_api_\")\n-    \n+\n     def test_feature_flags(self):\n         \"\"\"Test feature flag configuration.\"\"\"\n         # Test enabling/disabling features via environment\n         with patch.dict(os.environ, {\"ENABLE_DEBUG_MODE\": \"true\"}):\n             debug_enabled = os.getenv(\"ENABLE_DEBUG_MODE\", \"false\").lower() == \"true\"\n             assert debug_enabled is True\n-        \n+\n         with patch.dict(os.environ, {\"ENABLE_DEBUG_MODE\": \"false\"}):\n             debug_enabled = os.getenv(\"ENABLE_DEBUG_MODE\", \"false\").lower() == \"true\"\n             assert debug_enabled is False\n \n \n class TestErrorRecoveryAndResilience:\n     \"\"\"Test error recovery and system resilience.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_agent_failure_recovery(self):\n         \"\"\"Test agent failure recovery mechanisms.\"\"\"\n         # Create agent that will fail\n         config = AgentConfig(name=\"FailingAgent\")\n         agent = ReactAgent(config=config)\n-        \n+\n         # Mock agent to fail\n         with patch.object(agent, 'execute', side_effect=Exception(\"Agent failure\")):\n             try:\n                 await agent.execute(\"test task\")\n                 assert False, \"Should have raised exception\"\n             except Exception as e:\n                 assert \"Agent failure\" in str(e)\n-        \n+\n         # Agent should still be functional after failure\n         result = await agent.execute(\"recovery test\")\n         assert result is not None\n-    \n+\n     def test_api_error_recovery(self):\n         \"\"\"Test API error recovery and graceful degradation.\"\"\"\n         client = TestClient(app)\n-        \n+\n         # Test with malformed request\n         response = client.post(\"/tasks\", json={\"invalid\": \"data\"})\n         assert response.status_code == 422\n-        \n+\n         # Subsequent valid requests should still work\n-        valid_data = {\n-            \"task\": \"test task\",\n-            \"agent_id\": \"default\"\n-        }\n+        valid_data = {\"task\": \"test task\", \"agent_id\": \"default\"}\n         response = client.post(\"/tasks\", json=valid_data)\n         assert response.status_code == 200\n-    \n+\n     def test_resource_exhaustion_handling(self):\n         \"\"\"Test handling of resource exhaustion scenarios.\"\"\"\n         client = TestClient(app)\n-        \n+\n         # Simulate many concurrent connections (test graceful handling)\n         responses = []\n         for _ in range(100):\n             response = client.get(\"/health\")\n             responses.append(response.status_code)\n-        \n+\n         # Should handle load gracefully\n         success_count = sum(1 for status in responses if status == 200)\n         success_rate = success_count / len(responses)\n-        \n+\n         # Should maintain reasonable success rate even under load\n         assert success_rate >= 0.8\n \n \n class TestDeploymentScenarios:\n     \"\"\"Test deployment and containerization scenarios.\"\"\"\n-    \n+\n     def test_application_startup(self):\n         \"\"\"Test application startup sequence.\"\"\"\n         # Test that app starts successfully\n         assert app is not None\n         assert app.title == \"LlamaAgent Production API\"\n-        \n+\n         # Test that middleware is configured\n         assert len(app.user_middleware) > 0\n-        \n+\n         # Test that routes are registered\n         route_paths = [route.path for route in app.routes]\n         expected_routes = [\"/\", \"/health\", \"/metrics\", \"/agents\", \"/tasks\"]\n-        \n+\n         for expected_route in expected_routes:\n             assert any(expected_route in path for path in route_paths)\n-    \n+\n     def test_graceful_shutdown(self):\n         \"\"\"Test graceful shutdown capabilities.\"\"\"\n         # Test that app can be shut down gracefully\n         # This is more of a smoke test since we can't actually shut down in tests\n-        \n+\n         # Verify shutdown hooks exist\n         assert hasattr(app, 'router')\n         assert app.router is not None\n-        \n+\n         # Test cleanup functions exist\n         from llamaagent.api.production_app import cleanup_application\n+\n         assert cleanup_application is not None\n-    \n+\n     def test_container_readiness(self):\n         \"\"\"Test container readiness and health checks.\"\"\"\n         client = TestClient(app)\n-        \n+\n         # Test readiness probe\n         response = client.get(\"/health\")\n         assert response.status_code == 200\n-        \n+\n         health_data = response.json()\n         assert health_data[\"status\"] == \"healthy\"\n-        \n+\n         # Test liveness probe\n         response = client.get(\"/\")\n         assert response.status_code == 200\n \n \n # Performance benchmarks\n @pytest.mark.benchmark\n class TestPerformanceBenchmarks:\n     \"\"\"Performance benchmarking tests.\"\"\"\n-    \n+\n     def test_agent_execution_benchmark(self, benchmark):\n         \"\"\"Benchmark agent execution performance.\"\"\"\n         config = AgentConfig(name=\"BenchmarkAgent\")\n         agent = ReactAgent(config=config)\n-        \n+\n         def execute_simple_task():\n             import asyncio\n+\n             return asyncio.run(agent.execute(\"Calculate 2 + 2\"))\n-        \n+\n         # Benchmark the execution\n         result = benchmark(execute_simple_task)\n         assert result is not None\n-    \n+\n     def test_api_response_benchmark(self, benchmark):\n         \"\"\"Benchmark API response times.\"\"\"\n         client = TestClient(app)\n-        \n+\n         def make_health_request():\n             response = client.get(\"/health\")\n             return response.status_code\n-        \n+\n         # Benchmark the API call\n         status_code = benchmark(make_health_request)\n         assert status_code == 200\n-    \n+\n     def test_concurrent_request_benchmark(self, benchmark):\n         \"\"\"Benchmark concurrent request handling.\"\"\"\n         client = TestClient(app)\n-        \n+\n         def make_concurrent_requests():\n             import threading\n+\n             results = []\n-            \n+\n             def make_request():\n                 response = client.get(\"/health\")\n                 results.append(response.status_code)\n-            \n+\n             threads = []\n             for _ in range(10):\n                 thread = threading.Thread(target=make_request)\n                 threads.append(thread)\n                 thread.start()\n-            \n+\n             for thread in threads:\n                 thread.join()\n-            \n+\n             return len([r for r in results if r == 200])\n-        \n+\n         # Benchmark concurrent requests\n         success_count = benchmark(make_concurrent_requests)\n         assert success_count >= 8  # At least 8 out of 10 should succeed\n \n \n if __name__ == \"__main__\":\n     # Run the comprehensive test suite\n-    pytest.main([\n-        __file__,\n-        \"-v\",\n-        \"--tb=short\",\n-        \"--maxfail=10\",\n-        \"--durations=10\"\n-    ]) \n\\ No newline at end of file\n+    pytest.main([__file__, \"-v\", \"--tb=short\", \"--maxfail=10\", \"--durations=10\"])\n--- /Users/nemesis/llamaagent/tests/test_langgraph.py\t2025-07-05 18:10:27.575990+00:00\n+++ /Users/nemesis/llamaagent/tests/test_langgraph.py\t2025-07-07 19:16:48.649139+00:00\n@@ -184,16 +184,16 @@\n         \"\"\"Create from dictionary representation.\"\"\"\n         return cls(\n             messages=data.get(\"messages\", []),\n             context=data.get(\"context\", {}),\n             current_step=data.get(\"current_step\", \"start\"),\n-            task_input=TaskInput(**data[\"task_input\"])\n-            if data.get(\"task_input\")\n-            else None,\n-            task_output=TaskOutput(**data[\"task_output\"])\n-            if data.get(\"task_output\")\n-            else None,\n+            task_input=(\n+                TaskInput(**data[\"task_input\"]) if data.get(\"task_input\") else None\n+            ),\n+            task_output=(\n+                TaskOutput(**data[\"task_output\"]) if data.get(\"task_output\") else None\n+            ),\n             error=data.get(\"error\"),\n             metadata=data.get(\"metadata\", {}),\n             workflow_status=data.get(\"workflow_status\", \"pending\"),\n             tool_results=data.get(\"tool_results\", []),\n             iteration_count=data.get(\"iteration_count\", 0),\n"
    },
    "isort_check": {
      "exit_code": 1,
      "output": "--- /Users/nemesis/llamaagent/src/llamaagent/api.py:before\t2025-07-07 08:35:49.491783\n+++ /Users/nemesis/llamaagent/src/llamaagent/api.py:after\t2025-07-07 12:16:49.095490\n@@ -17,15 +17,8 @@\n from typing import Any, Dict, List, Optional\n \n import uvicorn\n-from fastapi import (\n-    BackgroundTasks,\n-    Depends,\n-    FastAPI,\n-    HTTPException,\n-    WebSocket,\n-    WebSocketDisconnect,\n-    status,\n-)\n+from fastapi import (BackgroundTasks, Depends, FastAPI, HTTPException,\n+                     WebSocket, WebSocketDisconnect, status)\n from fastapi.middleware.cors import CORSMiddleware\n from fastapi.middleware.trustedhost import TrustedHostMiddleware\n from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer\n--- /Users/nemesis/llamaagent/src/llamaagent/research/citations.py:before\t2025-07-06 06:11:07.157821\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/citations.py:after\t2025-07-07 12:16:49.100882\n@@ -2,8 +2,9 @@\n Citation management for research.\n \"\"\"\n \n+from dataclasses import dataclass, field\n from typing import Any, Dict, List, Optional\n-from dataclasses import dataclass, field\n+\n \n @dataclass\n class Citation:\n--- /Users/nemesis/llamaagent/src/llamaagent/research/knowledge_graph.py:before\t2025-07-06 06:11:07.157940\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/knowledge_graph.py:after\t2025-07-07 12:16:49.101495\n@@ -2,8 +2,9 @@\n Knowledge graph implementation.\n \"\"\"\n \n+from dataclasses import dataclass, field\n from typing import Any, Dict, List, Optional\n-from dataclasses import dataclass, field\n+\n \n @dataclass\n class KnowledgeNode:\n--- /Users/nemesis/llamaagent/src/llamaagent/research/scientific_reasoning.py:before\t2025-07-06 06:11:07.158002\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/scientific_reasoning.py:after\t2025-07-07 12:16:49.102064\n@@ -2,8 +2,9 @@\n Scientific reasoning implementation.\n \"\"\"\n \n+from dataclasses import dataclass, field\n from typing import Any, Dict, List, Optional\n-from dataclasses import dataclass, field\n+\n \n @dataclass\n class ScientificClaim:\n--- /Users/nemesis/llamaagent/src/llamaagent/research/__init__.py:before\t2025-07-06 06:10:17.640857\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/__init__.py:after\t2025-07-07 12:16:49.103299\n@@ -10,54 +10,23 @@\n - Academic report generation\n \"\"\"\n \n-from .citations import (\n-    Citation,\n-    CitationAnalyzer,\n-    CitationFormatter,\n-    CitationGraph,\n-    CitationManager,\n-    DOIResolver,\n-)\n-from .evidence import (\n-    ClaimVerifier,\n-    ConsensusBuilder,\n-    ContradictionDetector,\n-    Evidence,\n-    EvidenceAnalyzer,\n-    EvidenceRanker,\n-    SourceCredibilityAnalyzer,\n-)\n-from .knowledge_graph import (\n-    ConceptExtractor,\n-    GraphQuerying,\n-    GraphVisualizer,\n-    KnowledgeGraph,\n-    RelationshipMapper,\n-    SemanticSimilarityAnalyzer,\n-)\n-from .literature_review import (\n-    DataSynthesizer,\n-    ExperimentDesigner,\n-    HypothesisGenerator,\n-    LiteratureReviewer,\n-    ResearchGapAnalyzer,\n-    SystematicReviewProtocol,\n-)\n-from .report_generator import (\n-    AbstractGenerator,\n-    AcademicFormatter,\n-    ExecutiveSummarizer,\n-    ResearchReporter,\n-    VisualizationGenerator,\n-)\n-from .scientific_reasoning import (\n-    BayesianReasoner,\n-    CausalReasoner,\n-    EffectSizeCalculator,\n-    HypothesisTester,\n-    MetaAnalyzer,\n-    StatisticalAnalyzer,\n-)\n+from .citations import (Citation, CitationAnalyzer, CitationFormatter,\n+                        CitationGraph, CitationManager, DOIResolver)\n+from .evidence import (ClaimVerifier, ConsensusBuilder, ContradictionDetector,\n+                       Evidence, EvidenceAnalyzer, EvidenceRanker,\n+                       SourceCredibilityAnalyzer)\n+from .knowledge_graph import (ConceptExtractor, GraphQuerying, GraphVisualizer,\n+                              KnowledgeGraph, RelationshipMapper,\n+                              SemanticSimilarityAnalyzer)\n+from .literature_review import (DataSynthesizer, ExperimentDesigner,\n+                                HypothesisGenerator, LiteratureReviewer,\n+                                ResearchGapAnalyzer, SystematicReviewProtocol)\n+from .report_generator import (AbstractGenerator, AcademicFormatter,\n+                               ExecutiveSummarizer, ResearchReporter,\n+                               VisualizationGenerator)\n+from .scientific_reasoning import (BayesianReasoner, CausalReasoner,\n+                                   EffectSizeCalculator, HypothesisTester,\n+                                   MetaAnalyzer, StatisticalAnalyzer)\n \n __all__ = [\n     # Citations\n--- /Users/nemesis/llamaagent/src/llamaagent/research/literature_review.py:before\t2025-07-06 06:11:07.158067\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/literature_review.py:after\t2025-07-07 12:16:49.103952\n@@ -2,8 +2,9 @@\n Literature review implementation.\n \"\"\"\n \n+from dataclasses import dataclass, field\n from typing import Any, Dict, List, Optional\n-from dataclasses import dataclass, field\n+\n \n @dataclass\n class Paper:\n--- /Users/nemesis/llamaagent/src/llamaagent/research/report_generator.py:before\t2025-07-06 06:11:35.640445\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/report_generator.py:after\t2025-07-07 12:16:49.104543\n@@ -2,8 +2,9 @@\n Report generation for research.\n \"\"\"\n \n+from dataclasses import dataclass, field\n from typing import Any, Dict, List, Optional\n-from dataclasses import dataclass, field\n+\n \n @dataclass\n class Report:\n--- /Users/nemesis/llamaagent/src/llamaagent/research/evidence.py:before\t2025-07-06 06:11:35.640557\n+++ /Users/nemesis/llamaagent/src/llamaagent/research/evidence.py:after\t2025-07-07 12:16:49.106875\n@@ -2,8 +2,9 @@\n Evidence management for research.\n \"\"\"\n \n+from dataclasses import dataclass, field\n from typing import Any, Dict, List, Optional\n-from dataclasses import dataclass, field\n+\n \n @dataclass\n class Evidence:\n--- /Users/nemesis/llamaagent/src/llamaagent/tools/dynamic_loader.py:before\t2025-07-06 06:11:55.913440\n+++ /Users/nemesis/llamaagent/src/llamaagent/tools/dynamic_loader.py:after\t2025-07-07 12:16:49.111964\n@@ -340,14 +340,10 @@\n                 )\n \n                 # Import our tool classes\n-                from .tool_registry import (\n-                    Tool,\n-                    ToolCategory,\n-                    ToolExecutionContext,\n-                    ToolParameter,\n-                    ToolResult,\n-                    ToolSecurityLevel,\n-                )\n+                from .tool_registry import (Tool, ToolCategory,\n+                                            ToolExecutionContext,\n+                                            ToolParameter, ToolResult,\n+                                            ToolSecurityLevel)\n \n                 restricted_globals.update(\n                     {\n--- /Users/nemesis/llamaagent/src/llamaagent/tools/openai_tools.py:before\t2025-07-07 08:10:50.935212\n+++ /Users/nemesis/llamaagent/src/llamaagent/tools/openai_tools.py:after\t2025-07-07 12:16:49.120082\n@@ -23,10 +23,8 @@\n from typing import Any, Dict, List, Optional, Union\n \n from ..integration.openai_comprehensive import (\n-    OpenAIComprehensiveIntegration,\n-    OpenAIModelType,\n-    create_comprehensive_openai_integration,\n-)\n+    OpenAIComprehensiveIntegration, OpenAIModelType,\n+    create_comprehensive_openai_integration)\n from .base import BaseTool\n \n logger = logging.getLogger(__name__)\n--- /Users/nemesis/llamaagent/src/llamaagent/tools/__init__.py:before\t2025-07-06 06:10:17.673565\n+++ /Users/nemesis/llamaagent/src/llamaagent/tools/__init__.py:after\t2025-07-07 12:16:49.122873\n@@ -24,8 +24,9 @@\n \n # Optional imports with graceful fallback\n try:\n-    from .registry import ToolLoader, create_loader, get_registry\n+    from .registry import ToolLoader\n     from .registry import ToolMetadata as RegistryToolMetadata\n+    from .registry import create_loader, get_registry\n except (ImportError, SyntaxError):\n     ToolLoader = None\n     RegistryToolMetadata = None\n@@ -34,15 +35,9 @@\n \n try:\n     from .tool_registry import Tool as ToolRegistryTool\n-    from .tool_registry import (\n-        ToolCategory,\n-        ToolExecutionContext,\n-        ToolMetadata,\n-        ToolParameter,\n-        ToolResult,\n-        ToolSecurityLevel,\n-        ToolValidator,\n-    )\n+    from .tool_registry import (ToolCategory, ToolExecutionContext,\n+                                ToolMetadata, ToolParameter, ToolResult,\n+                                ToolSecurityLevel, ToolValidator)\n except (ImportError, SyntaxError):\n     ToolRegistryTool = None\n     ToolCategory = None\n@@ -61,7 +56,8 @@\n     DynamicToolMetadata = None\n \n try:\n-    from .plugin_framework import Plugin, PluginFramework, PluginManager, PluginState\n+    from .plugin_framework import (Plugin, PluginFramework, PluginManager,\n+                                   PluginState)\n except (ImportError, SyntaxError):\n     Plugin = None\n     PluginFramework = None\n--- /Users/nemesis/llamaagent/src/llamaagent/tools/plugin_framework.py:before\t2025-07-06 06:10:17.688504\n+++ /Users/nemesis/llamaagent/src/llamaagent/tools/plugin_framework.py:after\t2025-07-07 12:16:49.125723\n@@ -3,6 +3,7 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n+\n \n class PluginFramework:\n     \"\"\"Plugin framework for dynamic tool loading.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/mlx_provider.py:before\t2025-07-06 06:10:17.694967\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/mlx_provider.py:after\t2025-07-07 12:16:49.132351\n@@ -1,7 +1,8 @@\n \"\"\"MLX provider for Apple Silicon optimization.\"\"\"\n \n from typing import Any, Dict, List, Optional\n-from .base import LLMProvider, LLMMessage, LLMResponse\n+\n+from .base import LLMMessage, LLMProvider, LLMResponse\n \n \n class MlxProvider(LLMProvider):\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/__init__.py:before\t2025-07-06 06:15:02.272343\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/__init__.py:after\t2025-07-07 12:16:49.134191\n@@ -14,32 +14,17 @@\n # Core message and response types\n # Base classes\n from .base import LLMProvider\n-\n # Exceptions\n-from .exceptions import (\n-    AuthenticationError,\n-    ConfigurationError,\n-    LLMError,\n-    ModelNotFoundError,\n-    NetworkError,\n-    ProviderError,\n-    RateLimitError,\n-    TokenLimitError,\n-)\n-\n+from .exceptions import (AuthenticationError, ConfigurationError, LLMError,\n+                         ModelNotFoundError, NetworkError, ProviderError,\n+                         RateLimitError, TokenLimitError)\n # Factory and utilities\n from .factory import LLMFactory\n from .messages import LLMMessage, LLMResponse\n-\n # Provider registry\n-from .providers import (\n-    MockProvider,\n-    ProviderFactory,\n-    create_provider,\n-    get_available_providers,\n-    get_provider_class,\n-    is_provider_available,\n-)\n+from .providers import (MockProvider, ProviderFactory, create_provider,\n+                        get_available_providers, get_provider_class,\n+                        is_provider_available)\n from .providers.base_provider import BaseLLMProvider\n \n # Global factory instance\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/factory.py:before\t2025-07-06 06:16:03.374177\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/factory.py:after\t2025-07-07 12:16:49.137283\n@@ -15,7 +15,6 @@\n \n # Import base provider - this is required for the system to work\n from .providers.base_provider import BaseLLMProvider\n-\n \n # Import providers with optional dependencies\n _PROVIDER_IMPORTS = {\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/__init__.py:before\t2025-07-06 06:14:45.660542\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/__init__.py:after\t2025-07-07 12:16:49.151408\n@@ -14,7 +14,6 @@\n # Base provider - always available\n from .base import BaseProvider\n from .base_provider import BaseLLMProvider\n-\n # Mock provider - always available for testing/fallback\n from .mock_provider import MockProvider\n \n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/together_provider.py:before\t2025-07-06 06:10:17.728983\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/together_provider.py:after\t2025-07-07 12:16:49.158541\n@@ -3,8 +3,10 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n+\n from ..base import BaseLLMProvider\n from ..messages import LLMMessage, LLMResponse\n+\n \n class TogetherProvider(BaseLLMProvider):\n     \"\"\"Together AI provider implementation.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/litellm_provider.py:before\t2025-07-07 08:35:49.539496\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/litellm_provider.py:after\t2025-07-07 12:16:49.161035\n@@ -335,7 +335,7 @@\n             # LiteLLM supports many models, we'll do a basic check\n             # by trying to get model info\n             import litellm\n-            \n+\n             # Check if it's a known model pattern\n             known_prefixes = [\n                 \"gpt-\", \"claude-\", \"command\", \"j2-\", \"text-\",\n--- /Users/nemesis/llamaagent/src/llamaagent/llm/providers/together.py:before\t2025-07-06 06:10:17.737087\n+++ /Users/nemesis/llamaagent/src/llamaagent/llm/providers/together.py:after\t2025-07-07 12:16:49.163481\n@@ -56,6 +56,7 @@\n         if self._client is None:\n             try:\n                 from openai import AsyncOpenAI\n+\n                 # Together uses OpenAI-compatible API\n                 self._client = AsyncOpenAI(\n                     api_key=self.api_key,\n--- /Users/nemesis/llamaagent/src/llamaagent/core/__init__.py:before\t2025-07-06 06:10:17.774455\n+++ /Users/nemesis/llamaagent/src/llamaagent/core/__init__.py:after\t2025-07-07 12:16:49.179919\n@@ -4,25 +4,14 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n-from .agent import Agent, AgentCapability, AgentContext, AgentMessage, AgentState\n-from .error_handling import (\n-    CircuitBreaker,\n-    ErrorContext,\n-    ErrorHandler,\n-    ErrorSeverity,\n-    RecoveryResult,\n-    RecoveryStrategy,\n-    get_error_handler,\n-    with_error_handling,\n-)\n+from .agent import (Agent, AgentCapability, AgentContext, AgentMessage,\n+                    AgentState)\n+from .error_handling import (CircuitBreaker, ErrorContext, ErrorHandler,\n+                             ErrorSeverity, RecoveryResult, RecoveryStrategy,\n+                             get_error_handler, with_error_handling)\n from .message_bus import MessageBus, MessageRouter\n-from .orchestrator import (\n-    DistributedOrchestrator,\n-    Task,\n-    TaskPriority,\n-    TaskStatus,\n-    Workflow,\n-)\n+from .orchestrator import (DistributedOrchestrator, Task, TaskPriority,\n+                           TaskStatus, Workflow)\n from .service_mesh import ServiceMesh\n \n __all__ = [\n--- /Users/nemesis/llamaagent/src/llamaagent/core/error_handling.py:before\t2025-07-06 06:10:17.782104\n+++ /Users/nemesis/llamaagent/src/llamaagent/core/error_handling.py:after\t2025-07-07 12:16:49.184867\n@@ -16,15 +16,9 @@\n \n # Optional imports\n try:\n-    from tenacity import (\n-        RetryError,\n-        retry,\n-        retry_if_exception_type,\n-        stop_after_attempt,\n-        stop_after_delay,\n-        wait_exponential,\n-        wait_random_exponential,\n-    )\n+    from tenacity import (RetryError, retry, retry_if_exception_type,\n+                          stop_after_attempt, stop_after_delay,\n+                          wait_exponential, wait_random_exponential)\n \n     TENACITY_AVAILABLE = True\n except ImportError:\n--- /Users/nemesis/llamaagent/src/llamaagent/memory/__init__.py:before\t2025-07-06 06:19:12.739063\n+++ /Users/nemesis/llamaagent/src/llamaagent/memory/__init__.py:after\t2025-07-07 12:16:49.192012\n@@ -1,6 +1,7 @@\n \"\"\"Memory module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict, List, Optional\n+\n \n class MemoryManager:\n     \"\"\"Basic memory manager.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/cache/memory_pool.py:before\t2025-07-06 06:10:17.809505\n+++ /Users/nemesis/llamaagent/src/llamaagent/cache/memory_pool.py:after\t2025-07-07 12:16:49.197250\n@@ -3,6 +3,7 @@\n \"\"\"\n \n from typing import Any, Dict, Optional\n+\n \n class MemoryPool:\n     \"\"\"Memory pool for caching.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/cache/llm_cache.py:before\t2025-07-07 08:35:49.500734\n+++ /Users/nemesis/llamaagent/src/llamaagent/cache/llm_cache.py:after\t2025-07-07 12:16:49.200722\n@@ -319,7 +319,7 @@\n \n             def sync_wrapper(*args, **kwargs):\n                 import asyncio\n-                \n+\n                 # Generate cache key\n                 cache_key = key_generator(func.__name__, *args, **kwargs)\n \n--- /Users/nemesis/llamaagent/src/llamaagent/cache/query_optimizer.py:before\t2025-07-07 08:35:49.501080\n+++ /Users/nemesis/llamaagent/src/llamaagent/cache/query_optimizer.py:after\t2025-07-07 12:16:49.203551\n@@ -13,14 +13,13 @@\n \"\"\"\n \n import asyncio\n+import hashlib\n import logging\n+import time\n from collections import defaultdict\n from dataclasses import dataclass, field\n from enum import Enum\n from typing import Any, Callable, Dict, List, Optional, Tuple\n-import time\n-import hashlib\n-\n \n logger = logging.getLogger(__name__)\n \n--- /Users/nemesis/llamaagent/src/llamaagent/cache/__init__.py:before\t2025-07-06 06:19:12.739295\n+++ /Users/nemesis/llamaagent/src/llamaagent/cache/__init__.py:after\t2025-07-07 12:16:49.204417\n@@ -1,6 +1,7 @@\n \"\"\"Cache module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict, Optional\n+\n \n class CacheManager:\n     \"\"\"Basic cache manager.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/cache/advanced_cache.py:before\t2025-07-06 06:10:17.823185\n+++ /Users/nemesis/llamaagent/src/llamaagent/cache/advanced_cache.py:after\t2025-07-07 12:16:49.206522\n@@ -2,8 +2,8 @@\n Advanced cache implementation with multiple strategies and features.\n \"\"\"\n \n+import asyncio\n import logging\n-import asyncio\n import pickle\n import time\n from collections import OrderedDict, defaultdict\n@@ -11,6 +11,7 @@\n from enum import Enum\n from pathlib import Path\n from typing import Any, Dict, List, Optional, Tuple\n+\n logger = logging.getLogger(__name__)\n \n try:\n--- /Users/nemesis/llamaagent/src/llamaagent/config/__init__.py:before\t2025-07-06 06:19:12.739484\n+++ /Users/nemesis/llamaagent/src/llamaagent/config/__init__.py:after\t2025-07-07 12:16:49.222884\n@@ -1,6 +1,7 @@\n \"\"\"Configuration module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict, Optional\n+\n \n class ConfigManager:\n     \"\"\"Basic configuration manager.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/integration/openai_agents_complete.py:before\t2025-07-07 08:35:49.505785\n+++ /Users/nemesis/llamaagent/src/llamaagent/integration/openai_agents_complete.py:after\t2025-07-07 12:16:49.241539\n@@ -33,7 +33,6 @@\n \n from ..llm.factory import LLMFactory\n from ..types import LLMMessage\n-\n \n logger = logging.getLogger(__name__)\n \n--- /Users/nemesis/llamaagent/src/llamaagent/integration/simon_tools.py:before\t2025-07-06 06:10:17.904476\n+++ /Users/nemesis/llamaagent/src/llamaagent/integration/simon_tools.py:after\t2025-07-07 12:16:49.251843\n@@ -22,7 +22,6 @@\n from typing import Any, Dict, List, Optional\n \n from ..tools.base import Tool, ToolResult\n-\n \n logger = logging.getLogger(__name__)\n \n--- /Users/nemesis/llamaagent/src/llamaagent/optimization/__init__.py:before\t2025-07-06 06:10:17.911486\n+++ /Users/nemesis/llamaagent/src/llamaagent/optimization/__init__.py:after\t2025-07-07 12:16:49.256220\n@@ -8,18 +8,11 @@\n \"\"\"\n \n # Import from performance module\n-from .performance import (\n-    AdaptiveOptimizer,\n-    AsyncParallelizer,\n-    BatchProcessor,\n-    LazyLoader,\n-    PerformanceOptimizer,\n-    ResourceMonitor,\n-    ResourcePool,\n-    get_optimizer,\n-    optimize_parallel,\n-)\n+from .performance import (AdaptiveOptimizer, AsyncParallelizer, BatchProcessor,\n+                          LazyLoader)\n from .performance import OptimizationStrategy as PerfOptimizationStrategy\n+from .performance import (PerformanceOptimizer, ResourceMonitor, ResourcePool,\n+                          get_optimizer, optimize_parallel)\n \n # Import from prompt optimizer if available\n try:\n--- /Users/nemesis/llamaagent/src/llamaagent/optimization/prompt_optimizer.py:before\t2025-07-07 08:35:49.507754\n+++ /Users/nemesis/llamaagent/src/llamaagent/optimization/prompt_optimizer.py:after\t2025-07-07 12:16:49.259417\n@@ -8,14 +8,14 @@\n import asyncio\n import random\n import statistics\n+from abc import ABC, abstractmethod\n from dataclasses import dataclass, field\n from datetime import datetime, timezone\n from enum import Enum\n from pathlib import Path\n-from typing import Any, Dict, List, Optional, Callable, Tuple\n+from typing import Any, Callable, Dict, List, Optional, Tuple\n \n import numpy as np\n-from abc import ABC, abstractmethod\n \n \n class OptimizationStrategy(Enum):\n--- /Users/nemesis/llamaagent/src/llamaagent/optimization/performance.py:before\t2025-07-06 06:10:17.919347\n+++ /Users/nemesis/llamaagent/src/llamaagent/optimization/performance.py:after\t2025-07-07 12:16:49.263602\n@@ -105,7 +105,7 @@\n         \"\"\"Collect resource metrics.\"\"\"\n         try:\n             import psutil\n-            \n+\n             # CPU usage\n             cpu_percent = psutil.cpu_percent(interval=None)\n             self.cpu_usage_history.append(cpu_percent)\n--- /Users/nemesis/llamaagent/src/llamaagent/spawning/__init__.py:before\t2025-07-06 06:10:17.926161\n+++ /Users/nemesis/llamaagent/src/llamaagent/spawning/__init__.py:after\t2025-07-07 12:16:49.267700\n@@ -9,21 +9,11 @@\n \"\"\"\n \n from .agent_pool import AgentPool, PoolConfig, PoolStats\n-from .agent_spawner import (\n-    AgentHierarchy,\n-    AgentRelationship,\n-    AgentSpawner,\n-    SpawnConfig,\n-    SpawnResult,\n-)\n-from .communication import (\n-    AgentChannel,\n-    BroadcastChannel,\n-    CommunicationProtocol,\n-    DirectChannel,\n-    Message,\n-    MessageType,\n-)\n+from .agent_spawner import (AgentHierarchy, AgentRelationship, AgentSpawner,\n+                            SpawnConfig, SpawnResult)\n+from .communication import (AgentChannel, BroadcastChannel,\n+                            CommunicationProtocol, DirectChannel, Message,\n+                            MessageType)\n \n __all__ = [\n     # Core spawning classes\n--- /Users/nemesis/llamaagent/src/llamaagent/planning/__init__.py:before\t2025-07-06 06:10:17.956680\n+++ /Users/nemesis/llamaagent/src/llamaagent/planning/__init__.py:after\t2025-07-07 12:16:49.283599\n@@ -7,52 +7,22 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n-from .execution_engine import (\n-    AdaptiveExecutor,\n-    ExecutionContext,\n-    ExecutionEngine,\n-    ExecutionResult,\n-    ExecutionStatus,\n-    ParallelExecutor,\n-    ProgressTracker,\n-    TaskScheduler,\n-)\n-from .monitoring import (\n-    AlertManager,\n-    CheckpointManager,\n-    ExecutionMonitor,\n-    ExecutionReport,\n-    PerformanceMetrics,\n-)\n-from .optimization import (\n-    CostOptimizer,\n-    OptimizationConstraint,\n-    OptimizationObjective,\n-    OptimizationResult,\n-    PlanOptimizer,\n-    QualityOptimizer,\n-    ResourceAllocator,\n-    RiskOptimizer,\n-    TimeOptimizer,\n-)\n-from .strategies import (\n-    ConstraintBasedPlanning,\n-    GoalOrientedPlanning,\n-    HierarchicalTaskNetwork,\n-    PlanningStrategy,\n-    ProbabilisticPlanning,\n-    ReactivePlanning,\n-)\n-from .task_planner import (\n-    DependencyResolver,\n-    PlanValidator,\n-    Task,\n-    TaskDecomposer,\n-    TaskDependency,\n-    TaskPlan,\n-    TaskPlanner,\n-    TaskPriority,\n-)\n+from .execution_engine import (AdaptiveExecutor, ExecutionContext,\n+                               ExecutionEngine, ExecutionResult,\n+                               ExecutionStatus, ParallelExecutor,\n+                               ProgressTracker, TaskScheduler)\n+from .monitoring import (AlertManager, CheckpointManager, ExecutionMonitor,\n+                         ExecutionReport, PerformanceMetrics)\n+from .optimization import (CostOptimizer, OptimizationConstraint,\n+                           OptimizationObjective, OptimizationResult,\n+                           PlanOptimizer, QualityOptimizer, ResourceAllocator,\n+                           RiskOptimizer, TimeOptimizer)\n+from .strategies import (ConstraintBasedPlanning, GoalOrientedPlanning,\n+                         HierarchicalTaskNetwork, PlanningStrategy,\n+                         ProbabilisticPlanning, ReactivePlanning)\n+from .task_planner import (DependencyResolver, PlanValidator, Task,\n+                           TaskDecomposer, TaskDependency, TaskPlan,\n+                           TaskPlanner, TaskPriority)\n \n __all__ = [\n     # Task Planning\n--- /Users/nemesis/llamaagent/src/llamaagent/agents/multimodal_advanced.py:before\t2025-07-06 06:21:00.187860\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/multimodal_advanced.py:after\t2025-07-07 12:16:49.290512\n@@ -2,9 +2,10 @@\n Multimodal advanced agent implementation.\n \"\"\"\n \n-from typing import Any, Dict, List, Optional\n from dataclasses import dataclass, field\n from enum import Enum\n+from typing import Any, Dict, List, Optional\n+\n \n class ModalityType(Enum):\n     \"\"\"Types of modalities.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/agents/__init__.py:before\t2025-07-06 06:10:17.971099\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/__init__.py:after\t2025-07-07 12:16:49.291817\n@@ -6,20 +6,11 @@\n \n # Core building blocks\n # Advanced agent implementations\n-from .advanced_reasoning import (\n-    AdvancedReasoningAgent,\n-    ReasoningStrategy,\n-    ReasoningTrace,\n-    ThoughtNode,\n-)\n+from .advanced_reasoning import (AdvancedReasoningAgent, ReasoningStrategy,\n+                                 ReasoningTrace, ThoughtNode)\n from .base import AgentConfig, AgentResponse, AgentRole, BaseAgent\n-from .multimodal_advanced import (\n-    CrossModalContext,\n-    ModalityData,\n-    ModalityType,\n-    MultiModalAdvancedAgent,\n-)\n-\n+from .multimodal_advanced import (CrossModalContext, ModalityData,\n+                                  ModalityType, MultiModalAdvancedAgent)\n # Concrete agent implementations\n from .react import ReactAgent\n \n--- /Users/nemesis/llamaagent/src/llamaagent/agents/reasoning_chains.py:before\t2025-07-07 08:35:49.513015\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/reasoning_chains.py:after\t2025-07-07 12:16:49.294388\n@@ -12,8 +12,8 @@\n from enum import Enum\n from typing import Any, Dict, List, Optional\n \n+from ..llm.factory import LLMFactory\n from ..llm.messages import LLMMessage, LLMResponse\n-from ..llm.factory import LLMFactory\n from ..types import TaskInput, TaskOutput, TaskResult, TaskStatus\n from .base import AgentConfig, BaseAgent\n \n--- /Users/nemesis/llamaagent/src/llamaagent/agents/base.py:before\t2025-07-07 09:12:14.653988\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/base.py:after\t2025-07-07 12:16:49.299105\n@@ -24,7 +24,8 @@\n     if TYPE_CHECKING:\n         from llamaagent.memory.base import SimpleMemory\n         from llamaagent.tools import ToolRegistry\n-        from llamaagent.types import TaskInput, TaskOutput, TaskResult, TaskStatus\n+        from llamaagent.types import (TaskInput, TaskOutput, TaskResult,\n+                                      TaskStatus)\n     else:\n         # Runtime fallbacks\n         SimpleMemory = None\n--- /Users/nemesis/llamaagent/src/llamaagent/agents/advanced_reasoning.py:before\t2025-07-07 09:15:26.278208\n+++ /Users/nemesis/llamaagent/src/llamaagent/agents/advanced_reasoning.py:after\t2025-07-07 12:16:49.306800\n@@ -7,8 +7,8 @@\n Author: LlamaAgent Team\n \"\"\"\n \n+import json\n import logging\n-import json\n import time\n from dataclasses import dataclass, field\n from enum import Enum\n@@ -18,6 +18,7 @@\n from ..tools import ToolRegistry\n from .base import AgentConfig\n from .base import BaseAgent as Agent\n+\n logger = logging.getLogger(__name__)\n \n \n--- /Users/nemesis/llamaagent/src/llamaagent/cli/enhanced_cli.py:before\t2025-07-06 06:10:17.997624\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/enhanced_cli.py:after\t2025-07-07 12:16:49.321530\n@@ -19,18 +19,18 @@\n from typing import Any, Dict, List, Optional\n \n try:\n+    from rich.align import Align\n     from rich.console import Console\n     from rich.panel import Panel\n+    from rich.prompt import Confirm\n     from rich.table import Table\n     from rich.text import Text\n-    from rich.align import Align\n-    from rich.prompt import Confirm\n     rich_available = True\n except ImportError:\n     rich_available = False\n \n+from ..config.settings import AgentConfig\n from ..core.agent import ReactAgent\n-from ..config.settings import AgentConfig\n \n logger = logging.getLogger(__name__)\n console = Console() if rich_available else None\n--- /Users/nemesis/llamaagent/src/llamaagent/cli/master_cli.py:before\t2025-07-07 09:21:52.931780\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/master_cli.py:after\t2025-07-07 12:16:49.332891\n@@ -21,7 +21,8 @@\n from rich.table import Table\n from rich.tree import Tree\n \n-from ..agents import AdvancedReasoningAgent, MultiModalAdvancedAgent, ReactAgent\n+from ..agents import (AdvancedReasoningAgent, MultiModalAdvancedAgent,\n+                      ReactAgent)\n from ..agents.base import AgentConfig\n from ..cache import AdvancedCache, CacheStrategy\n from ..core import get_error_handler\n--- /Users/nemesis/llamaagent/src/llamaagent/cli/function_manager.py.bak:before\t2025-07-05 10:36:44.985080\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/function_manager.py.bak:after\t2025-07-07 12:16:49.349892\n@@ -18,6 +18,7 @@\n from dataclasses import dataclass\n from datetime import datetime\n from pathlib import Path\n+\n from typing import (\"DELETE\"], \"GET\", \"HTTP, \"POST\", \"PUT\", \"URL, %H:%M:%S\",\n                     False, 30, \", \"1\", \"%Y-%m-%d, \"*\", \".\", \"a\"], \"Command,\n                     \"Content, \"content\":, \"content\"], \"data\":, \"default\":,\n--- /Users/nemesis/llamaagent/src/llamaagent/cli/openai_cli.py:before\t2025-07-06 06:10:18.023450\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/openai_cli.py:after\t2025-07-07 12:16:49.361624\n@@ -32,11 +32,8 @@\n # Optional imports with fallbacks\n try:\n     from llamaagent.integration.openai_agents import (\n-        OPENAI_AGENTS_AVAILABLE,\n-        OpenAIAgentMode,\n-        OpenAIIntegrationConfig,\n-        create_openai_integration,\n-    )\n+        OPENAI_AGENTS_AVAILABLE, OpenAIAgentMode, OpenAIIntegrationConfig,\n+        create_openai_integration)\n except ImportError:\n     openai_agents_available = False\n     OpenAIAgentMode = None\n--- /Users/nemesis/llamaagent/src/llamaagent/cli/interactive.py:before\t2025-07-06 06:10:18.031178\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/interactive.py:after\t2025-07-07 12:16:49.366990\n@@ -13,7 +13,7 @@\n from rich.console import Console\n from rich.panel import Panel\n from rich.progress import Progress, SpinnerColumn, TextColumn\n-from rich.prompt import Prompt, Confirm\n+from rich.prompt import Confirm, Prompt\n from rich.syntax import Syntax\n from rich.table import Table\n \n--- /Users/nemesis/llamaagent/src/llamaagent/cli/shell_commands.py.bak:before\t2025-07-05 10:36:44.991562\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/shell_commands.py.bak:after\t2025-07-07 12:16:49.373685\n@@ -24,6 +24,7 @@\n                     \"shell\":, \"unknown\", .split, [-1], os.environ.get,\n                     platform.machine, platform.python_version,\n                     platform.release)\n+\n                 if os.name == \"posix\"\n                 else \"cmd\"\n             ), \"version\": platform.version(), Any, Any]] = None\n--- /Users/nemesis/llamaagent/src/llamaagent/cli/llm_cmd.py:before\t2025-07-07 08:35:49.518788\n+++ /Users/nemesis/llamaagent/src/llamaagent/cli/llm_cmd.py:after\t2025-07-07 12:16:49.387069\n@@ -6,9 +6,9 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n-import logging\n import asyncio\n import json\n+import logging\n import sqlite3\n from datetime import datetime, timezone\n from typing import Any, Dict, List, Optional\n@@ -21,6 +21,7 @@\n from rich.table import Table\n \n from llamaagent.llm.factory import LLMFactory\n+\n logger = logging.getLogger(__name__)\n \n \n--- /Users/nemesis/llamaagent/src/llamaagent/reasoning/__init__.py:before\t2025-07-06 06:10:18.060893\n+++ /Users/nemesis/llamaagent/src/llamaagent/reasoning/__init__.py:after\t2025-07-07 12:16:49.394996\n@@ -4,16 +4,10 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n-from .chain_engine import (\n-    AdvancedReasoningEngine,\n-    CausalReasoning,\n-    DeductiveReasoning,\n-    InductiveReasoning,\n-    ReasoningContext,\n-    ReasoningStrategy,\n-    ReasoningType,\n-    ThoughtNode,\n-)\n+from .chain_engine import (AdvancedReasoningEngine, CausalReasoning,\n+                           DeductiveReasoning, InductiveReasoning,\n+                           ReasoningContext, ReasoningStrategy, ReasoningType,\n+                           ThoughtNode)\n from .context_sharing import ContextSharingProtocol, SharedContext\n from .memory_manager import MemoryItem, MemoryManager, MemoryType\n \n--- /Users/nemesis/llamaagent/src/llamaagent/reasoning/chain_engine.py:before\t2025-07-07 08:35:49.520490\n+++ /Users/nemesis/llamaagent/src/llamaagent/reasoning/chain_engine.py:after\t2025-07-07 12:16:49.400661\n@@ -12,13 +12,13 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n+import logging\n import uuid\n-import logging\n from abc import ABC, abstractmethod\n from dataclasses import dataclass, field\n from datetime import datetime, timezone\n from enum import Enum\n-from typing import Dict, List, Optional, Any, Tuple\n+from typing import Any, Dict, List, Optional, Tuple\n \n try:\n     from opentelemetry import trace\n--- /Users/nemesis/llamaagent/src/llamaagent/knowledge/knowledge_generator.py:before\t2025-07-07 08:35:49.521347\n+++ /Users/nemesis/llamaagent/src/llamaagent/knowledge/knowledge_generator.py:after\t2025-07-07 12:16:49.409579\n@@ -15,7 +15,7 @@\n from datetime import datetime, timezone\n from enum import Enum\n from pathlib import Path\n-from typing import Dict, List, Optional, Any, Tuple, Union\n+from typing import Any, Dict, List, Optional, Tuple, Union\n \n try:\n     from opentelemetry import trace\n--- /Users/nemesis/llamaagent/src/llamaagent/knowledge/__init__.py:before\t2025-07-06 06:19:12.739681\n+++ /Users/nemesis/llamaagent/src/llamaagent/knowledge/__init__.py:after\t2025-07-07 12:16:49.410615\n@@ -1,6 +1,7 @@\n \"\"\"Knowledge module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict, List\n+\n \n class KnowledgeBase:\n     \"\"\"Basic knowledge base.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/ml/inference_engine.py:before\t2025-07-07 08:35:49.522038\n+++ /Users/nemesis/llamaagent/src/llamaagent/ml/inference_engine.py:after\t2025-07-07 12:16:49.416849\n@@ -25,7 +25,7 @@\n from dataclasses import dataclass, field\n from datetime import datetime, timezone\n from enum import Enum\n-from typing import Dict, List, Optional, Any, Callable, Tuple, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n # Optional imports with fallbacks\n try:\n@@ -57,7 +57,7 @@\n     ort = None\n \n try:\n-    from transformers import AutoTokenizer, AutoModel\n+    from transformers import AutoModel, AutoTokenizer\n     TRANSFORMERS_AVAILABLE = True\n except ImportError:\n     TRANSFORMERS_AVAILABLE = False\n--- /Users/nemesis/llamaagent/src/llamaagent/ml/__init__.py:before\t2025-07-06 06:19:12.739768\n+++ /Users/nemesis/llamaagent/src/llamaagent/ml/__init__.py:after\t2025-07-07 12:16:49.417934\n@@ -1,6 +1,7 @@\n \"\"\"ML module for LlamaAgent.\"\"\"\n \n from typing import Any, List\n+\n \n class MLModel:\n     \"\"\"Basic ML model interface.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/benchmarks/baseline_agents.py:before\t2025-07-06 06:10:18.128820\n+++ /Users/nemesis/llamaagent/src/llamaagent/benchmarks/baseline_agents.py:after\t2025-07-07 12:16:49.427283\n@@ -10,11 +10,11 @@\n from enum import Enum\n from typing import Any, Dict, List\n \n-from ..types import AgentConfig\n from ..agents.base import AgentRole, ExecutionPlan, PlanStep\n from ..agents.react import ReactAgent\n from ..llm import LLMProvider\n from ..tools import ToolRegistry, get_all_tools\n+from ..types import AgentConfig\n \n \n class BaselineType(str, Enum):\n--- /Users/nemesis/llamaagent/src/llamaagent/diagnostics/master_diagnostics.py:before\t2025-07-07 08:35:49.525151\n+++ /Users/nemesis/llamaagent/src/llamaagent/diagnostics/master_diagnostics.py:after\t2025-07-07 12:16:49.443879\n@@ -19,6 +19,7 @@\n from enum import Enum\n from pathlib import Path\n from typing import Any, Dict, List, Optional\n+\n logger = logging.getLogger(__name__)\n \n # Optional imports for advanced analysis\n--- /Users/nemesis/llamaagent/src/llamaagent/diagnostics/__init__.py:before\t2025-07-06 06:10:18.162717\n+++ /Users/nemesis/llamaagent/src/llamaagent/diagnostics/__init__.py:after\t2025-07-07 12:16:49.447568\n@@ -6,7 +6,8 @@\n \n from .code_analyzer import CodeAnalyzer\n from .dependency_checker import DependencyChecker\n-from .master_diagnostics import DiagnosticReport, MasterDiagnostics, ProblemSeverity\n+from .master_diagnostics import (DiagnosticReport, MasterDiagnostics,\n+                                 ProblemSeverity)\n from .system_validator import SystemValidator\n \n __all__ = [\n--- /Users/nemesis/llamaagent/src/llamaagent/api/production_app.py:before\t2025-07-07 08:35:49.527050\n+++ /Users/nemesis/llamaagent/src/llamaagent/api/production_app.py:after\t2025-07-07 12:16:49.463254\n@@ -32,21 +32,9 @@\n from typing import Any, Dict, List, Optional, Set\n \n import uvicorn\n-from fastapi import (\n-    BackgroundTasks,\n-    Body,\n-    Depends,\n-    FastAPI,\n-    File,\n-    HTTPException,\n-    Request,\n-    Response,\n-    Security,\n-    UploadFile,\n-    WebSocket,\n-    WebSocketDisconnect,\n-    status,\n-)\n+from fastapi import (BackgroundTasks, Body, Depends, FastAPI, File,\n+                     HTTPException, Request, Response, Security, UploadFile,\n+                     WebSocket, WebSocketDisconnect, status)\n from fastapi.middleware.cors import CORSMiddleware\n from fastapi.middleware.gzip import GZipMiddleware\n from fastapi.middleware.trustedhost import TrustedHostMiddleware\n@@ -97,10 +85,8 @@\n     GAIABenchmark = None\n \n try:\n-    from ..integration.openai_agents import (\n-        OPENAI_AGENTS_AVAILABLE,\n-        OpenAIAgentsIntegration,\n-    )\n+    from ..integration.openai_agents import (OPENAI_AGENTS_AVAILABLE,\n+                                             OpenAIAgentsIntegration)\n except ImportError:\n     OpenAIAgentsIntegration = None\n     OPENAI_AGENTS_AVAILABLE = False\n@@ -658,7 +644,7 @@\n async def health_check():\n     \"\"\"Comprehensive health check endpoint.\"\"\"\n     import psutil\n-    \n+\n     # Get memory usage\n     memory_info = psutil.virtual_memory()\n     \n@@ -1121,8 +1107,9 @@\n     if user.get(\"role\") != \"admin\":\n         raise HTTPException(status_code=403, detail=\"Admin access required\")\n     \n+    import sys\n+\n     import psutil\n-    import sys\n     \n     return {\n         \"system\": {\n--- /Users/nemesis/llamaagent/src/llamaagent/api/openai_comprehensive_api.py:before\t2025-07-07 08:35:49.527382\n+++ /Users/nemesis/llamaagent/src/llamaagent/api/openai_comprehensive_api.py:after\t2025-07-07 12:16:49.468551\n@@ -34,11 +34,8 @@\n from pydantic import BaseModel, Field\n \n from ..integration.openai_comprehensive import (\n-    BudgetExceededError,\n-    OpenAIComprehensiveIntegration,\n-    OpenAIModelType,\n-    create_comprehensive_openai_integration,\n-)\n+    BudgetExceededError, OpenAIComprehensiveIntegration, OpenAIModelType,\n+    create_comprehensive_openai_integration)\n from ..tools.openai_tools import OPENAI_TOOLS, create_openai_tool\n \n # Configure logging\n--- /Users/nemesis/llamaagent/src/llamaagent/api/complete_api.py:before\t2025-07-07 12:12:59.797541\n+++ /Users/nemesis/llamaagent/src/llamaagent/api/complete_api.py:after\t2025-07-07 12:16:49.477190\n@@ -27,21 +27,9 @@\n from uuid import uuid4\n \n import uvicorn\n-from fastapi import (\n-    BackgroundTasks,\n-    Depends,\n-    FastAPI,\n-    File,\n-    Form,\n-    HTTPException,\n-    Query,\n-    Request,\n-    Response,\n-    UploadFile,\n-    WebSocket,\n-    WebSocketDisconnect,\n-    status,\n-)\n+from fastapi import (BackgroundTasks, Depends, FastAPI, File, Form,\n+                     HTTPException, Query, Request, Response, UploadFile,\n+                     WebSocket, WebSocketDisconnect, status)\n from fastapi.middleware.cors import CORSMiddleware\n from fastapi.middleware.gzip import GZipMiddleware\n from fastapi.responses import FileResponse, JSONResponse, StreamingResponse\n@@ -50,13 +38,13 @@\n from pydantic import BaseModel, Field, validator\n from starlette.middleware.base import BaseHTTPMiddleware\n \n-# LlamaAgent imports\n-from ..data_generation.spre import SPREGenerator, DataType, ValidationStatus\n from ..agents.base import AgentConfig, AgentRole\n from ..agents.react import ReactAgent\n+# LlamaAgent imports\n+from ..data_generation.spre import DataType, SPREGenerator, ValidationStatus\n from ..llm.factory import ProviderFactory\n+from ..orchestrator import AgentOrchestrator\n from ..tools import ToolRegistry, get_all_tools\n-from ..orchestrator import AgentOrchestrator\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n--- /Users/nemesis/llamaagent/src/llamaagent/api/main.py:before\t2025-07-07 08:35:49.529268\n+++ /Users/nemesis/llamaagent/src/llamaagent/api/main.py:after\t2025-07-07 12:16:49.492799\n@@ -26,16 +26,8 @@\n from typing import Any, Dict, List, Optional\n \n import uvicorn\n-from fastapi import (\n-    BackgroundTasks,\n-    Body,\n-    Depends,\n-    FastAPI,\n-    HTTPException,\n-    Request,\n-    Response,\n-    Security,\n-)\n+from fastapi import (BackgroundTasks, Body, Depends, FastAPI, HTTPException,\n+                     Request, Response, Security)\n from fastapi.middleware.cors import CORSMiddleware\n from fastapi.middleware.gzip import GZipMiddleware\n from fastapi.responses import JSONResponse, StreamingResponse\n@@ -81,10 +73,8 @@\n     GAIABenchmark = None\n \n try:\n-    from ..integration.openai_agents import (\n-        OPENAI_AGENTS_AVAILABLE,\n-        OpenAIAgentsIntegration,\n-    )\n+    from ..integration.openai_agents import (OPENAI_AGENTS_AVAILABLE,\n+                                             OpenAIAgentsIntegration)\n except ImportError:\n     OpenAIAgentsIntegration = None\n     OPENAI_AGENTS_AVAILABLE = False\n@@ -1017,9 +1007,7 @@\n     \"\"\"Direct OpenAI completions with budget tracking.\"\"\"\n     try:\n         from ..integration.openai_comprehensive import (\n-            OpenAIComprehensiveConfig,\n-            OpenAIComprehensiveIntegration,\n-        )\n+            OpenAIComprehensiveConfig, OpenAIComprehensiveIntegration)\n \n         # Get or create OpenAI integration\n         integration = app_state.get(\"openai_comprehensive\")\n--- /Users/nemesis/llamaagent/src/llamaagent/api/premium_endpoints.py:before\t2025-07-06 06:10:18.206237\n+++ /Users/nemesis/llamaagent/src/llamaagent/api/premium_endpoints.py:after\t2025-07-07 12:16:49.499119\n@@ -3,6 +3,7 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n+\n from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException\n from pydantic import BaseModel, Field\n \n--- /Users/nemesis/llamaagent/src/llamaagent/evaluation/__init__.py:before\t2025-07-06 06:10:18.221843\n+++ /Users/nemesis/llamaagent/src/llamaagent/evaluation/__init__.py:after\t2025-07-07 12:16:49.509782\n@@ -7,16 +7,16 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n-from .a_b_testing import ABTestingFramework, ABTestResult, StatisticalSignificance\n+from .a_b_testing import (ABTestingFramework, ABTestResult,\n+                          StatisticalSignificance)\n from .automated_grading import AutomatedGrader, GradingCriteria, GradingResult\n from .benchmark_engine import BenchmarkEngine, BenchmarkResult, BenchmarkSuite\n-from .evaluation_framework import (\n-    EvaluationFramework,\n-    EvaluationMetrics,\n-    EvaluationReport,\n-)\n-from .golden_dataset import DataQualityReport, DatasetMetrics, GoldenDatasetManager\n-from .model_comparison import ComparisonReport, ModelComparator, ModelPerformance\n+from .evaluation_framework import (EvaluationFramework, EvaluationMetrics,\n+                                   EvaluationReport)\n+from .golden_dataset import (DataQualityReport, DatasetMetrics,\n+                             GoldenDatasetManager)\n+from .model_comparison import (ComparisonReport, ModelComparator,\n+                               ModelPerformance)\n \n __all__ = [\n     \"GoldenDatasetManager\",\n--- /Users/nemesis/llamaagent/src/llamaagent/routing/__init__.py:before\t2025-07-06 06:10:18.245682\n+++ /Users/nemesis/llamaagent/src/llamaagent/routing/__init__.py:after\t2025-07-07 12:16:49.521701\n@@ -9,16 +9,10 @@\n from .ai_router import AIRouter, RoutingDecision\n from .metrics import PerformanceTracker, RoutingMetrics\n from .provider_registry import ProviderCapabilities, ProviderRegistry\n-from .strategies import (\n-    ComplexityBasedRouting,\n-    ConsensusRouting,\n-    CostOptimizedRouting,\n-    HybridRouting,\n-    LanguageBasedRouting,\n-    PerformanceBasedRouting,\n-    RoutingStrategy,\n-    TaskBasedRouting,\n-)\n+from .strategies import (ComplexityBasedRouting, ConsensusRouting,\n+                         CostOptimizedRouting, HybridRouting,\n+                         LanguageBasedRouting, PerformanceBasedRouting,\n+                         RoutingStrategy, TaskBasedRouting)\n from .task_analyzer import TaskAnalyzer, TaskCharacteristics\n \n __all__ = [\n--- /Users/nemesis/llamaagent/src/llamaagent/data_generation/__init__.py:before\t2025-07-06 06:10:18.253230\n+++ /Users/nemesis/llamaagent/src/llamaagent/data_generation/__init__.py:after\t2025-07-07 12:16:49.526934\n@@ -4,17 +4,9 @@\n This package provides data generation capabilities for AI agent training and evaluation.\n \"\"\"\n \n-from ..data.gdt import (\n-    ConversationDataGenerator,\n-    DataType,\n-    GDTDataset,\n-    GDTGenerator,\n-    GDTItem,\n-    GDTTransformer,\n-    GDTValidator,\n-    TextDataGenerator,\n-    ValidationStatus,\n-)\n+from ..data.gdt import (ConversationDataGenerator, DataType, GDTDataset,\n+                        GDTGenerator, GDTItem, GDTTransformer, GDTValidator,\n+                        TextDataGenerator, ValidationStatus)\n \n __all__ = [\n     \"GDTGenerator\",\n--- /Users/nemesis/llamaagent/src/llamaagent/data_generation/spre.py:before\t2025-07-07 12:05:51.499098\n+++ /Users/nemesis/llamaagent/src/llamaagent/data_generation/spre.py:after\t2025-07-07 12:16:49.534753\n@@ -13,7 +13,7 @@\n from dataclasses import dataclass, field\n from datetime import datetime, timezone\n from enum import Enum\n-from typing import Any, Dict, List, Optional, TYPE_CHECKING\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional\n \n import numpy as np\n \n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/logging.py:before\t2025-07-07 08:05:10.449787\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/logging.py:after\t2025-07-07 12:16:49.543070\n@@ -322,7 +322,7 @@\n     # File handler\n     if enable_file and file_path:\n         from logging.handlers import RotatingFileHandler\n-        \n+\n         # Create directory if needed\n         Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n         \n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/metrics.py:before\t2025-07-07 08:29:32.743705\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/metrics.py:after\t2025-07-07 12:16:49.546593\n@@ -18,14 +18,8 @@\n \n # Optional imports\n try:\n-    from prometheus_client import (\n-        CONTENT_TYPE_LATEST,\n-        CollectorRegistry,\n-        Counter,\n-        Gauge,\n-        Histogram,\n-        generate_latest,\n-    )\n+    from prometheus_client import (CONTENT_TYPE_LATEST, CollectorRegistry,\n+                                   Counter, Gauge, Histogram, generate_latest)\n \n     PROMETHEUS_AVAILABLE = True\n except ImportError:\n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/tracing.py:before\t2025-07-07 08:35:49.534990\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/tracing.py:after\t2025-07-07 12:16:49.550687\n@@ -22,13 +22,16 @@\n try:\n     from opentelemetry import trace\n     from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n-    from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n-    from opentelemetry.instrumentation.aiohttp_client import AioHttpClientInstrumentor\n+    from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import \\\n+        OTLPSpanExporter\n+    from opentelemetry.instrumentation.aiohttp_client import \\\n+        AioHttpClientInstrumentor\n     from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor\n     from opentelemetry.instrumentation.redis import RedisInstrumentor\n     from opentelemetry.sdk.resources import Resource\n     from opentelemetry.sdk.trace import TracerProvider\n-    from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n+    from opentelemetry.sdk.trace.export import (BatchSpanProcessor,\n+                                                ConsoleSpanExporter)\n     from opentelemetry.trace import SpanKind, Status, StatusCode\n     \n     OPENTELEMETRY_AVAILABLE = True\n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/__init__.py:before\t2025-07-06 06:19:12.739848\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/__init__.py:after\t2025-07-07 12:16:49.553155\n@@ -1,6 +1,7 @@\n \"\"\"Monitoring module for LlamaAgent.\"\"\"\n \n from typing import Any, Dict\n+\n \n class Monitor:\n     \"\"\"Basic monitoring.\"\"\"\n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/metrics_collector.py:before\t2025-07-06 06:10:18.294231\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/metrics_collector.py:after\t2025-07-07 12:16:49.557600\n@@ -14,7 +14,6 @@\n import asyncio\n import logging\n import os\n-import psutil\n import time\n from contextlib import asynccontextmanager\n from dataclasses import dataclass, field\n@@ -22,17 +21,12 @@\n from enum import Enum\n from typing import Any, Dict, List, Optional\n \n+import psutil\n+\n try:\n-    from prometheus_client import (\n-        Counter,\n-        Gauge,\n-        Histogram,\n-        Info,\n-        Summary,\n-        CollectorRegistry,\n-        generate_latest,\n-        CONTENT_TYPE_LATEST,\n-    )\n+    from prometheus_client import (CONTENT_TYPE_LATEST, CollectorRegistry,\n+                                   Counter, Gauge, Histogram, Info, Summary,\n+                                   generate_latest)\n     PROMETHEUS_AVAILABLE = True\n except ImportError:\n     PROMETHEUS_AVAILABLE = False\n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/advanced_monitoring.py:before\t2025-07-07 08:35:49.536403\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/advanced_monitoring.py:after\t2025-07-07 12:16:49.562208\n@@ -286,10 +286,10 @@\n from opentelemetry.sdk.trace import TracerProvider\n from opentelemetry.sdk.trace.export import BatchSpanProcessor\n from prometheus_client import (Alert, CollectorRegistry, None:, Summary, ->,\n-    [\"agent_id\", [\"agent_type\"], [\"cache_type\"],\n-    [\"component\", [\"component\"], [\"method\",\n-    [\"priority\"], [\"task_type\"], agent_port=6831,\n-    alert:)\n+                               [\"agent_id\", [\"agent_type\"], [\"cache_type\"],\n+                               [\"component\", [\"component\"], [\"method\",\n+                               [\"priority\"], [\"task_type\"], agent_port=6831,\n+                               alert:)\n \n         \"\"\"Send Slack notification\"\"\"\n         # Implement Slack webhook notification\n--- /Users/nemesis/llamaagent/src/llamaagent/monitoring/middleware.py:before\t2025-07-07 07:54:33.974318\n+++ /Users/nemesis/llamaagent/src/llamaagent/monitoring/middleware.py:after\t2025-07-07 12:16:49.568171\n@@ -16,6 +16,7 @@\n from fastapi import FastAPI, Request, Response\n from fastapi.middleware.base import BaseHTTPMiddleware\n from fastapi.responses import JSONResponse\n+\n \n # Simple metrics collector implementation\n class MetricsCollector:\n--- /Users/nemesis/llamaagent/tests/test_simon_ecosystem_integration.py:before\t2025-07-05 11:10:27.371949\n+++ /Users/nemesis/llamaagent/tests/test_simon_ecosystem_integration.py:after\t2025-07-07 12:16:49.581811\n@@ -12,17 +12,12 @@\n \n import pytest\n \n-from src.llamaagent.integration.simon_tools import (\n-    LLMChatTool,\n-    SimonToolRegistry,\n-    SQLiteQueryTool,\n-    create_simon_tools,\n-)\n-from src.llamaagent.llm.simon_ecosystem import (\n-    LLMTool,\n-    SimonEcosystemConfig,\n-    SimonLLMEcosystem,\n-)\n+from src.llamaagent.integration.simon_tools import (LLMChatTool,\n+                                                    SimonToolRegistry,\n+                                                    SQLiteQueryTool,\n+                                                    create_simon_tools)\n+from src.llamaagent.llm.simon_ecosystem import (LLMTool, SimonEcosystemConfig,\n+                                                SimonLLMEcosystem)\n \n \n class TestSimonEcosystemConfig:\n--- /Users/nemesis/llamaagent/tests/test_basic_repl.py:before\t2025-07-07 07:18:49.831231\n+++ /Users/nemesis/llamaagent/tests/test_basic_repl.py:after\t2025-07-07 12:16:49.584845\n@@ -32,7 +32,7 @@\n \n def test_chat_session_basic():\n     \"\"\"Test ChatSession dataclass creation.\"\"\"\n-    from src.llamaagent.cli.chat_repl import ChatSession, ChatMessage\n+    from src.llamaagent.cli.chat_repl import ChatMessage, ChatSession\n     \n     messages = [\n         ChatMessage(role=\"user\", content=\"Hello\", timestamp=time.time()),\n@@ -80,11 +80,12 @@\n @pytest.mark.asyncio\n async def test_chat_engine_basic():\n     \"\"\"Test ChatEngine basic functionality.\"\"\"\n+    from unittest.mock import AsyncMock\n+\n     from src.llamaagent.cli.chat_repl import ChatEngine\n     from src.llamaagent.llm.providers.mock_provider import MockProvider\n     from src.llamaagent.types import LLMResponse\n-    from unittest.mock import AsyncMock\n-    \n+\n     # Create chat engine with mock provider\n     chat_engine = ChatEngine()\n     chat_engine.provider = MockProvider(model_name=\"test-model\")\n@@ -115,9 +116,10 @@\n @pytest.mark.asyncio \n async def test_repl_interface_commands():\n     \"\"\"Test REPL interface command handling.\"\"\"\n+    from unittest.mock import Mock\n+\n     from src.llamaagent.cli.chat_repl import REPLInterface\n-    from unittest.mock import Mock\n-    \n+\n     # Create mock chat engine\n     mock_engine = Mock()\n     mock_engine.session_manager = Mock()\n--- /Users/nemesis/llamaagent/tests/test_spawning.py:before\t2025-07-05 11:10:27.427471\n+++ /Users/nemesis/llamaagent/tests/test_spawning.py:after\t2025-07-07 12:16:49.595057\n@@ -7,24 +7,12 @@\n \n from llamaagent.agents.base import AgentConfig, AgentResponse, AgentRole\n from llamaagent.agents.react import ReactAgent\n-from llamaagent.orchestrator import (\n-    AgentOrchestrator,\n-    OrchestrationStrategy,\n-    WorkflowDefinition,\n-    WorkflowStep,\n-)\n-from llamaagent.spawning import (\n-    AgentChannel,\n-    AgentPool,\n-    AgentSpawner,\n-    BroadcastChannel,\n-    DirectChannel,\n-    Message,\n-    MessageBus,\n-    MessageType,\n-    PoolConfig,\n-    SpawnConfig,\n-)\n+from llamaagent.orchestrator import (AgentOrchestrator, OrchestrationStrategy,\n+                                     WorkflowDefinition, WorkflowStep)\n+from llamaagent.spawning import (AgentChannel, AgentPool, AgentSpawner,\n+                                 BroadcastChannel, DirectChannel, Message,\n+                                 MessageBus, MessageType, PoolConfig,\n+                                 SpawnConfig)\n \n \n class TestAgentSpawner:\n--- /Users/nemesis/llamaagent/tests/test_baseline_agents_comprehensive.py:before\t2025-07-05 11:10:27.095310\n+++ /Users/nemesis/llamaagent/tests/test_baseline_agents_comprehensive.py:after\t2025-07-07 12:16:49.599155\n@@ -6,13 +6,11 @@\n import pytest\n \n from llamaagent.agents.base import AgentConfig, AgentRole\n-from llamaagent.benchmarks.baseline_agents import (\n-    BaselineAgentFactory,\n-    BaselineType,\n-    PreActOnlyAgent,\n-    SEMOnlyAgent,\n-    VanillaReactAgent,\n-)\n+from llamaagent.benchmarks.baseline_agents import (BaselineAgentFactory,\n+                                                   BaselineType,\n+                                                   PreActOnlyAgent,\n+                                                   SEMOnlyAgent,\n+                                                   VanillaReactAgent)\n \n \n class TestBaselineAgentFactory:\n--- /Users/nemesis/llamaagent/tests/test_openai_stub_integration.py:before\t2025-07-05 11:10:27.239263\n+++ /Users/nemesis/llamaagent/tests/test_openai_stub_integration.py:after\t2025-07-07 12:16:49.601737\n@@ -11,10 +11,8 @@\n \n import pytest\n \n-from src.llamaagent.integration._openai_stub import (\n-    install_openai_stub,\n-    uninstall_openai_stub,\n-)\n+from src.llamaagent.integration._openai_stub import (install_openai_stub,\n+                                                     uninstall_openai_stub)\n \n \n class TestOpenAIStubIntegration:\n--- /Users/nemesis/llamaagent/tests/test_production_comprehensive.py:before\t2025-07-06 06:01:44.935069\n+++ /Users/nemesis/llamaagent/tests/test_production_comprehensive.py:after\t2025-07-07 12:16:49.607838\n@@ -21,6 +21,8 @@\n import asyncio\n import json\n import os\n+# Import the production app and components\n+import sys\n import tempfile\n import time\n import uuid\n@@ -31,8 +33,6 @@\n import requests\n from fastapi.testclient import TestClient\n \n-# Import the production app and components\n-import sys\n sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))\n \n from llamaagent.agents.base import AgentConfig, AgentRole\n--- /Users/nemesis/llamaagent/tests/test_master_integration.py:before\t2025-07-05 11:10:27.347097\n+++ /Users/nemesis/llamaagent/tests/test_master_integration.py:after\t2025-07-07 12:16:49.613437\n@@ -13,21 +13,16 @@\n import pytest\n \n from llamaagent import AgentConfig, AgentRole, ReactAgent\n-from llamaagent.integration.openai_agents import (\n-    OpenAIAgentMode,\n-    OpenAIAgentsIntegration,\n-    OpenAIIntegrationConfig,\n-)\n+from llamaagent.integration.openai_agents import (OpenAIAgentMode,\n+                                                  OpenAIAgentsIntegration,\n+                                                  OpenAIIntegrationConfig)\n from llamaagent.llm import create_provider\n from llamaagent.memory import SimpleMemory\n from llamaagent.tools import CalculatorTool, PythonREPLTool, ToolRegistry\n from llamaagent.types import TaskInput, TaskOutput, TaskStatus\n-from src.llamaagent.orchestrator import (\n-    AgentOrchestrator,\n-    OrchestrationStrategy,\n-    WorkflowDefinition,\n-    WorkflowStep,\n-)\n+from src.llamaagent.orchestrator import (AgentOrchestrator,\n+                                         OrchestrationStrategy,\n+                                         WorkflowDefinition, WorkflowStep)\n \n \n class TestMasterIntegration:\n--- /Users/nemesis/llamaagent/tests/test_comprehensive_coverage.py:before\t2025-07-05 11:10:27.315501\n+++ /Users/nemesis/llamaagent/tests/test_comprehensive_coverage.py:after\t2025-07-07 12:16:49.620343\n@@ -14,33 +14,22 @@\n import pytest\n from PIL import Image\n \n-from src.llamaagent.agents import (\n-    AdvancedReasoningAgent,\n-    MultiModalAdvancedAgent,\n-    ReactAgent,\n-)\n-from src.llamaagent.agents.multimodal_advanced import ModalityData, ModalityType\n+from src.llamaagent.agents import (AdvancedReasoningAgent,\n+                                   MultiModalAdvancedAgent, ReactAgent)\n+from src.llamaagent.agents.multimodal_advanced import (ModalityData,\n+                                                       ModalityType)\n from src.llamaagent.llm import LLMProvider\n from src.llamaagent.memory import BaseMemory\n-from src.llamaagent.planning import (\n-    ExecutionEngine,\n-    OptimizationObjective,\n-    PlanOptimizer,\n-    Task,\n-    TaskPlanner,\n-    TaskPriority,\n-)\n-from src.llamaagent.research import (\n-    Citation,\n-    CitationFormat,\n-    CitationManager,\n-    EvidenceAnalyzer,\n-    KnowledgeGraph,\n-)\n+from src.llamaagent.planning import (ExecutionEngine, OptimizationObjective,\n+                                     PlanOptimizer, Task, TaskPlanner,\n+                                     TaskPriority)\n+from src.llamaagent.research import (Citation, CitationFormat, CitationManager,\n+                                     EvidenceAnalyzer, KnowledgeGraph)\n from src.llamaagent.routing import AIRouter, RoutingMode\n from src.llamaagent.spawning import AgentSpawner, SpawnConfig\n from src.llamaagent.types import AgentCapability\n-from src.llamaagent.visualization import ResearchVisualizer, create_performance_plots\n+from src.llamaagent.visualization import (ResearchVisualizer,\n+                                          create_performance_plots)\n \n \n class TestAgentSpawning:\n--- /Users/nemesis/llamaagent/tests/test_ai_routing.py:before\t2025-07-05 11:10:27.188726\n+++ /Users/nemesis/llamaagent/tests/test_ai_routing.py:after\t2025-07-07 12:16:49.629027\n@@ -4,25 +4,16 @@\n \n import pytest\n \n-from llamaagent.routing import (\n-    AIRouter,\n-    PerformanceTracker,\n-    ProviderRegistry,\n-    RoutingDecision,\n-    TaskAnalyzer,\n-    TaskCharacteristics,\n-)\n+from llamaagent.routing import (AIRouter, PerformanceTracker, ProviderRegistry,\n+                                RoutingDecision, TaskAnalyzer,\n+                                TaskCharacteristics)\n from llamaagent.routing.ai_router import RoutingConfig, RoutingMode\n from llamaagent.routing.provider_registry import ProviderStatus, ProviderType\n-from llamaagent.routing.strategies import (\n-    AdaptiveRouting,\n-    ConsensusRouting,\n-    CostOptimizedRouting,\n-    HybridRouting,\n-    LanguageBasedRouting,\n-    PerformanceBasedRouting,\n-    TaskBasedRouting,\n-)\n+from llamaagent.routing.strategies import (AdaptiveRouting, ConsensusRouting,\n+                                           CostOptimizedRouting, HybridRouting,\n+                                           LanguageBasedRouting,\n+                                           PerformanceBasedRouting,\n+                                           TaskBasedRouting)\n from llamaagent.routing.task_analyzer import TaskComplexity, TaskType\n \n \n--- /Users/nemesis/llamaagent/tests/test_openai_comprehensive_integration.py:before\t2025-07-05 11:10:27.530907\n+++ /Users/nemesis/llamaagent/tests/test_openai_comprehensive_integration.py:after\t2025-07-07 12:16:49.635386\n@@ -26,28 +26,21 @@\n from fastapi.testclient import TestClient\n \n from src.llamaagent.api.openai_comprehensive_api import app\n-\n # Import modules to test\n from src.llamaagent.integration.openai_comprehensive import (\n-    BudgetExceededError,\n-    OpenAIComprehensiveConfig,\n-    OpenAIComprehensiveIntegration,\n-    OpenAIModelType,\n-    OpenAIUsageTracker,\n-    create_comprehensive_openai_integration,\n-)\n-from src.llamaagent.tools.openai_tools import (\n-    OPENAI_TOOLS,\n-    OpenAIComprehensiveTool,\n-    OpenAIEmbeddingsTool,\n-    OpenAIImageGenerationTool,\n-    OpenAIModerationTool,\n-    OpenAIReasoningTool,\n-    OpenAITextToSpeechTool,\n-    OpenAITranscriptionTool,\n-    create_all_openai_tools,\n-    create_openai_tool,\n-)\n+    BudgetExceededError, OpenAIComprehensiveConfig,\n+    OpenAIComprehensiveIntegration, OpenAIModelType, OpenAIUsageTracker,\n+    create_comprehensive_openai_integration)\n+from src.llamaagent.tools.openai_tools import (OPENAI_TOOLS,\n+                                               OpenAIComprehensiveTool,\n+                                               OpenAIEmbeddingsTool,\n+                                               OpenAIImageGenerationTool,\n+                                               OpenAIModerationTool,\n+                                               OpenAIReasoningTool,\n+                                               OpenAITextToSpeechTool,\n+                                               OpenAITranscriptionTool,\n+                                               create_all_openai_tools,\n+                                               create_openai_tool)\n \n \n class TestOpenAIUsageTracker:\n--- /Users/nemesis/llamaagent/tests/test_chat_repl_comprehensive.py:before\t2025-07-07 07:15:08.783697\n+++ /Users/nemesis/llamaagent/tests/test_chat_repl_comprehensive.py:after\t2025-07-07 12:16:49.645871\n@@ -15,13 +15,8 @@\n \n import pytest\n \n-from src.llamaagent.cli.chat_repl import (\n-    ChatEngine,\n-    ChatMessage,\n-    ChatSession,\n-    REPLInterface,\n-    SessionManager,\n-)\n+from src.llamaagent.cli.chat_repl import (ChatEngine, ChatMessage, ChatSession,\n+                                          REPLInterface, SessionManager)\n from src.llamaagent.llm.providers.mock_provider import MockProvider\n from src.llamaagent.types import LLMMessage, LLMResponse\n \n--- /Users/nemesis/llamaagent/tests/test_openai_stub.py:before\t2025-07-05 11:10:27.250025\n+++ /Users/nemesis/llamaagent/tests/test_openai_stub.py:after\t2025-07-07 12:16:49.649856\n@@ -7,16 +7,14 @@\n \n import pytest\n \n-from src.llamaagent.integration._openai_stub import (\n-    MockAsyncOpenAIClient,\n-    MockAuthenticationError,\n-    MockChatCompletion,\n-    MockEmbedding,\n-    MockModeration,\n-    MockOpenAIClient,\n-    install_openai_stub,\n-    uninstall_openai_stub,\n-)\n+from src.llamaagent.integration._openai_stub import (MockAsyncOpenAIClient,\n+                                                     MockAuthenticationError,\n+                                                     MockChatCompletion,\n+                                                     MockEmbedding,\n+                                                     MockModeration,\n+                                                     MockOpenAIClient,\n+                                                     install_openai_stub,\n+                                                     uninstall_openai_stub)\n \n \n class TestOpenAIStub:\n--- /Users/nemesis/llamaagent/tests/test_comprehensive_integration.py:before\t2025-07-06 06:01:44.893187\n+++ /Users/nemesis/llamaagent/tests/test_comprehensive_integration.py:after\t2025-07-07 12:16:49.654129\n@@ -12,9 +12,8 @@\n - Error handling and edge cases\n \"\"\"\n \n+import asyncio\n import logging\n-import asyncio\n-\n # Import core modules\n import sys\n import time\n@@ -29,6 +28,7 @@\n from llamaagent.llm.factory import LLMFactory\n from llamaagent.tools import ToolRegistry, get_all_tools\n from llamaagent.types import LLMMessage, LLMResponse\n+\n logger = logging.getLogger(__name__)\n \n \n--- /Users/nemesis/llamaagent/tests/test_gdt.py:before\t2025-07-05 11:10:27.259078\n+++ /Users/nemesis/llamaagent/tests/test_gdt.py:after\t2025-07-07 12:16:49.658289\n@@ -11,17 +11,10 @@\n \n import pytest\n \n-from src.llamaagent.data.gdt import (\n-    ConversationDataGenerator,\n-    DataType,\n-    GDTDataset,\n-    GDTGenerator,\n-    GDTItem,\n-    GDTTransformer,\n-    GDTValidator,\n-    TextDataGenerator,\n-    ValidationStatus,\n-)\n+from src.llamaagent.data.gdt import (ConversationDataGenerator, DataType,\n+                                     GDTDataset, GDTGenerator, GDTItem,\n+                                     GDTTransformer, GDTValidator,\n+                                     TextDataGenerator, ValidationStatus)\n \n \n class MockGDTGenerator:\n--- /Users/nemesis/llamaagent/tests/test_langgraph_integration_comprehensive.py:before\t2025-07-05 11:10:27.219159\n+++ /Users/nemesis/llamaagent/tests/test_langgraph_integration_comprehensive.py:after\t2025-07-07 12:16:49.661600\n@@ -115,9 +115,8 @@\n         self, mock_langgraph: Iterator[None]\n     ) -> None:\n         \"\"\"Validate custom parameter propagation in `_build_agent`.\"\"\"\n-        from src.llamaagent.integration.langgraph import (\n-            _build_agent,\n-        )  # pyright: ignore[reportPrivateUsage]\n+        from src.llamaagent.integration.langgraph import \\\n+            _build_agent  # pyright: ignore[reportPrivateUsage]\n \n         with patch(\"llamaagent.integration.langgraph.ToolRegistry\") as mock_registry:\n             with patch(\n@@ -210,10 +209,8 @@\n     ) -> None:\n         \"\"\"Test agent node function execution.\"\"\"\n         from llamaagent.agents.base import AgentResponse\n-        from src.llamaagent.integration.langgraph import (\n-            AgentState,\n-            LangGraphIntegration,\n-        )\n+        from src.llamaagent.integration.langgraph import (AgentState,\n+                                                          LangGraphIntegration)\n \n         integration = LangGraphIntegration()\n \n--- /Users/nemesis/llamaagent/tests/test_spre_evaluator_comprehensive.py:before\t2025-07-05 11:10:27.591263\n+++ /Users/nemesis/llamaagent/tests/test_spre_evaluator_comprehensive.py:after\t2025-07-07 12:16:49.667260\n@@ -9,11 +9,8 @@\n import pytest\n \n from llamaagent.benchmarks.gaia_benchmark import GAIATask\n-from llamaagent.benchmarks.spre_evaluator import (\n-    BenchmarkResult,\n-    SPREEvaluator,\n-    TaskResult,\n-)\n+from llamaagent.benchmarks.spre_evaluator import (BenchmarkResult,\n+                                                  SPREEvaluator, TaskResult)\n \n \n class TestTaskResult:\n--- /Users/nemesis/llamaagent/tests/test_comprehensive_functionality.py:before\t2025-07-07 12:08:16.825315\n+++ /Users/nemesis/llamaagent/tests/test_comprehensive_functionality.py:after\t2025-07-07 12:16:49.674985\n@@ -13,24 +13,20 @@\n \n import asyncio\n import json\n-import pytest\n import tempfile\n from pathlib import Path\n from unittest.mock import AsyncMock, MagicMock, patch\n \n-from src.llamaagent.data_generation.spre import (\n-    SPREGenerator,\n-    SPREDataset,\n-    SPREItem,\n-    DataType,\n-    ValidationStatus,\n-    SpreConfig,\n-)\n+import pytest\n+\n+from src.llamaagent import cli_main\n from src.llamaagent.agents.base import AgentConfig, AgentRole\n from src.llamaagent.agents.react import ReactAgent\n+from src.llamaagent.data_generation.spre import (DataType, SpreConfig,\n+                                                 SPREDataset, SPREGenerator,\n+                                                 SPREItem, ValidationStatus)\n from src.llamaagent.llm.providers.mock import MockProvider\n from src.llamaagent.tools import ToolRegistry\n-from src.llamaagent import cli_main\n \n \n class TestSPREGenerator:\n@@ -383,7 +379,8 @@\n     def test_openai_integration_import(self):\n         \"\"\"Test OpenAI integration import\"\"\"\n         try:\n-            from src.llamaagent.integration.openai_agents import get_openai_integration\n+            from src.llamaagent.integration.openai_agents import \\\n+                get_openai_integration\n             assert get_openai_integration is not None\n         except ImportError:\n             pytest.skip(\"OpenAI integration not available\")\n@@ -391,7 +388,8 @@\n     def test_langgraph_integration_import(self):\n         \"\"\"Test LangGraph integration import\"\"\"\n         try:\n-            from src.llamaagent.integration.langgraph import is_langgraph_available\n+            from src.llamaagent.integration.langgraph import \\\n+                is_langgraph_available\n             assert is_langgraph_available is not None\n         except ImportError:\n             pytest.skip(\"LangGraph integration not available\")\n--- /Users/nemesis/llamaagent/tests/test_advanced_features.py:before\t2025-07-06 06:01:44.894135\n+++ /Users/nemesis/llamaagent/tests/test_advanced_features.py:after\t2025-07-07 12:16:49.684421\n@@ -5,28 +5,21 @@\n and performance optimization.\n \"\"\"\n \n+import asyncio\n import logging\n-import asyncio\n from unittest.mock import AsyncMock, Mock\n \n import pytest\n \n-from llamaagent.agents import (\n-    AdvancedReasoningAgent,\n-    ModalityType,\n-    MultiModalAdvancedAgent,\n-    ReasoningStrategy,\n-)\n+from llamaagent.agents import (AdvancedReasoningAgent, ModalityType,\n+                               MultiModalAdvancedAgent, ReasoningStrategy)\n from llamaagent.agents.base import AgentConfig\n from llamaagent.cache import AdvancedCache, CacheStrategy\n-from llamaagent.core import (\n-    ErrorHandler,\n-    ErrorSeverity,\n-    RecoveryStrategy,\n-    with_error_handling,\n-)\n+from llamaagent.core import (ErrorHandler, ErrorSeverity, RecoveryStrategy,\n+                             with_error_handling)\n from llamaagent.llm import LLMResponse\n from llamaagent.optimization import BatchProcessor, PerformanceOptimizer\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -659,12 +652,8 @@\n @pytest.mark.asyncio\n async def test_monitoring_modules():\n     \"\"\"Test monitoring and observability modules.\"\"\"\n-    from llamaagent.monitoring import (\n-        AlertManager,\n-        HealthChecker,\n-        MetricsCollector,\n-        TracingService,\n-    )\n+    from llamaagent.monitoring import (AlertManager, HealthChecker,\n+                                       MetricsCollector, TracingService)\n \n     # Test MetricsCollector\n     metrics = MetricsCollector()\n@@ -728,12 +717,8 @@\n @pytest.mark.asyncio\n async def test_knowledge_modules():\n     \"\"\"Test knowledge management modules.\"\"\"\n-    from llamaagent.knowledge import (\n-        DocumentStore,\n-        KnowledgeBase,\n-        KnowledgeGraph,\n-        SemanticSearch,\n-    )\n+    from llamaagent.knowledge import (DocumentStore, KnowledgeBase,\n+                                      KnowledgeGraph, SemanticSearch)\n \n     # Test KnowledgeBase\n     kb = KnowledgeBase()\n@@ -809,10 +794,8 @@\n @pytest.mark.asyncio\n async def test_reasoning_strategy_coverage():\n     \"\"\"Test all reasoning strategies for coverage.\"\"\"\n-    from llamaagent.agents.advanced_reasoning import (\n-        AdvancedReasoningAgent,\n-        ReasoningStrategy,\n-    )\n+    from llamaagent.agents.advanced_reasoning import (AdvancedReasoningAgent,\n+                                                      ReasoningStrategy)\n     from llamaagent.agents.base import AgentConfig\n \n     # Mock LLM provider\n@@ -852,12 +835,8 @@\n @pytest.mark.asyncio\n async def test_security_modules():\n     \"\"\"Test security module functionality.\"\"\"\n-    from llamaagent.security import (\n-        AuditLogger,\n-        AuthenticationService,\n-        EncryptionService,\n-        SecurityManager,\n-    )\n+    from llamaagent.security import (AuditLogger, AuthenticationService,\n+                                     EncryptionService, SecurityManager)\n \n     # Test SecurityManager with a workaround for initialization issues\n     try:\n@@ -959,12 +938,8 @@\n @pytest.mark.asyncio\n async def test_tools_modules():\n     \"\"\"Test tools module functionality.\"\"\"\n-    from llamaagent.tools import (\n-        DynamicToolLoader,\n-        ToolRegistry,\n-        ToolValidator,\n-        create_tool_from_function,\n-    )\n+    from llamaagent.tools import (DynamicToolLoader, ToolRegistry,\n+                                  ToolValidator, create_tool_from_function)\n \n     # Test ToolRegistry\n     registry = ToolRegistry()\n--- /Users/nemesis/llamaagent/tests/test_langgraph_integration.py:before\t2025-07-05 11:10:27.100839\n+++ /Users/nemesis/llamaagent/tests/test_langgraph_integration.py:after\t2025-07-07 12:16:49.686606\n@@ -5,7 +5,8 @@\n \n import pytest\n \n-from src.llamaagent.integration.langgraph import BudgetExceededError, LangGraphAgent\n+from src.llamaagent.integration.langgraph import (BudgetExceededError,\n+                                                  LangGraphAgent)\n from src.llamaagent.llm.providers.mock_provider import MockProvider\n from src.llamaagent.types import TaskInput\n \n--- /Users/nemesis/llamaagent/tests/test_openai_stub_direct.py:before\t2025-07-05 11:10:27.273555\n+++ /Users/nemesis/llamaagent/tests/test_openai_stub_direct.py:after\t2025-07-07 12:16:49.689097\n@@ -7,10 +7,8 @@\n \n import pytest\n \n-from src.llamaagent.integration._openai_stub import (\n-    install_openai_stub,\n-    uninstall_openai_stub,\n-)\n+from src.llamaagent.integration._openai_stub import (install_openai_stub,\n+                                                     uninstall_openai_stub)\n \n \n class TestOpenAIStubDirect:\n--- /Users/nemesis/llamaagent/tests/unit/test_llm_providers.py:before\t2025-07-06 06:01:44.937718\n+++ /Users/nemesis/llamaagent/tests/unit/test_llm_providers.py:after\t2025-07-07 12:16:49.691455\n@@ -4,8 +4,10 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n+from unittest.mock import Mock, patch\n+\n import pytest\n-from unittest.mock import Mock, patch\n+\n from src.llamaagent.llm.factory import LLMFactory\n from src.llamaagent.llm.providers.mock_provider import MockProvider\n \n--- /Users/nemesis/llamaagent/tests/unit/test_agents.py:before\t2025-07-06 06:01:44.937782\n+++ /Users/nemesis/llamaagent/tests/unit/test_agents.py:after\t2025-07-07 12:16:49.692291\n@@ -4,11 +4,13 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n+from unittest.mock import Mock\n+\n import pytest\n-from unittest.mock import Mock\n+\n from src.llamaagent.agents.react import ReactAgent\n+from src.llamaagent.llm.providers.mock_provider import MockProvider\n from src.llamaagent.types import AgentConfig\n-from src.llamaagent.llm.providers.mock_provider import MockProvider\n \n \n class TestAgents:\n--- /Users/nemesis/llamaagent/tests/integration/test_workflow.py:before\t2025-07-06 06:01:44.937953\n+++ /Users/nemesis/llamaagent/tests/integration/test_workflow.py:after\t2025-07-07 12:16:49.693499\n@@ -5,10 +5,11 @@\n \"\"\"\n \n import pytest\n+\n from src.llamaagent.agents.react import ReactAgent\n+from src.llamaagent.llm.factory import LLMFactory\n+from src.llamaagent.llm.providers.mock_provider import MockProvider\n from src.llamaagent.types import AgentConfig\n-from src.llamaagent.llm.providers.mock_provider import MockProvider\n-from src.llamaagent.llm.factory import LLMFactory\n \n \n class TestWorkflows:\n--- /Users/nemesis/llamaagent/tests/integration/test_api.py:before\t2025-07-06 06:01:44.937842\n+++ /Users/nemesis/llamaagent/tests/integration/test_api.py:after\t2025-07-07 12:16:49.694476\n@@ -6,6 +6,7 @@\n \n import pytest\n from fastapi.testclient import TestClient\n+\n from src.llamaagent.api.main import app\n \n \n--- /Users/nemesis/llamaagent/tests/performance/test_benchmarks.py:before\t2025-07-06 06:01:44.938095\n+++ /Users/nemesis/llamaagent/tests/performance/test_benchmarks.py:after\t2025-07-07 12:16:49.701578\n@@ -4,12 +4,14 @@\n Author: Nik Jois <nikjois@llamasearch.ai>\n \"\"\"\n \n-import pytest\n import time\n from concurrent.futures import ThreadPoolExecutor\n+\n+import pytest\n+\n from src.llamaagent.agents.react import ReactAgent\n+from src.llamaagent.llm.providers.mock_provider import MockProvider\n from src.llamaagent.types import AgentConfig\n-from src.llamaagent.llm.providers.mock_provider import MockProvider\n \n \n class TestPerformance:\n--- /Users/nemesis/llamaagent/tests/e2e/test_complete_system.py:before\t2025-07-06 06:01:44.938022\n+++ /Users/nemesis/llamaagent/tests/e2e/test_complete_system.py:after\t2025-07-07 12:16:49.703289\n@@ -6,6 +6,7 @@\n \n import pytest\n from fastapi.testclient import TestClient\n+\n from src.llamaagent.api.main import app\n \n \n"
    },
    "flake8_check": {
      "exit_code": 1,
      "output": "src/llamaagent/__init__.py:240:9: F811 redefinition of unused 'shutil' from line 11\nsrc/llamaagent/__init__.py:285:1: E402 module level import not at top of file\nsrc/llamaagent/agents/advanced_reasoning.py:4:89: E501 line too long (91 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:5:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:182:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:185:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:191:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:207:9: E722 do not use bare 'except'\nsrc/llamaagent/agents/advanced_reasoning.py:229:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:233:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:240:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:248:89: E501 line too long (103 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:552:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:559:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:562:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:563:89: E501 line too long (123 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:564:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/advanced_reasoning.py:576:89: E501 line too long (100 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:655:89: E501 line too long (117 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:698:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:723:89: E501 line too long (104 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:754:89: E501 line too long (120 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:761:9: E722 do not use bare 'except'\nsrc/llamaagent/agents/advanced_reasoning.py:770:89: E501 line too long (121 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:790:89: E501 line too long (121 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:810:89: E501 line too long (101 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:833:89: E501 line too long (93 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:856:89: E501 line too long (116 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:880:89: E501 line too long (106 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:899:89: E501 line too long (101 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:920:89: E501 line too long (109 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:952:89: E501 line too long (128 > 88 characters)\nsrc/llamaagent/agents/advanced_reasoning.py:971:89: E501 line too long (131 > 88 characters)\nsrc/llamaagent/agents/multimodal_advanced.py:5:1: F401 'typing.Optional' imported but unused\nsrc/llamaagent/agents/multimodal_advanced.py:9:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/agents/multimodal_advanced.py:16:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/agents/multimodal_advanced.py:23:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/agents/multimodal_advanced.py:29:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/agents/multimodal_advanced.py:34:1: W293 blank line contains whitespace\nsrc/llamaagent/agents/multimodal_advanced.py:39:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/agents/multimodal_advanced.py:39:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/agents/multimodal_reasoning.py:110:6: E999 SyntaxError: invalid syntax\nsrc/llamaagent/agents/react.py:36:89: E501 line too long (181 > 88 characters)\nsrc/llamaagent/agents/react.py:60:89: E501 line too long (143 > 88 characters)\nsrc/llamaagent/agents/react.py:74:89: E501 line too long (169 > 88 characters)\nsrc/llamaagent/agents/react.py:104:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/agents/react.py:119:89: E501 line too long (116 > 88 characters)\nsrc/llamaagent/agents/react.py:124:89: E501 line too long (122 > 88 characters)\nsrc/llamaagent/agents/react.py:136:89: E501 line too long (145 > 88 characters)\nsrc/llamaagent/agents/react.py:216:41: W291 trailing whitespace\nsrc/llamaagent/agents/react.py:217:54: W291 trailing whitespace\nsrc/llamaagent/agents/react.py:218:5: E129 visually indented line with same indent as next logical line\nsrc/llamaagent/agents/react.py:287:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/agents/react.py:337:89: E501 line too long (99 > 88 characters)\nsrc/llamaagent/agents/react.py:859:89: E501 line too long (95 > 88 characters)\nsrc/llamaagent/agents/react.py:861:89: E501 line too long (136 > 88 characters)\nsrc/llamaagent/agents/react.py:863:89: E501 line too long (153 > 88 characters)\nsrc/llamaagent/agents/react.py:865:89: E501 line too long (119 > 88 characters)\nsrc/llamaagent/agents/react.py:867:89: E501 line too long (123 > 88 characters)\nsrc/llamaagent/agents/react.py:870:89: E501 line too long (149 > 88 characters)\nsrc/llamaagent/agents/react.py:1209:89: E501 line too long (111 > 88 characters)\nsrc/llamaagent/agents/react.py:1291:89: E501 line too long (89 > 88 characters)\nsrc/llamaagent/agents/react.py:1292:89: E501 line too long (91 > 88 characters)\nsrc/llamaagent/agents/reasoning_chains.py:254:123: E999 SyntaxError: closing parenthesis ']' does not match opening parenthesis '('\nsrc/llamaagent/api.py:536:10: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 535\nsrc/llamaagent/api/complete_api.py:19:1: F401 'json' imported but unused\nsrc/llamaagent/api/complete_api.py:22:1: F401 'tempfile' imported but unused\nsrc/llamaagent/api/complete_api.py:26:1: F401 'typing.Union' imported but unused\nsrc/llamaagent/api/complete_api.py:30:1: F401 'fastapi.Response' imported but unused\nsrc/llamaagent/api/complete_api.py:30:1: F401 'fastapi.status' imported but unused\nsrc/llamaagent/api/complete_api.py:47:1: F401 'fastapi.responses.StreamingResponse' imported but unused\nsrc/llamaagent/api/complete_api.py:54:1: F401 '..data_generation.spre.ValidationStatus' imported but unused\nsrc/llamaagent/api/complete_api.py:109:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:120:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:122:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/api/complete_api.py:124:89: E501 line too long (101 > 88 characters)\nsrc/llamaagent/api/complete_api.py:127:89: E501 line too long (93 > 88 characters)\nsrc/llamaagent/api/complete_api.py:128:89: E501 line too long (96 > 88 characters)\nsrc/llamaagent/api/complete_api.py:130:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:140:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:152:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:154:89: E501 line too long (91 > 88 characters)\nsrc/llamaagent/api/complete_api.py:156:89: E501 line too long (97 > 88 characters)\nsrc/llamaagent/api/complete_api.py:161:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:164:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/api/complete_api.py:169:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:171:89: E501 line too long (96 > 88 characters)\nsrc/llamaagent/api/complete_api.py:185:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:188:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:194:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:201:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:204:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:214:89: E501 line too long (94 > 88 characters)\nsrc/llamaagent/api/complete_api.py:218:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:233:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:240:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:290:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:296:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:309:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:312:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:338:89: E501 line too long (111 > 88 characters)\nsrc/llamaagent/api/complete_api.py:341:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:343:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:357:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:360:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:367:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:373:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:386:89: E501 line too long (105 > 88 characters)\nsrc/llamaagent/api/complete_api.py:398:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:400:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:416:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:421:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:427:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:436:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:439:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:445:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:452:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:457:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:470:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:485:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:488:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:491:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:496:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:501:89: E501 line too long (89 > 88 characters)\nsrc/llamaagent/api/complete_api.py:504:89: E501 line too long (91 > 88 characters)\nsrc/llamaagent/api/complete_api.py:509:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:538:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:540:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:550:89: E501 line too long (116 > 88 characters)\nsrc/llamaagent/api/complete_api.py:551:89: E501 line too long (103 > 88 characters)\nsrc/llamaagent/api/complete_api.py:560:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:563:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:565:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:579:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:584:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:588:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:592:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:595:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:602:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:604:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:614:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:618:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:631:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:648:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:653:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:656:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:661:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:665:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:671:89: E501 line too long (93 > 88 characters)\nsrc/llamaagent/api/complete_api.py:680:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:695:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:706:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:709:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:724:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:726:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:745:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:767:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:776:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:781:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:792:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:795:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:800:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:807:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:816:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:823:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:828:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:841:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:843:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:845:89: E501 line too long (102 > 88 characters)\nsrc/llamaagent/api/complete_api.py:846:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:848:1: W293 blank line contains whitespace\nsrc/llamaagent/api/complete_api.py:859:6: W291 trailing whitespace\nsrc/llamaagent/api/complete_api.py:859:7: W292 no newline at end of file\nsrc/llamaagent/api/gateway.py:212:13: E999 IndentationError: unexpected indent\nsrc/llamaagent/api/main.py:1180:14: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 1179\nsrc/llamaagent/api/openai_comprehensive_api.py:202:10: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 199\nsrc/llamaagent/api/premium_endpoints.py:76:2: E999 SyntaxError: invalid syntax\nsrc/llamaagent/api/production_app.py:752:62: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{' on line 750\nsrc/llamaagent/api/shell_endpoints.py:478:50: E999 SyntaxError: '(' was never closed\nsrc/llamaagent/api/simon_ecosystem_api.py:288:60: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '('\nsrc/llamaagent/api/simple_app.py:35:1: F401 '..types.LLMMessage' imported but unused\nsrc/llamaagent/api/simple_app.py:42:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/api/simple_app.py:50:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/api/simple_app.py:139:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:144:89: E501 line too long (91 > 88 characters)\nsrc/llamaagent/api/simple_app.py:145:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:163:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:165:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:167:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:170:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:178:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:238:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:246:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:279:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:287:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:305:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:311:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:314:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:321:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:334:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:354:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:361:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:368:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:372:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:383:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:397:89: E501 line too long (111 > 88 characters)\nsrc/llamaagent/api/simple_app.py:401:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:411:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:415:89: E501 line too long (107 > 88 characters)\nsrc/llamaagent/api/simple_app.py:426:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:434:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:437:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:447:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:456:1: W293 blank line contains whitespace\nsrc/llamaagent/api/simple_app.py:486:6: W291 trailing whitespace\nsrc/llamaagent/api/simple_app.py:486:7: W292 no newline at end of file\nsrc/llamaagent/benchmarks/baseline_agents.py:139:89: E501 line too long (102 > 88 characters)\nsrc/llamaagent/benchmarks/baseline_agents.py:195:89: E501 line too long (103 > 88 characters)\nsrc/llamaagent/benchmarks/baseline_agents.py:196:89: E501 line too long (94 > 88 characters)\nsrc/llamaagent/benchmarks/baseline_agents.py:197:89: E501 line too long (103 > 88 characters)\nsrc/llamaagent/benchmarks/baseline_agents.py:198:89: E501 line too long (101 > 88 characters)\nsrc/llamaagent/benchmarks/frontier_evaluation.py:108:13: E999 IndentationError: unexpected indent\nsrc/llamaagent/benchmarks/gaia_benchmark.py:260:13: E999 IndentationError: unexpected indent\nsrc/llamaagent/benchmarks/spre_evaluator.py:109:18: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 101\nsrc/llamaagent/cache/__init__.py:5:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/cache/__init__.py:7:1: W293 blank line contains whitespace\nsrc/llamaagent/cache/__init__.py:10:1: W293 blank line contains whitespace\nsrc/llamaagent/cache/__init__.py:14:1: W293 blank line contains whitespace\nsrc/llamaagent/cache/__init__.py:19:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/cache/advanced_cache.py:300:10: E999 IndentationError: expected an indented block after 'except' statement on line 299\nsrc/llamaagent/cache/cache_manager.py:142:41: E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\nsrc/llamaagent/cache/llm_cache.py:206:6: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/cache/memory_pool.py:5:1: F401 'typing.Dict' imported but unused\nsrc/llamaagent/cache/memory_pool.py:7:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/cache/memory_pool.py:9:1: W293 blank line contains whitespace\nsrc/llamaagent/cache/memory_pool.py:13:1: W293 blank line contains whitespace\nsrc/llamaagent/cache/memory_pool.py:17:1: W293 blank line contains whitespace\nsrc/llamaagent/cache/memory_pool.py:25:1: W293 blank line contains whitespace\nsrc/llamaagent/cache/query_optimizer.py:285:10: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 284\nsrc/llamaagent/cache/result_cache.py:125:30: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/cli/__init__.py:3:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/cli/__init__.py:7:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/cli/chat_repl.py:142:35: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/cli/code_generator.py:97:10: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 94\nsrc/llamaagent/cli/config_manager.py:197:10: E999 SyntaxError: invalid syntax\nsrc/llamaagent/cli/diagnostics_cli.py:78:6: E999 SyntaxError: invalid syntax\nsrc/llamaagent/cli/enhanced_cli.py:160:6: E999 SyntaxError: invalid syntax\nsrc/llamaagent/cli/enhanced_shell_cli.py:72:58: E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\nsrc/llamaagent/cli/function_manager.py:184:45: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/cli/interactive.py:94:12: E999 SyntaxError: invalid syntax\nsrc/llamaagent/cli/llm_cmd.py:194:10: E999 IndentationError: expected an indented block after 'except' statement on line 193\nsrc/llamaagent/cli/main.py:405:50: E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\nsrc/llamaagent/cli/master_cli.py:60:89: E501 line too long (103 > 88 characters)\nsrc/llamaagent/cli/master_cli.py:173:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/cli/master_cli.py:304:14: F841 local variable 'live' is assigned to but never used\nsrc/llamaagent/cli/master_cli.py:430:74: F841 local variable 'live' is assigned to but never used\nsrc/llamaagent/cli/master_cli.py:468:1: W293 blank line contains whitespace\nsrc/llamaagent/cli/master_cli.py:476:1: W293 blank line contains whitespace\nsrc/llamaagent/cli/master_cli.py:481:1: W293 blank line contains whitespace\nsrc/llamaagent/cli/master_cli.py:540:89: E501 line too long (109 > 88 characters)\nsrc/llamaagent/cli/master_cli.py:581:1: W293 blank line contains whitespace\nsrc/llamaagent/cli/master_cli.py:669:1: W293 blank line contains whitespace\nsrc/llamaagent/cli/master_cli.py:676:1: W293 blank line contains whitespace\nsrc/llamaagent/cli/openai_cli.py:101:2: E999 SyntaxError: invalid syntax\nsrc/llamaagent/cli/role_manager.py:229:31: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/cli/shell_commands.py:392:30: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/config/__init__.py:5:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/config/__init__.py:7:1: W293 blank line contains whitespace\nsrc/llamaagent/config/__init__.py:10:1: W293 blank line contains whitespace\nsrc/llamaagent/config/__init__.py:14:1: W293 blank line contains whitespace\nsrc/llamaagent/config/__init__.py:19:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/config/settings.py:2:89: E501 line too long (89 > 88 characters)\nsrc/llamaagent/config/settings.py:37:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:40:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:101:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:171:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:183:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/config/settings.py:226:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:229:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:232:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:235:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:241:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:245:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:251:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:254:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:258:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:262:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:266:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:270:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:287:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:291:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:298:35: F821 undefined name 'json'\nsrc/llamaagent/config/settings.py:299:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:311:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:314:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:317:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:320:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:323:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:326:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:329:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:332:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:354:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:357:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:364:17: F821 undefined name 'json'\nsrc/llamaagent/config/settings.py:369:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:376:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:379:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:382:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:386:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:390:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:394:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:418:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:431:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:439:1: W293 blank line contains whitespace\nsrc/llamaagent/config/settings.py:504:34: W292 no newline at end of file\nsrc/llamaagent/core/agent.py:175:54: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/core/error_handling.py:323:62: E999 SyntaxError: invalid syntax\nsrc/llamaagent/core/message_bus.py:59:64: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/core/orchestrator.py:186:53: E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\nsrc/llamaagent/core/service_mesh.py:147:61: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/data/__init__.py:4:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/data/__init__.py:15:24: W291 trailing whitespace\nsrc/llamaagent/data/__init__.py:23:2: W291 trailing whitespace\nsrc/llamaagent/data/__init__.py:23:3: W292 no newline at end of file\nsrc/llamaagent/data/gdt.py:72:121: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/data_generation/agentic_pipelines.py:392:10: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 389\nsrc/llamaagent/data_generation/base.py:135:2: W292 no newline at end of file\nsrc/llamaagent/data_generation/gdt.py:39:69: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/data_generation/spre.py:586:35: E999 SyntaxError: invalid syntax\nsrc/llamaagent/diagnostics/code_analyzer.py:64:14: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 62\nsrc/llamaagent/diagnostics/dependency_checker.py:115:25: E999 IndentationError: unexpected indent\nsrc/llamaagent/diagnostics/master_diagnostics.py:858:10: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 851\nsrc/llamaagent/diagnostics/system_validator.py:57:17: E999 IndentationError: unexpected indent\nsrc/llamaagent/evaluation/benchmark_engine.py:196:6: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/evaluation/golden_dataset.py:77:6: E999 SyntaxError: invalid syntax\nsrc/llamaagent/evaluation/model_comparison.py:175:48: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/evolution/adaptive_learning.py:123:46: E999 SyntaxError: invalid syntax\nsrc/llamaagent/evolution/knowledge_base.py:62:10: E999 SyntaxError: closing parenthesis ']' does not match opening parenthesis '(' on line 58\nsrc/llamaagent/evolution/orchestrator.py:18:89: E501 line too long (142 > 88 characters)\nsrc/llamaagent/evolution/reflection.py:146:23: E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\nsrc/llamaagent/experiment_runner.py:56:36: E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\nsrc/llamaagent/integration/_openai_stub.py:293:18: E999 SyntaxError: closing parenthesis ']' does not match opening parenthesis '(' on line 292\nsrc/llamaagent/integration/langgraph.py:88:10: F821 undefined name 'AgentResponse'\nsrc/llamaagent/integration/langgraph.py:328:89: E501 line too long (111 > 88 characters)\nsrc/llamaagent/integration/openai_agents.py:259:22: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/integration/openai_agents_complete.py:166:57: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/integration/openai_comprehensive.py:118:54: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/integration/simon_tools.py:74:14: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 73\nsrc/llamaagent/knowledge/__init__.py:5:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/knowledge/__init__.py:7:1: W293 blank line contains whitespace\nsrc/llamaagent/knowledge/__init__.py:10:1: W293 blank line contains whitespace\nsrc/llamaagent/knowledge/__init__.py:14:1: W293 blank line contains whitespace\nsrc/llamaagent/knowledge/__init__.py:19:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/knowledge/knowledge_generator.py:760:14: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 759\nsrc/llamaagent/llm/__init__.py:16:1: E402 module level import not at top of file\nsrc/llamaagent/llm/__init__.py:19:1: E402 module level import not at top of file\nsrc/llamaagent/llm/__init__.py:31:1: E402 module level import not at top of file\nsrc/llamaagent/llm/__init__.py:32:1: E402 module level import not at top of file\nsrc/llamaagent/llm/__init__.py:35:1: E402 module level import not at top of file\nsrc/llamaagent/llm/__init__.py:43:1: E402 module level import not at top of file\nsrc/llamaagent/llm/factory.py:17:1: E402 module level import not at top of file\nsrc/llamaagent/llm/factory.py:64:1: E402 module level import not at top of file\nsrc/llamaagent/llm/factory.py:168:89: E501 line too long (115 > 88 characters)\nsrc/llamaagent/llm/factory.py:180:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/factory.py:193:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/factory.py:196:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/factory.py:200:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/factory.py:203:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/factory.py:205:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/llm/factory.py:206:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/llm/factory.py:207:89: E501 line too long (95 > 88 characters)\nsrc/llamaagent/llm/factory.py:209:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/llm/factory.py:211:89: E501 line too long (94 > 88 characters)\nsrc/llamaagent/llm/mlx_provider.py:3:1: F401 'typing.Any' imported but unused\nsrc/llamaagent/llm/mlx_provider.py:3:1: F401 'typing.Dict' imported but unused\nsrc/llamaagent/llm/mlx_provider.py:3:1: F401 'typing.Optional' imported but unused\nsrc/llamaagent/llm/mlx_provider.py:21:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/mlx_provider.py:35:13: W292 no newline at end of file\nsrc/llamaagent/llm/providers/__init__.py:15:1: E402 module level import not at top of file\nsrc/llamaagent/llm/providers/__init__.py:16:1: E402 module level import not at top of file\nsrc/llamaagent/llm/providers/__init__.py:19:1: E402 module level import not at top of file\nsrc/llamaagent/llm/providers/__init__.py:109:89: E501 line too long (119 > 88 characters)\nsrc/llamaagent/llm/providers/anthropic.py:93:45: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/llm/providers/base.py:4:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/llm/providers/base.py:150:89: E501 line too long (103 > 88 characters)\nsrc/llamaagent/llm/providers/cohere.py:364:29: E999 SyntaxError: '(' was never closed\nsrc/llamaagent/llm/providers/cohere_provider.py:7:1: F401 'typing.Union' imported but unused\nsrc/llamaagent/llm/providers/cohere_provider.py:18:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:21:44: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:30:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:34:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:41:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:49:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:51:14: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:52:21: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:59:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:63:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:70:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:73:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:76:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:79:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:82:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:84:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:89:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:98:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:106:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:108:14: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:109:36: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:114:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:124:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:128:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:131:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:132:89: E501 line too long (106 > 88 characters)\nsrc/llamaagent/llm/providers/cohere_provider.py:134:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:141:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:144:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:149:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:154:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:157:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:171:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:175:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cohere_provider.py:179:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:4:76: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:5:80: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:6:78: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:7:76: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:8:75: W291 trailing whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:26:5: F401 '.ollama_provider.OllamaProvider' imported but unused\nsrc/llamaagent/llm/providers/cuda_provider.py:68:17: F811 redefinition of unused 'OllamaProvider' from line 26\nsrc/llamaagent/llm/providers/cuda_provider.py:94:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:97:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:182:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:193:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:205:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:207:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:225:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:231:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:279:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:327:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/cuda_provider.py:348:19: W292 no newline at end of file\nsrc/llamaagent/llm/providers/litellm_provider.py:298:32: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/llm/providers/mlx_provider.py:16:1: F401 '..exceptions.LLMError' imported but unused\nsrc/llamaagent/llm/providers/mlx_provider.py:171:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mlx_provider.py:204:89: E501 line too long (95 > 88 characters)\nsrc/llamaagent/llm/providers/mlx_provider.py:239:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mlx_provider.py:251:89: E501 line too long (97 > 88 characters)\nsrc/llamaagent/llm/providers/mlx_provider.py:274:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mlx_provider.py:287:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mlx_provider.py:307:19: W292 no newline at end of file\nsrc/llamaagent/llm/providers/mock_provider.py:34:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:38:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:47:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:54:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:56:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:67:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:74:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:79:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:83:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:92:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:94:89: E501 line too long (91 > 88 characters)\nsrc/llamaagent/llm/providers/mock_provider.py:99:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:110:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:112:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:114:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:120:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:123:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:125:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:129:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:131:89: E501 line too long (115 > 88 characters)\nsrc/llamaagent/llm/providers/mock_provider.py:133:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:135:89: E501 line too long (89 > 88 characters)\nsrc/llamaagent/llm/providers/mock_provider.py:137:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:139:89: E501 line too long (91 > 88 characters)\nsrc/llamaagent/llm/providers/mock_provider.py:144:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/mock_provider.py:146:89: E501 line too long (96 > 88 characters)\nsrc/llamaagent/llm/providers/mock_provider.py:163:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/ollama_provider.py:177:89: E501 line too long (89 > 88 characters)\nsrc/llamaagent/llm/providers/ollama_provider.py:243:89: E501 line too long (97 > 88 characters)\nsrc/llamaagent/llm/providers/ollama_provider.py:269:89: E501 line too long (89 > 88 characters)\nsrc/llamaagent/llm/providers/ollama_provider.py:286:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/ollama_provider.py:417:19: W292 no newline at end of file\nsrc/llamaagent/llm/providers/openai.py:148:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/openai.py:284:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/openai.py:348:25: W292 no newline at end of file\nsrc/llamaagent/llm/providers/openai_provider.py:116:9: F841 local variable 'start_time' is assigned to but never used\nsrc/llamaagent/llm/providers/together.py:193:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/together.py:226:10: W292 no newline at end of file\nsrc/llamaagent/llm/providers/together_provider.py:9:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/llm/providers/together_provider.py:11:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/together_provider.py:14:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/providers/together_provider.py:22:1: W293 blank line contains whitespace\nsrc/llamaagent/llm/simon_ecosystem.py:103:10: E999 SyntaxError: invalid syntax\nsrc/llamaagent/memory/__init__.py:5:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/memory/__init__.py:7:1: W293 blank line contains whitespace\nsrc/llamaagent/memory/__init__.py:10:1: W293 blank line contains whitespace\nsrc/llamaagent/memory/__init__.py:14:1: W293 blank line contains whitespace\nsrc/llamaagent/memory/__init__.py:19:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/ml/__init__.py:5:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/ml/__init__.py:7:1: W293 blank line contains whitespace\nsrc/llamaagent/ml/__init__.py:12:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/ml/inference_engine.py:136:72: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/monitoring/__init__.py:5:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/monitoring/__init__.py:7:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/__init__.py:10:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/__init__.py:15:1: E305 expected 2 blank lines after class or function definition, found 1\nsrc/llamaagent/monitoring/advanced_monitoring.py:40:10: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{' on line 34\nsrc/llamaagent/monitoring/alerting.py:389:62: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/monitoring/logging.py:20:1: F401 'typing.List' imported but unused\nsrc/llamaagent/monitoring/logging.py:32:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:54:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:94:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:104:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:135:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:143:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:148:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:177:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:183:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:192:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:196:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:203:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:211:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:218:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:237:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:242:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:253:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:282:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:287:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:325:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:328:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/logging.py:366:2: W292 no newline at end of file\nsrc/llamaagent/monitoring/metrics_collector.py:409:34: E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\nsrc/llamaagent/monitoring/middleware.py:21:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/monitoring/middleware.py:23:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:29:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:30:89: E501 line too long (97 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:34:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:37:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:42:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:45:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:52:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:58:89: E501 line too long (110 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:59:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:62:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:64:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:86:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:90:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:94:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:97:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:101:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:104:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:115:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:118:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:126:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:130:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:132:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:143:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:148:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:156:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:161:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:165:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:175:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:183:21: E722 do not use bare 'except'\nsrc/llamaagent/monitoring/middleware.py:189:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:191:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:204:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:207:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:215:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:218:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:226:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:231:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:236:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:244:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:257:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:260:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:264:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:271:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:276:89: E501 line too long (106 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:284:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:286:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:291:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:293:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:298:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:301:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:310:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:312:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:316:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:320:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:322:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:328:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:330:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:331:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:333:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:337:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:342:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:351:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:356:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:360:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:363:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:366:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:375:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:377:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:387:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:388:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:389:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:398:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:409:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:414:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:418:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:427:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:431:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:433:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:435:89: E501 line too long (93 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:437:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:442:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:446:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:453:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:458:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:462:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:464:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:468:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:470:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:474:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:481:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:483:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:486:89: E501 line too long (100 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:487:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:489:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:492:89: E501 line too long (93 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:493:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:502:89: E501 line too long (130 > 88 characters)\nsrc/llamaagent/monitoring/middleware.py:504:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:514:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:516:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:520:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:524:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:531:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:538:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:541:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:544:1: W293 blank line contains whitespace\nsrc/llamaagent/monitoring/middleware.py:555:6: W291 trailing whitespace\nsrc/llamaagent/monitoring/middleware.py:555:7: W292 no newline at end of file\nsrc/llamaagent/monitoring/profiling.py:94:30: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/monitoring/tracing.py:59:54: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/optimization/performance.py:324:10: E999 SyntaxError: closing parenthesis ']' does not match opening parenthesis '(' on line 322\nsrc/llamaagent/optimization/prompt_optimizer.py:296:47: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/orchestration/adaptive_orchestra.py:128:2: E999 SyntaxError: invalid syntax\nsrc/llamaagent/orchestrator.py:271:66: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/planning/execution_engine.py:101:9: E999 IndentationError: expected an indented block after 'except' statement on line 100\nsrc/llamaagent/planning/monitoring.py:375:10: E999 SyntaxError: closing parenthesis '}' does not match opening parenthesis '(' on line 371\nsrc/llamaagent/planning/optimization.py:182:14: E999 SyntaxError: invalid syntax\nsrc/llamaagent/planning/strategies.py:68:33: E999 SyntaxError: invalid syntax\nsrc/llamaagent/planning/task_planner.py:67:64: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/prompting/chain_prompting.py:593:89: E501 line too long (94 > 88 characters)\nsrc/llamaagent/prompting/chain_prompting.py:624:30: F821 undefined name 're'\nsrc/llamaagent/prompting/compound_prompting.py:389:41: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/prompting/dspy_optimizer.py:100:36: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/prompting/optimization.py:238:13: E999 IndentationError: unexpected indent\nsrc/llamaagent/prompting/prompt_templates.py:46:20: E999 SyntaxError: invalid syntax\nsrc/llamaagent/reasoning/chain_engine.py:157:38: E999 SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\nsrc/llamaagent/reasoning/context_sharing.py:63:64: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/reasoning/memory_manager.py:57:64: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/report_generator.py:23:60: E999 SyntaxError: invalid syntax\nsrc/llamaagent/research/citations.py:5:1: F401 'typing.Any' imported but unused\nsrc/llamaagent/research/citations.py:5:1: F401 'typing.Dict' imported but unused\nsrc/llamaagent/research/citations.py:8:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/citations.py:15:1: W293 blank line contains whitespace\nsrc/llamaagent/research/citations.py:16:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/citations.py:18:1: W293 blank line contains whitespace\nsrc/llamaagent/research/citations.py:21:1: W293 blank line contains whitespace\nsrc/llamaagent/research/citations.py:25:1: W293 blank line contains whitespace\nsrc/llamaagent/research/evidence.py:5:1: F401 'typing.Optional' imported but unused\nsrc/llamaagent/research/evidence.py:8:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/evidence.py:16:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/evidence.py:18:1: W293 blank line contains whitespace\nsrc/llamaagent/research/evidence.py:21:1: W293 blank line contains whitespace\nsrc/llamaagent/research/evidence.py:25:1: W293 blank line contains whitespace\nsrc/llamaagent/research/knowledge_graph.py:5:1: F401 'typing.List' imported but unused\nsrc/llamaagent/research/knowledge_graph.py:8:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/knowledge_graph.py:15:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/knowledge_graph.py:17:1: W293 blank line contains whitespace\nsrc/llamaagent/research/knowledge_graph.py:21:1: W293 blank line contains whitespace\nsrc/llamaagent/research/knowledge_graph.py:25:1: W293 blank line contains whitespace\nsrc/llamaagent/research/literature_review.py:5:1: F401 'typing.Any' imported but unused\nsrc/llamaagent/research/literature_review.py:5:1: F401 'typing.Dict' imported but unused\nsrc/llamaagent/research/literature_review.py:8:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/literature_review.py:16:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/literature_review.py:18:1: W293 blank line contains whitespace\nsrc/llamaagent/research/literature_review.py:21:1: W293 blank line contains whitespace\nsrc/llamaagent/research/literature_review.py:25:1: W293 blank line contains whitespace\nsrc/llamaagent/research/report_generator.py:5:1: F401 'typing.Optional' imported but unused\nsrc/llamaagent/research/report_generator.py:8:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/report_generator.py:16:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/report_generator.py:18:1: W293 blank line contains whitespace\nsrc/llamaagent/research/report_generator.py:21:1: W293 blank line contains whitespace\nsrc/llamaagent/research/report_generator.py:27:1: W293 blank line contains whitespace\nsrc/llamaagent/research/scientific_reasoning.py:5:1: F401 'typing.Any' imported but unused\nsrc/llamaagent/research/scientific_reasoning.py:5:1: F401 'typing.Dict' imported but unused\nsrc/llamaagent/research/scientific_reasoning.py:5:1: F401 'typing.Optional' imported but unused\nsrc/llamaagent/research/scientific_reasoning.py:8:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/scientific_reasoning.py:15:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/research/scientific_reasoning.py:17:1: W293 blank line contains whitespace\nsrc/llamaagent/research/scientific_reasoning.py:20:1: W293 blank line contains whitespace\nsrc/llamaagent/research/scientific_reasoning.py:24:1: W293 blank line contains whitespace\nsrc/llamaagent/routing/ai_router.py:195:31: E999 SyntaxError: invalid syntax. Perhaps you forgot a comma?\nsrc/llamaagent/routing/metrics.py:67:6: E999 SyntaxError: invalid syntax\nsrc/llamaagent/routing/provider_registry.py:391:6: E999 SyntaxError: invalid syntax\nsrc/llamaagent/routing/strategies.py:451:21: E999 IndentationError: unexpected indent\nsrc/llamaagent/routing/task_analyzer.py:308:26: E999 SyntaxError: invalid syntax\nsrc/llamaagent/security/audit.py:25:13: E999 IndentationError: unexpected indent\nsrc/llamaagent/security/manager.py:397:42: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/security/rate_limiter.py:100:6: E999 SyntaxError: invalid syntax\nsrc/llamaagent/security/validator.py:81:6: E999 SyntaxError: invalid syntax\nsrc/llamaagent/spawning/agent_pool.py:187:35: E999 SyntaxError: invalid syntax\nsrc/llamaagent/spawning/agent_spawner.py:207:10: E999 SyntaxError: invalid syntax\nsrc/llamaagent/spawning/communication.py:61:64: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/statistical_analysis.py:125:13: E999 IndentationError: unexpected indent\nsrc/llamaagent/storage/database.py:232:89: E501 line too long (100 > 88 characters)\nsrc/llamaagent/storage/database.py:233:89: E501 line too long (104 > 88 characters)\nsrc/llamaagent/storage/database.py:234:89: E501 line too long (104 > 88 characters)\nsrc/llamaagent/storage/database.py:235:89: E501 line too long (102 > 88 characters)\nsrc/llamaagent/storage/database.py:237:89: E501 line too long (90 > 88 characters)\nsrc/llamaagent/storage/database.py:243:89: E501 line too long (161 > 88 characters)\nsrc/llamaagent/storage/database.py:259:22: F821 undefined name 'time'\nsrc/llamaagent/storage/database.py:264:26: F821 undefined name 'time'\nsrc/llamaagent/storage/database.py:352:29: W291 trailing whitespace\nsrc/llamaagent/storage/database.py:353:31: W291 trailing whitespace\nsrc/llamaagent/storage/database.py:354:33: W291 trailing whitespace\nsrc/llamaagent/storage/database.py:400:22: W291 trailing whitespace\nsrc/llamaagent/storage/database.py:472:21: W291 trailing whitespace\nsrc/llamaagent/storage/database.py:521:89: E501 line too long (92 > 88 characters)\nsrc/llamaagent/storage/database.py:577:15: W291 trailing whitespace\nsrc/llamaagent/tools/__init__.py:22:1: E402 module level import not at top of file\nsrc/llamaagent/tools/__init__.py:23:1: E402 module level import not at top of file\nsrc/llamaagent/tools/openai_tools.py:50:89: E501 line too long (160 > 88 characters)\nsrc/llamaagent/tools/openai_tools.py:74:89: E501 line too long (154 > 88 characters)\nsrc/llamaagent/tools/openai_tools.py:114:89: E501 line too long (96 > 88 characters)\nsrc/llamaagent/tools/openai_tools.py:177:89: E501 line too long (108 > 88 characters)\nsrc/llamaagent/tools/openai_tools.py:259:89: E501 line too long (112 > 88 characters)\nsrc/llamaagent/tools/openai_tools.py:326:89: E501 line too long (129 > 88 characters)\nsrc/llamaagent/tools/openai_tools.py:385:89: E501 line too long (98 > 88 characters)\nsrc/llamaagent/tools/openai_tools.py:446:89: E501 line too long (154 > 88 characters)\nsrc/llamaagent/tools/plugin_framework.py:5:1: F401 'typing.Dict' imported but unused\nsrc/llamaagent/tools/plugin_framework.py:7:1: E302 expected 2 blank lines, found 1\nsrc/llamaagent/tools/plugin_framework.py:9:1: W293 blank line contains whitespace\nsrc/llamaagent/tools/plugin_framework.py:12:1: W293 blank line contains whitespace\nsrc/llamaagent/tools/plugin_framework.py:16:1: W293 blank line contains whitespace\nsrc/llamaagent/tools/plugin_framework.py:20:1: W293 blank line contains whitespace\nsrc/llamaagent/tools/python_repl.py:49:89: E501 line too long (104 > 88 characters)\nsrc/llamaagent/tools/python_repl.py:56:89: E501 line too long (101 > 88 characters)\nsrc/llamaagent/tools/registry.py:272:17: F402 import 'field' from line 15 shadowed by loop variable\nsrc/llamaagent/tools/tool_registry.py:82:74: E999 SyntaxError: unmatched ')'\nsrc/llamaagent/visualization.py:59:49: E999 SyntaxError: Generator expression must be parenthesized\ntests/e2e/test_complete_system.py:7:1: F401 'pytest' imported but unused\ntests/e2e/test_complete_system.py:14:1: W293 blank line contains whitespace\ntests/e2e/test_complete_system.py:18:1: W293 blank line contains whitespace\ntests/e2e/test_complete_system.py:30:1: W293 blank line contains whitespace\ntests/e2e/test_complete_system.py:37:1: W293 blank line contains whitespace\ntests/e2e/test_complete_system.py:46:1: W293 blank line contains whitespace\ntests/e2e/test_complete_system.py:51:1: W293 blank line contains whitespace\ntests/e2e/test_complete_system.py:65:1: W293 blank line contains whitespace\ntests/e2e/test_complete_system.py:74:1: W293 blank line contains whitespace\ntests/integration/test_api.py:7:1: F401 'pytest' imported but unused\ntests/integration/test_api.py:14:1: W293 blank line contains whitespace\ntests/integration/test_api.py:18:1: W293 blank line contains whitespace\ntests/integration/test_api.py:24:1: W293 blank line contains whitespace\ntests/integration/test_api.py:35:1: W293 blank line contains whitespace\ntests/integration/test_api.py:46:1: W293 blank line contains whitespace\ntests/integration/test_api.py:55:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:7:1: F401 'pytest' imported but unused\ntests/integration/test_workflow.py:11:1: F401 'src.llamaagent.llm.factory.LLMFactory' imported but unused\ntests/integration/test_workflow.py:16:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:26:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:33:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:40:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:42:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:51:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:55:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:60:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:69:1: W293 blank line contains whitespace\ntests/integration/test_workflow.py:72:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:7:1: F401 'pytest' imported but unused\ntests/performance/test_benchmarks.py:17:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:26:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:31:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:34:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:44:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:46:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:53:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:57:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:60:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:69:1: W293 blank line contains whitespace\ntests/performance/test_benchmarks.py:74:1: W293 blank line contains whitespace\ntests/test_advanced_features.py:332:9: E999 IndentationError: expected an indented block after 'except' statement on line 331\ntests/test_ai_routing.py:87:71: W291 trailing whitespace\ntests/test_baseline_agents_comprehensive.py:29:89: E501 line too long (103 > 88 characters)\ntests/test_baseline_agents_comprehensive.py:30:89: E501 line too long (94 > 88 characters)\ntests/test_baseline_agents_comprehensive.py:31:89: E501 line too long (103 > 88 characters)\ntests/test_baseline_agents_comprehensive.py:32:89: E501 line too long (101 > 88 characters)\ntests/test_basic.py:164:89: E501 line too long (93 > 88 characters)\ntests/test_basic_repl.py:8:1: F401 'asyncio' imported but unused\ntests/test_basic_repl.py:11:1: F401 'pathlib.Path' imported but unused\ntests/test_basic_repl.py:19:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:26:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:36:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:41:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:51:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:62:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:65:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:70:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:74:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:87:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:91:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:94:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:103:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:106:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:115:21: W291 trailing whitespace\ntests/test_basic_repl.py:120:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:124:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:126:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:130:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:135:1: W293 blank line contains whitespace\ntests/test_basic_repl.py:142:34: W291 trailing whitespace\ntests/test_basic_repl.py:142:35: W292 no newline at end of file\ntests/test_chat_repl_comprehensive.py:9:1: F401 'json' imported but unused\ntests/test_chat_repl_comprehensive.py:10:1: F401 'os' imported but unused\ntests/test_chat_repl_comprehensive.py:26:1: F401 'src.llamaagent.types.LLMMessage' imported but unused\ntests/test_chat_repl_comprehensive.py:40:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:52:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:68:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:78:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:112:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:122:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:137:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:140:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:157:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:166:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:169:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:172:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:174:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:179:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:192:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:195:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:199:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:244:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:246:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:260:89: E501 line too long (90 > 88 characters)\ntests/test_chat_repl_comprehensive.py:262:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:271:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:272:9: F841 local variable 'response' is assigned to but never used\ntests/test_chat_repl_comprehensive.py:273:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:277:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:289:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:291:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:295:89: E501 line too long (94 > 88 characters)\ntests/test_chat_repl_comprehensive.py:302:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:304:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:309:89: E501 line too long (90 > 88 characters)\ntests/test_chat_repl_comprehensive.py:314:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:316:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:327:89: E501 line too long (94 > 88 characters)\ntests/test_chat_repl_comprehensive.py:329:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:331:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:356:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:361:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:370:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:382:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:393:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:395:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:405:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:407:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:416:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:418:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:429:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:430:89: E501 line too long (91 > 88 characters)\ntests/test_chat_repl_comprehensive.py:431:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:439:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:441:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:453:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:457:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:459:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:471:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:473:89: E501 line too long (101 > 88 characters)\ntests/test_chat_repl_comprehensive.py:474:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:476:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:498:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:500:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:512:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:521:89: E501 line too long (96 > 88 characters)\ntests/test_chat_repl_comprehensive.py:522:89: E501 line too long (91 > 88 characters)\ntests/test_chat_repl_comprehensive.py:524:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:528:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:530:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:542:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:544:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:551:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:553:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:567:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:573:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:577:89: E501 line too long (91 > 88 characters)\ntests/test_chat_repl_comprehensive.py:580:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:588:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:604:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:607:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:620:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:623:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:632:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:638:89: E501 line too long (90 > 88 characters)\ntests/test_chat_repl_comprehensive.py:641:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:644:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:656:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:659:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:680:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:683:89: E501 line too long (97 > 88 characters)\ntests/test_chat_repl_comprehensive.py:685:89: E501 line too long (100 > 88 characters)\ntests/test_chat_repl_comprehensive.py:688:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:691:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:695:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:700:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:704:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:708:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:712:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:715:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:719:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:731:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:734:89: E501 line too long (90 > 88 characters)\ntests/test_chat_repl_comprehensive.py:735:89: E501 line too long (97 > 88 characters)\ntests/test_chat_repl_comprehensive.py:740:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:743:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:746:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:752:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:757:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:764:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:766:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:776:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:778:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:781:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:783:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:786:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:791:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:799:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:801:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:809:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:813:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:819:89: E501 line too long (92 > 88 characters)\ntests/test_chat_repl_comprehensive.py:822:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:829:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:832:1: W293 blank line contains whitespace\ntests/test_chat_repl_comprehensive.py:840:34: W291 trailing whitespace\ntests/test_chat_repl_comprehensive.py:840:35: W292 no newline at end of file\ntests/test_comprehensive_coverage.py:674:9: F841 local variable 'evidence_analyzer' is assigned to but never used\ntests/test_comprehensive_coverage.py:675:9: F841 local variable 'knowledge_graph' is assigned to but never used\ntests/test_comprehensive_functionality.py:15:1: F401 'json' imported but unused\ntests/test_comprehensive_functionality.py:38:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:50:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:55:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:61:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:77:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:83:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:87:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:97:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:103:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:107:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:117:89: E501 line too long (91 > 88 characters)\ntests/test_comprehensive_functionality.py:126:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:128:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:130:89: E501 line too long (106 > 88 characters)\ntests/test_comprehensive_functionality.py:131:89: E501 line too long (110 > 88 characters)\ntests/test_comprehensive_functionality.py:132:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:135:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:146:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:154:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:160:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:162:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:171:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:177:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:178:89: E501 line too long (97 > 88 characters)\ntests/test_comprehensive_functionality.py:179:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:181:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:185:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:207:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:209:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:211:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:220:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:226:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:232:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:238:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:244:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:252:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:254:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:256:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:259:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:265:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:268:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:269:89: E501 line too long (98 > 88 characters)\ntests/test_comprehensive_functionality.py:271:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:279:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:282:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:288:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:291:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:292:89: E501 line too long (94 > 88 characters)\ntests/test_comprehensive_functionality.py:298:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:300:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:308:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:315:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:324:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:329:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:334:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:339:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:346:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:351:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:355:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:362:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:364:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:367:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:373:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:375:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:382:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:390:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:398:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:410:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:415:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:420:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:429:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:432:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:436:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:443:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:450:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:457:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:461:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:463:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:470:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:478:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:482:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:486:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:488:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:495:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:505:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:512:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:518:89: E501 line too long (95 > 88 characters)\ntests/test_comprehensive_functionality.py:520:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:526:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:533:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:535:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:538:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:546:89: E501 line too long (89 > 88 characters)\ntests/test_comprehensive_functionality.py:550:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:553:1: W293 blank line contains whitespace\ntests/test_comprehensive_functionality.py:559:34: W291 trailing whitespace\ntests/test_comprehensive_functionality.py:559:35: W292 no newline at end of file\ntests/test_comprehensive_integration.py:347:9: E999 IndentationError: expected an indented block after 'except' statement on line 346\ntests/test_full_system.py:37:89: E501 line too long (99 > 88 characters)\ntests/test_full_system.py:53:89: E501 line too long (92 > 88 characters)\ntests/test_full_system.py:60:89: E501 line too long (111 > 88 characters)\ntests/test_full_system.py:84:89: E501 line too long (93 > 88 characters)\ntests/test_full_system.py:183:89: E501 line too long (89 > 88 characters)\ntests/test_full_system.py:184:89: E501 line too long (105 > 88 characters)\ntests/test_full_system.py:185:89: E501 line too long (102 > 88 characters)\ntests/test_full_system.py:206:89: E501 line too long (89 > 88 characters)\ntests/test_full_system.py:207:89: E501 line too long (105 > 88 characters)\ntests/test_full_system.py:208:89: E501 line too long (102 > 88 characters)\ntests/test_full_system.py:239:89: E501 line too long (105 > 88 characters)\ntests/test_langgraph.py:232:89: E501 line too long (91 > 88 characters)\ntests/test_langgraph.py:401:9: F841 local variable 'graph' is assigned to but never used\ntests/test_langgraph_integration_comprehensive.py:147:89: E501 line too long (91 > 88 characters)\ntests/test_production_comprehensive.py:26:1: F401 'uuid' imported but unused\ntests/test_production_comprehensive.py:28:1: F401 'unittest.mock.AsyncMock' imported but unused\ntests/test_production_comprehensive.py:28:1: F401 'unittest.mock.Mock' imported but unused\ntests/test_production_comprehensive.py:31:1: F401 'requests' imported but unused\ntests/test_production_comprehensive.py:38:1: F401 'llamaagent.agents.base.AgentRole' imported but unused\ntests/test_production_comprehensive.py:38:1: E402 module level import not at top of file\ntests/test_production_comprehensive.py:39:1: E402 module level import not at top of file\ntests/test_production_comprehensive.py:40:1: E402 module level import not at top of file\ntests/test_production_comprehensive.py:41:1: F401 'llamaagent.llm.create_provider' imported but unused\ntests/test_production_comprehensive.py:41:1: E402 module level import not at top of file\ntests/test_production_comprehensive.py:42:1: F401 'llamaagent.tools.ToolRegistry' imported but unused\ntests/test_production_comprehensive.py:42:1: E402 module level import not at top of file\ntests/test_production_comprehensive.py:43:1: F401 'llamaagent.types.TaskStatus' imported but unused\ntests/test_production_comprehensive.py:43:1: E402 module level import not at top of file\ntests/test_production_comprehensive.py:48:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:53:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:58:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:63:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:70:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:75:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:83:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:88:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:94:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:105:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:108:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:112:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:119:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:121:89: E501 line too long (91 > 88 characters)\ntests/test_production_comprehensive.py:123:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:131:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:134:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:140:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:150:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:153:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:160:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:165:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:169:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:179:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:184:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:188:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:196:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:200:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:204:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:213:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:219:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:221:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:229:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:231:9: F841 local variable 'rate_limited' is assigned to but never used\ntests/test_production_comprehensive.py:234:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:240:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:244:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:252:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:262:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:268:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:279:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:286:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:297:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:306:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:316:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:319:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:329:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:333:89: E501 line too long (91 > 88 characters)\ntests/test_production_comprehensive.py:337:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:339:89: E501 line too long (93 > 88 characters)\ntests/test_production_comprehensive.py:341:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:344:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:346:89: E501 line too long (91 > 88 characters)\ntests/test_production_comprehensive.py:348:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:352:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:355:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:362:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:365:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:368:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:371:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:377:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:382:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:389:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:393:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:395:89: E501 line too long (93 > 88 characters)\ntests/test_production_comprehensive.py:402:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:406:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:409:89: E501 line too long (89 > 88 characters)\ntests/test_production_comprehensive.py:411:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:415:23: W291 trailing whitespace\ntests/test_production_comprehensive.py:420:15: W291 trailing whitespace\ntests/test_production_comprehensive.py:424:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:429:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:436:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:437:89: E501 line too long (90 > 88 characters)\ntests/test_production_comprehensive.py:440:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:446:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:450:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:454:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:460:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:464:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:469:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:472:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:475:89: E501 line too long (93 > 88 characters)\ntests/test_production_comprehensive.py:478:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:481:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:489:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:496:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:504:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:512:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:518:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:525:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:528:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:531:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:535:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:544:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:548:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:551:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:554:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:558:24: W291 trailing whitespace\ntests/test_production_comprehensive.py:564:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:569:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:573:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:577:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:580:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:582:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:592:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:596:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:600:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:606:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:609:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:616:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:623:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:628:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:635:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:641:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:648:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:656:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:663:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:671:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:675:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:679:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:683:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:691:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:695:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:701:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:705:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:712:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:718:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:721:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:725:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:728:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:733:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:737:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:741:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:745:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:749:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:752:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:762:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:767:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:771:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:775:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:779:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:783:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:787:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:791:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:795:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:799:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:805:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:808:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:810:1: W293 blank line contains whitespace\ntests/test_production_comprehensive.py:824:7: W291 trailing whitespace\ntests/test_production_comprehensive.py:824:8: W292 no newline at end of file\ntests/test_simon_ecosystem_integration.py:400:57: F841 local variable 'mock_open' is assigned to but never used\ntests/test_spawning.py:327:9: F841 local variable 'sent_count' is assigned to but never used\ntests/test_spre_evaluator_comprehensive.py:529:89: E501 line too long (90 > 88 characters)\ntests/test_spre_evaluator_comprehensive.py:703:89: E501 line too long (97 > 88 characters)\ntests/test_ultrathink_full_coverage.py:9:89: E501 line too long (94 > 88 characters)\ntests/test_ultrathink_full_coverage.py:28:89: E501 line too long (91 > 88 characters)\ntests/test_ultrathink_full_coverage.py:29:89: E501 line too long (89 > 88 characters)\ntests/test_ultrathink_full_coverage.py:39:89: E501 line too long (89 > 88 characters)\ntests/test_vector_memory_comprehensive.py:36:89: E501 line too long (90 > 88 characters)\ntests/test_vector_memory_comprehensive.py:49:89: E501 line too long (102 > 88 characters)\ntests/unit/test_agents.py:7:1: F401 'pytest' imported but unused\ntests/unit/test_agents.py:8:1: F401 'unittest.mock.Mock' imported but unused\ntests/unit/test_agents.py:16:1: W293 blank line contains whitespace\ntests/unit/test_agents.py:27:1: W293 blank line contains whitespace\ntests/unit/test_agents.py:36:1: W293 blank line contains whitespace\ntests/unit/test_agents.py:41:1: W293 blank line contains whitespace\ntests/unit/test_agents.py:50:1: W293 blank line contains whitespace\ntests/unit/test_agents.py:55:1: W293 blank line contains whitespace\ntests/unit/test_agents.py:64:1: W293 blank line contains whitespace\ntests/unit/test_llm_providers.py:8:1: F401 'unittest.mock.Mock' imported but unused\ntests/unit/test_llm_providers.py:8:1: F401 'unittest.mock.patch' imported but unused\ntests/unit/test_llm_providers.py:15:1: W293 blank line contains whitespace\ntests/unit/test_llm_providers.py:21:1: W293 blank line contains whitespace\ntests/unit/test_llm_providers.py:28:1: W293 blank line contains whitespace\ntests/unit/test_llm_providers.py:34:1: W293 blank line contains whitespace\ntests/unit/test_llm_providers.py:41:1: W293 blank line contains whitespace\n"
    },
    "mypy_check": {
      "exit_code": 2,
      "output": "src/llamaagent/agents/multimodal_reasoning.py:110: error: invalid syntax  [syntax]\nFound 1 error in 1 file (errors prevented further checking)\n"
    },
    "bandit_scan": {
      "exit_code": 1,
      "output": "Working... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00\n"
    },
    "safety_scan": {
      "exit_code": 2,
      "output": ""
    },
    "pytest": {
      "exit_code": 4,
      "output": ""
    },
    "comprehensive_tests": {
      "exit_code": 4,
      "output": ""
    },
    "docker_build": {
      "exit_code": 1,
      "output": ""
    },
    "distribution_build": {
      "exit_code": 2,
      "output": ""
    },
    "performance_benchmarks": {
      "exit_code": 4,
      "output": ""
    }
  },
  "summary": {
    "total_checks": 11,
    "passed_checks": 0,
    "failed_checks": 11,
    "success_rate": 0.0
  }
}