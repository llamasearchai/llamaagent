# Example LlamaAgent Configuration File
# This demonstrates all available configuration options

# LLM Provider Configuration
llm:
  provider: "openai"  # Options: openai, anthropic, cohere, together, ollama, mock
  model: "gpt-4o-mini"
  api_key: null  # Will use OPENAI_API_KEY env variable if not set
  base_url: null  # Optional custom base URL
  temperature: 0.7
  max_tokens: 2000
  timeout: 300.0
  max_retries: 3
  stream: false
  top_p: null
  frequency_penalty: null
  presence_penalty: null

# Agent Configuration
agent:
  name: "LlamaAgent"
  max_iterations: 10
  spree_enabled: true
  debug: false
  trace_execution: false
  memory_enabled: true
  planning_enabled: true
  reflection_enabled: true
  tool_use_enabled: true

# Database Configuration
database:
  url: null  # Will use DATABASE_URL env variable if not set
  auto_migrate: true
  vector_dimensions: 1536
  connection_pool_size: 10
  echo: false
  pool_pre_ping: true
  pool_recycle: 3600

# API Server Configuration
api:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  cors_origins:
    - "*"
  enable_metrics: true
  enable_docs: true
  trusted_hosts:
    - "127.0.0.1"
    - "localhost"
  api_prefix: "/api/v1"

# Logging Configuration
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "json"  # Options: json, text
  file: null  # Optional log file path
  max_file_size: "10MB"
  backup_count: 5
  enable_color: true

# Cache Configuration
cache:
  enabled: true
  backend: "memory"  # Options: memory, redis, disk
  ttl: 3600  # Time to live in seconds
  max_size: 1000
  redis_url: null  # Required if backend is redis

# Security Configuration
security:
  enable_auth: false
  secret_key: null  # Required if enable_auth is true
  algorithm: "HS256"
  access_token_expire_minutes: 30
  allow_origins:
    - "*"

# Metadata
version: "1.0.0"
environment: "development"  # Options: development, staging, production