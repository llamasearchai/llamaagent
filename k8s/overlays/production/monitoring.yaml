apiVersion: v1
kind: ServiceMonitor
metadata:
  name: llamaagent-api-monitor
  namespace: llamaagent
  labels:
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: llamaagent-api
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
---
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: llamaagent-worker-monitor
  namespace: llamaagent
  labels:
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      component: worker
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: llamaagent-alerts
  namespace: llamaagent
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: llamaagent.rules
    interval: 30s
    rules:
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{job="llamaagent-api",status=~"5.."}[5m]) > 0.05
      for: 5m
      labels:
        severity: critical
        team: platform
      annotations:
        summary: "High error rate detected"
        description: "Error rate is above 5% for {{ $labels.instance }}"
    
    - alert: HighResponseTime
      expr: |
        histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
      for: 5m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "High response time detected"
        description: "95th percentile response time is above 1s"
    
    - alert: PodMemoryUsage
      expr: |
        container_memory_usage_bytes{namespace="llamaagent"} / container_spec_memory_limit_bytes > 0.9
      for: 5m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "High memory usage"
        description: "Pod {{ $labels.pod }} memory usage is above 90%"
    
    - alert: QueueBacklog
      expr: |
        redis_queue_length{queue="evolution_tasks"} > 1000
      for: 10m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "High queue backlog"
        description: "Evolution task queue has {{ $value }} pending tasks"
    
    - alert: DatabaseConnectionPool
      expr: |
        pg_stat_database_numbackends{datname="llamaagent"} / pg_settings_max_connections > 0.8
      for: 5m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "Database connection pool near limit"
        description: "Database connection usage is above 80%"
    
    - alert: GPUUtilization
      expr: |
        nvidia_gpu_duty_cycle{namespace="llamaagent"} < 0.3
      for: 30m
      labels:
        severity: info
        team: ml
      annotations:
        summary: "Low GPU utilization"
        description: "GPU {{ $labels.gpu }} utilization is below 30%"
    
    - alert: CertificateExpiry
      expr: |
        certmanager_certificate_expiration_timestamp_seconds - time() < 7 * 24 * 60 * 60
      for: 1h
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "Certificate expiring soon"
        description: "Certificate {{ $labels.name }} expires in less than 7 days"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: llamaagent
data:
  llamaagent-overview.json: |
    {
      "dashboard": {
        "title": "LlamaAgent Overview",
        "panels": [
          {
            "title": "Request Rate",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{job=\"llamaagent-api\"}[5m])) by (status)"
              }
            ]
          },
          {
            "title": "Response Time",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
              }
            ]
          },
          {
            "title": "Active Tasks",
            "targets": [
              {
                "expr": "redis_queue_length{queue=~\".*_tasks\"}"
              }
            ]
          },
          {
            "title": "LLM Token Usage",
            "targets": [
              {
                "expr": "sum(rate(llm_tokens_used_total[5m])) by (provider, model)"
              }
            ]
          },
          {
            "title": "Cost per Hour",
            "targets": [
              {
                "expr": "sum(rate(llm_cost_dollars_total[1h])) by (provider)"
              }
            ]
          }
        ]
      }
    }